[
    {
        "uri": "/content/blog/_index",
        "content": "---\r\ntitle: blog\r\n---",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-04-17-how-to-default-ssrs-date-parameters-to-the-first-and-last-day-of-the-the-previous-month",
        "content": "---\r\ndate: \"2013-04-17T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: How to default SSRS date parameters to the first and last day of the the previous\r\n  month\r\n---\r\n\r\nPopulating default dates in SSRS can be helpful to save the user from having to constantly input the date range they normally would use. When a report is pulled for last month's information, defaulting the date fields for the user can help streamline their usage of the report, instead of them manually selecting with the date-picker control in SSRS. The formula's I used were:\r\n\r\n`ssrs\r\nBeginning of Current Month (EOM) DateSerial(Year(Date.Now), Month(Date.Now), 1)\r\nBeginning of Last Month (BOM) DateAdd(DateInterval.Month, -1, DateSerial(Year(Date.Now), Month(Date.Now), 1))\r\nEnd of Last Month (EOM) DateAdd(DateInterval.Minute, -1, DateSerial(Year(Date.Now), Month(Date.Now), 1))\r\n`\r\nTo set the default date of the parameters:\r\n\r\nFirst open up the Report Data Window, and choose your date parameters.\r\n\r\n\r\n!--  --\r\nNavigate to Default values, and click the Fx button to edit the expression for the field.\r\nPaste the formula into the expression field and save.\r\n\r\nResult: Your default dates should now show last month's date range. You can apply your own rounding or date types if you wish, this provides the time as well, since I was working with smalldatetime, datetime, and datetime2 datatypes.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-04-18-dynamic-sql-and-a-char-crash",
        "content": "---\r\ndate: \"2013-04-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: dynamic sql and a char crash\r\n---\r\n\r\nDynamic SQL can be helpful, but a pain to debug. I spent hours today working on figuring out why my simple date comparison in dynamic SQL wasn't working. Found out that the remote database I was connecting to had a char date instead of a datetime. I found the comparison of CHARDATE  VARCHARDATE failed to error out, but also failed to give a proper result set. Changing the look-up to ensure both dates were converted to date fixed the issue. During this debugging I was reviewing my dynamically created SQL statement.\r\n\r\nI learned that SSMS limits the amount of text it will return. In trying to view the single large UNION ALL statement, I was experiencing truncated results. I wanted to ensure the code being executed looked proper, but couldn't get past the truncation.\r\n\r\nEnter SSMSBoost to the rescue!\r\n\r\nSSMSboost is created by developers and very responsive to requests. I'll do a proper review soon. They offer a visualize data option that goes beyond the usage I employed. For my purpose, I clicked on the cell and selected visual cell as text, and opened the data in notepad++. This showed the full text without truncation. I was able to move on in my debugging then as I knew the dynamic sql statement was not actually truncated except to my SSMS output. Dynamic SQL is a great tool, but if I had been working with direct queries, the issue would have been much faster to resolve!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-04-22-native-ssms-a-second-class-citizen-no-longer",
        "content": "---\r\ndate: \"2013-04-22T00:00:00Z\"\r\nlastmodifiedat: \"2013-04-21\"\r\npublished: true\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: Native SSMS a second class citizen no longer...\r\ntoc: true\r\n---\r\n\r\nIntellisense can be a boon to adding quick development. Quick hints on scope specific variables, syntax suggestions, function descriptions and more provide a valuable tool to productive coding.Coding in SQL Server Management Studio (SSMS) has greatly improved over the version releases, but it still lags behind the power of Visual Studio's intellisense, template insertions with \"fill in the blank\" functionality .\r\n\r\nAdditionally, the lack of automatic indentation means that lining up sub-queries and levels of logic can be annoyingly time consuming... especially when not everyone on a team following the exact same coding standards. Legibility can easily suffer. Intellisense fails to properly update at times and reads pending statements as errors in syntax disabling the prompts from providing help. Automatic SQL formatting has been a long missed feature in SSMS. Trying to line up levels of nested queries can be very time consuming, and variances in the way people layout the query can effect readability.\r\n\r\nAs a developer, I'm constantly looking for ways to improve my coding experience and streamline repetitive bits of coding. One such tool is SSMSBoost which provides many valuable shortcuts and features that SSMS omitted. The SSMSBoost team provides a free community edition to their fellow developers and responds to feedback promptly. I'd highly recommend checking them out.\r\n\r\nHowever, SSMS Intellisense improvement is by far the most powerful and productive coding tool that developers can look to improve. This tool enables quick hints to job the foggy mind missing that all important cup of espresso in the morning!\r\n\r\nCut above the competition?\r\n\r\nIn my search for coding tools I've reviewed three main contenders in the arena: ApexSQL Complete, Red Gate SQL Prompt, and DbForge SQL Complete. For me, the best tool ended up being DbForge SQL Complete. The product fills in the gap on intellisense improvements. I've installed in SSMS 2012, which is an improved environment and developers can use while still working with SQL Server 2008R2.\r\n\r\nApexSQL Complete was offered free to the community with some powerful formatting capabilities. However, I was never able to get the formatting to truly match the clean results of Red Gate SQL Prompt and DbForge SQL Complete. Additionally, ApexSQL Complete had some lag issues originally which seem to have been improved in recent releases, but still seemed to get in the way more than the other tools. Their object info prompts were rudimentary and not formatted well.\r\n\r\nRed Gate SQL Prompt is a fine product, and rates highly. However, the customization offered in the formatting profiles was very limited and seemed focused on simplicity rather than offering complete control. It also seemed to have some unpolished edges, such as typing INSERT INTO TABLE, would provide the template of columns and value upon accepting the intellisense prompt, but the insertion didn't follow the same formatting standards you had setup, and thus was harder to work with. It also was an expensive option compared to the others, with a Professional license running over $300+ for just a year of upgrades and support. DbForge SQL Complete offered the best solution for my needs. The amount of customization offered was incredible and the end result was a powerful formatting tool with amazing results, and a intellisense tool that truly improved my coding experience (enough that I wanted to share with others this tool!) This tool is also compatible with Visual Studio, so now those doing SQL work in Visual studio will have a great tool to help fill the deficiencies of VS for SQL development.\r\n\r\nBefore reviewing the tool, I'd also mention that DBForge offers a user vote/forum with which I've had feedback on almost every single issue right away. Their communication seems top notch, in additional to providing special programs and discounts for their products. One bug I found in the formatting was addressed with a new update within 2 weeks. That's a responsive company!\r\n\r\n DB Forge SQL Complete\r\n\r\nThis review is performed on the full version. An express version is also offered to the community as a service. This tool has a portion of the full version's functionality to whet the eager developer's appetite. A free trial is offered on the full version... but beware you won't be able to be satisfied with native SSMS features after this!\r\n\r\nGeneral Options - The Normally Boring Stuff That's Not Quite Boring!\r\n\r\nImprovements to the SSMS Coding environment are shown. A shortcut to refresh the cache of intellisense, formatting options, a command to execute the currently selected statement, outline the structure of the document, snippet management, and feedback. I'll review only the portions that seem to have the greatest impact, as most of it is self explanatory.\r\n\r\n\r\n\r\nOf particular note is the \"automatically trigger after\" option. For users that want intellisense to help out only after a long pause and not constantly change while typing, this option is helpful. Setting a longer time on the ms will keep the box from showing up unless a command is pressed to initiate the intellisense showing up (Default: Control-Space).\r\n\r\nThe highlight occurrences option is a great feature added by Devart. When a particular object is selected in the query editor window, other occurrences are highlighted for easy viewing.\r\n\r\n\r\n\r\n\r\n\r\nAutomatic alias generation is another boon to the developer. SQL Complete is will automatically generate an alias for you upon object selection from intellisense. This also means that column selections later on will be correctly assign the alias, reducing typing.\r\nselect * from company c -- this alias was auto generated when selected\r\n\r\n\r\n\r\n\r\n Seamless Shortcuts provide SSMS Satisfaction\r\n\r\nAlias generation is powerful, but overshadowed by one of the simplest yet much needed features that SSMS Native Intellisense lacks: column selection and wild card expansion.\r\n\r\nDragging the column names from object explorer results in a list of columns delimited by commas, but not stacked vertically, it also doesn't handle multiple table columns being dragged at the same time, or a \"limited selection of columns.\"\r\n\r\nSQL Complete fills this gap. Pressing tab allows expansion of columns in a stacked list, with alias's already assigned correctly. The column select appends to the same line, but with the automatic formatting tool, the finished list is easily stacked vertically with the press of the format shortcut. These are simple examples, but I've found to extremely helpful when working with complex queries involving numerous tables. Expanding all the columns available or selecting, with correct alias generation is a major improvement to the SSMS development environment\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nInsert statements are \"auto completed\" helping provide quick statement generation, this is a lifesaver!\r\n\r\n\r\n\r\n\r\n\r\n\r\nPrompts also help developers know the table structure of objects/views, stored procedure parameters, and function usage. Might save some folks MSDN googling just to remind themselves about the correct function usage!\r\n\r\n\r\n\r\n\r\n\r\n\r\nCompare this to the native function prompt:\r\n\r\n\r\n\r\n\r\nSnippet Templates are made a reality\r\n\r\nenter mind blown event Snippet tools are helpful for saving some typing, but until now they've been typically limited to just pasting in text. The full functioned snippet functionality of Visual Studio with field selection by navigating with tab hasn't existed in SSMS.... till now.\r\n\r\nSnippets are brought to us with SQL Complete with an amazing thought to detail. Not only are the snippets brought into our editor window, but are also displayed in intellisense, with\"fields\" provided to navigate to to \"fill in the blanks.\" Variables for customization are offered and the sky is the limit here!\r\n\r\n\r\n\r\n\r\n\r\n\r\nBelow I demonstrate a cursor snippet. The only keystrokes I used to complete this action were:\r\n\r\ncu + Tab @MyCustom + Tab + customnameienteredinoneplace\r\nTotal Keystrokes = 48 (10.9% Cost ) Total keystrokes without addin (and no mistakes) = 440 (89.1% cost) Would you keep an execution plan version that was 89% higher cost... 392 keystrokes later?\r\n\r\n\r\n\r\n Formatting made painless\r\n\r\nFormatting SQL code manually can be a pain. There are online tools offered with some limited functionality, but who really wants to use a web based formatting tool with production code? Addins for SSMS formatting are limited in number. Parsing the logical construct of a SQL statement isn't quite a simple as indenting a foreach loop in C#. SQL Complete offers one of the best SSMS SQL formatters I've come across.\r\n\r\nThe level of customization is intense. A few screenshots are provided, but I will not go into all the levels of customization, instead I'll provide you with some MSDN code samples that SQL Complete beautified. Running the format can be done on a selection or an entire document. Settings can be saved so a team could easily share a formatting standard. This would prevent each person from having to configure all their settings manually, a major plus in setup for any team!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nUgly Duckling SQL turned into the beautified SQL you'd be proud of\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHere are some examples of the formatting tool at work in order from a simple query to a complex recursive CTE. I believe this tool handles it beautifully!\r\n\r\nBEFORE\r\n\r\n\r\n\r\nAFTER\r\n\r\n\r\n\r\nBEFORE\r\n\r\n\r\n\r\nAfter: *line breaks handled with union's\r\n\r\n\r\n\r\nBEFORE: Complex CTE\r\n\r\n\r\n\r\n*AFTER *\r\n\r\n\r\n\r\n Final Thoughts\r\n\r\nAdditional features are available, such as automatically finding object in object explorer when selected in query window, outlining of document, contextually based join statements, and more. I covered only the features I considered the highest impact on my workflow. Finding a tool to enhance the development process in SSMS should be something SQL developer's consider.\r\n\r\nOnce you utilize a tool like this, you'll find that the improvements help you focus on the more important things. Spending time thinking about \"lining\" up code is a waste of time when a developer could be focused on the content. Formatting all code to be lined up the same with a press of your shortcut keys also helps you not focus on trying to discern the formatting standards of others, and instead get down to the important work! The continual improvements and responsiveness of Devart to feedback and resolving issues I had makes me give their product a two thumbs up award!\r\n\r\nDisclaimer\r\n\r\nI do not work for Devart or any competing product. I reviewed this program since I've been sold on the value of it, and wanted to participate in their High Five program which helps developers that like their products share with others, and be eligible for discount/reward if a review of their product is completed. I believe the tool to be useful, and has improved my experience with SSMS. I hope other SQL developers can benefit, especially those who were unaware of the availability of such a great addin.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-04-23-snippet-designate-a-certain-time-of-the-day-in-getdate()",
        "content": "---\r\ndate: \"2013-04-23T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: snippet designate a certain time of the day in getdate()\r\n---\r\n\r\nSnippet to designate a certain time of the day to evaluate in the current day. If you need to limit a result to the current date after a particular time, strip the time out of the date, and concatenate the current time together with it, and then convert back to datetime2.\r\n\r\n`sql\r\nselect convert(datetime2(0),cast(cast(getdate() as date) as varchar(10)) + ' 09:00 ')\r\n`\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-04-30-installing-ssms-2012-all-by-it's-lonesome",
        "content": "---\r\ndate: \"2013-04-30T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Installing SSMS 2012 all by it's lonesome\r\n---\r\n\r\nSQL Server Management Studio (SSMS) is not offered as a standalone download on MSDN. Installation requires the user to download the sql server installation package and choose to install only this single feature. For developers, SQL Developer edition is a great choice.\r\n\r\nHere's some screenshots to give you a guide on installing SSMS by itself when working with the full installer.\r\n\r\n info \"Updated: 2017-02\"\r\n This doesn't apply for future SSMS versions as they began (I believe with 2014) to package SSMS outside of the database engine installer, allowing continual iterations and improvements for SSMS outside of being included as part of SQL server patching (finally!). If you are still reading this.... get an updated SSMS!\r\n\r\n\r\n\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n\r\nhr\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-05-01-calculating-the-next-beginning-of-month-and-the-current-end-of-month",
        "content": "---\r\ndate: \"2013-05-01T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Calculating the next beginning of month and the current end of month\r\n---\r\n\r\nHandling dates is always a fun challenge in T-SQL! Finding the current end of month and next months beginning of month is straight forward, but I like to find new ways to do things that take less coding, and hate date conversions that require a lot of manipulation of characters and concatenation. This was what I came up with for avoiding character conversions and concatenation for finding the current BOM (beginning of month) and EOM (end of month) values. Adjust according to your needs. Cheers!\r\n\r\n`sql\r\n    --use datediff from 0, ie default 1900 date, to calculate current months as int\r\n    declare @ThisMonth int = datediff(month,0,cast(getdate() as date))\r\n\r\n    --add 1 to the current month to get the next month\r\n    declare @NextBom date = dateadd(month,@ThisMonth+1,0)\r\n\r\n    -- subtract a day from the beginning of next month to get the current end of month, without worrying about 28, 30, or 31 days.\r\n    declare @ThisEom date = dateadd(day,-1,@NextBom)\r\n    select @ThisMonth select @NextBom select @ThisEom\r\n`",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-05-21-get-synonym-definitions-for-all-databases-in-server",
        "content": "---\r\ndate: \"2013-05-21T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Get synonym definitions for all databases in server\r\n---\r\n\r\nIf you want to audit your enviroment to look at all your synonyms and see where they are pointing, you can use exec sys.sp_MSforeachdb to loop through databases, and even filter. It will save some coding. However, my research indicates it is probably a bad practice to rely on this undocumented function as it may have issues not forseen and fully tested.\r\n\r\nAdditionally, support may drop for it in the future. I recreated what I needed with a cursor to obtain all the synonym definitions into a temp table and display  results.:\r\n\r\n`sql\r\n/*\r\n    create temp table for holding synonym definitions & list of DB\r\n    */\r\n\r\n    if object_id('tempdb..#dblist') is not null\r\n        drop table #dblist;\r\n    select\r\n        into #dblist\r\n    from\r\n        sys.databases\r\n    where\r\n        name not in ('master', 'tempdb', 'model', 'msdb')\r\n        and State_desc = 'ONLINE'\r\n        and IsInStandby = 0\r\n    if object_id('tempdb..#temp') is not null\r\n        drop table #temp;\r\n\r\n    create table #temp\r\n        (\r\n            db_name               sysname\r\n            ,object_id             int\r\n            ,name                  sysname\r\n            ,baseobjectname      sysname\r\n            ,servernamehardcoded as case\r\n                when baseobjectname like '%ThisDatabaseIsOkToHardCode%'\r\n                then 0\r\n                when len(baseobjectname)\r\n                        len(replace(baseobjectname, '.', ''))  2\r\n                then 1\r\n                else 0\r\n            end\r\n        )\r\n\r\n    go\r\n\r\n    declare @DbName sysname\r\n    declare @XSQL varchar(max)\r\n    declare @CompleteSQL varchar(max)\r\n    declare dbcursor cursor fastforward read_only local for select\r\n                name\r\n            from\r\n                #dblist\r\n    open db_cursor\r\n    fetch next from db_cursor into @DbName;\r\n\r\n    while @@fetch_status = 0\r\n    begin\r\n        set @XSQL = '\r\n                    insert into #temp\r\n                    ( dbname ,objectid ,name,baseobjectname )\r\n                    select\r\n                        db_name()\r\n                        ,s.object_id\r\n                        ,s.name\r\n                        ,s.baseobjectname\r\n                    from\r\n                        sys.synonyms s\r\n                    '\r\n        set @CompleteSQL = 'USE ' + @DbName\r\n                            '; EXEC sp_executesql N'''\r\n                            @XSQL + '''';\r\n        exec (@CompleteSQL)\r\n        fetch next from db_cursor into @DbName;\r\n    end\r\n\r\n    close db_cursor\r\n    deallocate db_cursor\r\n    go\r\n    select\r\n        from\r\n        #temp t\r\n`",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-05-22-on-how-to-googlify-your-sql-statements-for-future-searching",
        "content": "---\r\ndate: \"2013-05-22T00:00:00Z\"\r\nexcerpt: On using SSMSBoost to improve your SSMS workflow\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: On how to Googlify your SQL statements for future searching\r\ntoc: true\r\n---\r\n\r\nFor sake of future generations, let's begin to reduce typing and reuse code we've built. I think we can all agree that TSQL statements are often repeated.\r\nIdeally, snippets should be created to reduce repeated typing and let us focus on logic and content. However, some statements may not really be \"snippet worthy\", and just be quick adhoc queries.\r\nIn the past, the solution for saving queries for reuse or reference in the future would be to just save in the projects folder manually. However, it is difficult to always make sure the file is saved, review previous version that may be want had overrode, or even review what statements you actually executed. SSMSToolsPack has historically offered a great logging option. However, as an individual it was hard to justify the cost out of my own pocket. SSMSBoost has provided a great solution! Note that this was recently added (April), and is a \"rough draft\" , with minimal interface options, yet provides an amazing solution that I've found to offer a great solution.\r\nIn addition to the other great features that SSMSBoost offers (which I'll write about in the future), SSMSBoost now offers 3 unique solutions to saving work.\r\n\r\nExecuted Statement Logging This feature saves all your executed statements (ie, when you hit execute) as a .sql file for future reference.  As of today, there is no GUI for managing this. Never fear, I have a great solution for you.\r\nEditor History Logging This feature saves the current contents of your open query windows at predefined intervals, by default set to 60 seconds. According to their documentation, if no changes have been made to file, it will not save a new version. It will only add a new version once changes are detected to a file.\r\n\r\n3.Tab History Logging If you crash SSMS, close SSMS without saving tabs, or have some unsavory Windows behavior that requires SSMS to be restarted, don't fear... your tabs are saved. When restarting you can select restore tabs and begin work again. I've found this feature to be a lifesaver!\r\n\r\n\r\nSearching Your Executed and Editor History\r\nInstructions I recommend for setup and searching your entire sql history nearly instantly.\r\n\r\nInstall SSMSBoost (free community edition if you can't support with professional version)\r\nInstall DocFetcher (open source full text search tool. Best I found for searching and previewing sql files without the complexity of using GREP or other similar tools)\r\nDownload and run Preview Handler from WinHelp\r\nRun Preview Handler  Find .SQL  Preview as plain text\r\nRun SSMS  Open Settings in SSMSBoost\r\nConfigure settings as you see fit. I personally move my Editor History and Executed statement's location to my SSMS Folder, so that I can use something like Create Synchronicity to backup all my work daily.\r\n\r\nRestart SSMS for settings to take effect.\r\nStart DocFetcher, go to settings in the top right hand corner.\r\n\r\n\r\nBasic Settings I choose (If you aren't using Bitstream font... you are missing out)\r\n\r\n\r\n Docfetcher Advance settings tweaks Change:\r\n\r\n    CurvyTabs = true HtmlExtensions = html;htm;xhtml;shtml;shtm;php;asp;jsp;sql InitialSorting = -8\r\n\r\nWhy? Curvy tabs... because curves are nice\r\nHTML Extensions, obvious\r\nInitial Sorting = -8 means that instead of sorting by \"match %\" which I didn't find helpful for me, to sort by modified date in desc order. This means I'll find the most most recent match for the text I'm searching for at the top of my list.\r\n\r\nSetup your custom indexes. I setup separate indexes for executed statements and editor history so I could filter down what I cared about and eliminate near duplicate matches for the most part. Right click in blank space to create index.\r\n\r\nI setup as follows:\r\n\r\nbr\r\n\r\n\r\nNow the DocFetcher daemon will run in the background, if you copied my settings, and update your indexes.  Searching requires no complex regex, and can be done easily with statements. I'd caution on putting exact phrases in quotes, as it does detect wildcards.\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-05-22-view-computed-columns-in-database",
        "content": "---\r\ndate: \"2013-05-22T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: View computed columns in database\r\n---\r\n\r\nSnippet to quickly view computed column information. You can also view this by doing a \"create table\" script. This however, was a little cleaner to read and view for me.\r\n\r\n`sql\r\n\r\nselect\r\n    databasename = dbname()\r\n    ,objectschemaname = objectschemaname( object_id )\r\n    ,objectname = objectname( object_id )\r\n    ,fullobjectname = objectschemaname( objectid ) + '.' + objectname( object_id )\r\n    ,column_name = name\r\n    ,cc.is_persisted\r\n    ,cc.Definition\r\nfrom\r\n    sys.computed_columns cc\r\norder\r\n    by fullobjectname asc\r\n`",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-05-30-a-moment-of-void-in-the-cranium-reveals-a-recursive-computed-column-with-an-esoteric-message",
        "content": "---\r\ndate: \"2013-05-30T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: A moment of void in the cranium reveals a recursive computed column with an\r\n  esoteric message\r\n---\r\n\r\nMsg 402, Level 16, State 1, Line 67 The data types varchar and void type are incompatible in the add operator.\r\n\r\nI came across this error today when I accidentally used a computed column in a temp table, that referenced itself. This very unhelpful message was caused by referring to the computed column itself in the computed column definition, ie typo. Beware!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-05-31-tsql-snippet-for-viewing-basic-info-on-database-principals-and-their-permissions",
        "content": "---\r\ndate: \"2013-05-31T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: TSQL Snippet for viewing basic info on database principals and their permissions\r\n---\r\n\r\nQuick snippet I put together for reviewing basic info on database users/principals, permissions, and members if the principal is a role.:\r\n\r\n`sql\r\n/***************\r\n    Some Basic Info on Database principals, permissions, explicit permissions, and if role, who is in this role currently\r\n    ***************/\r\n\r\n    ;with roleMembers as (\r\n                            select\r\n                                drm.roleprincipalid\r\n                            ,dp.principal_id\r\n                            ,dp.name\r\n                            from\r\n                                sys.databaserolemembers drm\r\n                                inner join sys.database_principals dp\r\n                                    on drm.memberprincipalid = dp.principal_id\r\n                            )\r\n    select\r\n        db_name()\r\n        ,dp.name\r\n        ,stuff((\r\n                select distinct\r\n                    ', ' + p.permission_name\r\n                from\r\n                    sys.database_permissions p\r\n                where\r\n                    dp.principalid = p.granteeprincipal_id\r\n                    and p.major_id  = 0\r\n                    and p.state     = 'G'\r\n                for xml path(''), type\r\n                ).value('.', 'varchar(max)'), 1, 1, ''\r\n                ) as general_permissions\r\n        ,stuff((\r\n                select distinct\r\n                    ', ' + p.permission_name\r\n                from\r\n                    sys.database_permissions p\r\n                where\r\n                    dp.principalid = p.granteeprincipal_id\r\n                    and p.major_id  = 0\r\n                    and p.state     = 'D'\r\n                for xml path(''), type\r\n                ).value('.', 'varchar(max)'), 1, 1, ''\r\n                ) as deny_permissions\r\n        ,stuff((\r\n                select distinct\r\n                    ', ' + p.permissionname + ' on ' + objectschemaname(p.majorid) + '.' + objectname(p.majorid)\r\n                from\r\n                    sys.database_permissions p\r\n                where\r\n                    dp.principalid = p.granteeprincipal_id\r\n                    and p.major_id   0\r\n                for xml path(''), type\r\n                ).value('.', 'varchar(max)'), 1, 1, ''\r\n                ) as specific_permissions\r\n        ,stuff((\r\n                select distinct\r\n                    ', ' + r.name\r\n                from\r\n                    roleMembers r\r\n                where\r\n                    r.roleprincipalid = dp.principal_id\r\n                for xml path(''), type\r\n                ).value('.', 'varchar(max)'), 1, 1, ''\r\n                ) as currentactivemembers\r\n    from\r\n        sys.database_principals dp\r\n    order by\r\n        dp.name asc;\r\n`",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-07-16-ssms-2012-extender-for-the-times-you-want-some-organization-to-the-random-pile-of-objects-gathering-in-the-dusty-confines-of-that-database",
        "content": "---\r\ndate: \"2013-07-16T00:00:00Z\"\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: SSMS 2012 Extender for the times you want some organization to the random pile\r\n  of objects gathering in the dusty confines of that database\r\n---\r\n\r\nWhen dealing with large amounts of objects in a database, navigation can be tedious with SSMS object explorer. This extender organizes the objects into groups based on schema, helping a developer easily navigate to the appropriate object. The current version didn't work for views, but the other objects were grouped effectively. Highly recommend!\r\n\r\nSSMS 2012 Extender \r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-07-18-check-constraints-can-help-enforce-the-all-or-nothing-approach-when-it-comes-to-column-updates",
        "content": "---\r\ndate: \"2013-07-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Check Constraints can help enforce the all or nothing approach when it comes\r\n  to column updates\r\n---\r\n\r\nIf you have a set of columns inside your table that you want to allow nulls in, however if one of the columns is updated force all columns in the set to be updated, use a check constraint. In my case, I had 3 columns for delete info, which were nullable. However, if one value was updated in there, I want all three of the delete columns to require updating. I created the script below to generate the creation and removal of these constraints on a list of tables:\r\n\r\n`sql\r\n\r\n/* CHECK CONSTRAINT TO ENSURE SET OF COLUMNS IS NULL OR IF UPDATED,\r\nTHAT ALL COLUMNS IN SET ARE UPDATED Columns: deletedate null deletebyid null deletecomment null PASS CONDITION\r\n1: IF ALL COLUMNS NULL = PASS PASS CONDITION\r\n2: IF ALL COLUMNS ARE UPDATED/NOT NULL = PASS\r\n\r\nFAIL: IF 1,2 OF THE COLUMNS ARE UPDATED, BUT NOT ALL 3 THEN FAIL\r\n*/\r\n\r\n/* GENERATE CHECK CONSTRAINT ON ALL SELECTED TABLES TO REQUIRE ALL DELETE\r\nDATE COLUMNS TO BE UPDATED CORRECTLY */\r\n\r\nselect\r\n    t.TABLE_SCHEMA\r\n    ,t.TABLE_NAME\r\n    ,scripttoremoveifexists = ' IF exists (select * from sys.objects where name =''check' + t.TABLESCHEMA + '' + t.TABLENAME + 'softdeleterequiresalldeletecolumnspopulated20130718'') begin alter table ' + t.TABLESCHEMA + '.' + t.TABLENAME + ' drop constraint check' + t.TABLESCHEMA + '' + t.TABLENAME + 'softdeleterequiresalldeletecolumnspopulated20130718 end '\r\n    ,scripttorun =              ' alter table ' + t.TABLESCHEMA + '.' + t.TABLENAME + ' add constraint check' + t.TABLESCHEMA + '' + t.TABLENAME + 'softdeleterequiresalldeletecolumnspopulated20130718 check ( ( case when deletedate is not null then 1 else 0 end + case when deletebyid is not null then 1 else 0 end + case when delete_comment is not null then 1 else 0 end ) in (0, 3) ) '\r\nfrom\r\n    INFORMATION_SCHEMA.TABLES t\r\nwhere\r\n    t.TABLE_NAME like 'mytablename%'\r\n    and exists (select\r\n            from\r\n            INFORMATION_SCHEMA.COLUMNS C\r\n        where\r\n            t.TABLECATALOG = C.TABLECATALOG\r\n            and t.TABLESCHEMA = C.TABLESCHEMA\r\n            and t.TABLENAME = C.TABLENAME\r\n            and C.COLUMNNAME = 'deleteby_id')\r\n    and exists (select\r\n            from\r\n            INFORMATION_SCHEMA.COLUMNS C\r\n        where\r\n            t.TABLECATALOG = C.TABLECATALOG\r\n            and t.TABLESCHEMA = C.TABLESCHEMA\r\n            and t.TABLENAME = C.TABLENAME\r\n            and C.COLUMNNAME = 'deletecomment')\r\n    and exists (select\r\n            from\r\n            INFORMATION_SCHEMA.COLUMNS C\r\n        where\r\n            t.TABLECATALOG = C.TABLECATALOG\r\n            and t.TABLESCHEMA = C.TABLESCHEMA\r\n            and t.TABLENAME = C.TABLENAME\r\n            and C.COLUMNNAME = 'deletedate')\r\norder by\r\n    t.TABLE_SCHEMA asc\r\ngo\r\n`",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-07-24-shortcut-to-reference-examples,-syntax,-and-definitions-straight-from-ssms",
        "content": "---\r\ndate: \"2013-07-24T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Shortcut to reference examples, syntax, and definitions straight from SSMS\r\n---\r\n\r\nI've never really used the F1 key for help files with most applications. I was surprised at the usefulness in SSMS I discovered today that uses scripting to actually get you MSDN articles relevant to your current selection in the query editor.\r\n\r\nIf you have a keyword selected and want to view details, definition, and examples on it, you can highlight the phrase or select the word, press F1, and SSMS will pull up the appropriate MSDN article. The only issue I ran into was that it pulls up the most recent article, so if you aren't running SQL Server 2012, make sure you select the\r\n< 2012 documentation to be sure it is accurate.\r\n\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2013-08-13-renaming-all-references-inside-stored-procedures-and-functions-can-be-migraine-worthy-without-a-little-help...",
        "content": "---\r\ndate: \"2013-08-13T00:00:00Z\"\r\nexcerpt: assistance to convert database references when using synonyms\r\ntags:\r\nsql-server\r\ntitle: Renaming all references inside stored procedures and functions can be migraine worthy without a little help...\r\nslug: renaming-all-references-inside-stored-procedures-and-functions-can-be-migraine-worthy-without-a-little-help\r\n---\r\n\r\n info \"Updated: 2016-03-18\"\r\n Cleaned up formatting. This is older code limited to procs and functions. I'm sure there is a better way to do this now, but leaving here as it might help someone else in the meantime.\r\n\r\nIf you run across migrating or copying a database structure for some purpose, yet need to change the database references or some other string value inside all the procedures and functions to point to the newly named object, you are in for a lot of work! I built this procedure to search all procedures and functions, and script the replacement across multiple databases, to streamline this type of conversion.\r\n\r\nI'll post up one for views and synonyms later, as my time was limited to post this. In my case, this script was built to replace DB1 with DB2, and I had to accomplish this across several databases at once.\r\n\r\nThis script might help save you some time!\r\n\r\n{{% gist fd2e49f4f69202cd2da6 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-02-11-scalar-functions-can-be-the-hidden-boogie-man",
        "content": "---\r\ndate: \"2014-02-11T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Scalar functions can be the hidden boogie man\r\n---\r\n\r\nRan across a comment the other day that scalar functions prohibit parallelism for a query when included. I thought it would be worth taking a look, but didn't take it 100% seriously. Came across the same indication today when reviewing MVP deep dives, so I put it to the test.Turns out even a simple select with a dateadd in a scalar format was affected enough with that one action to drop 5% on the execution plan. When dealing with merge or other processes that would benefit from parallelism, this would become even more pronounced.\r\nSuggest reading \"Death by UDF\" section by Kevin Boles.\r\nThis comment is buried at the very end of the chapter. He indicates\r\n\r\n \"One final parting gift: scalar UDFs also void the use of parallel query plans, which is why the FormatDate UDFpegged only ONE CPU core on my laptop! \" (Page 194-summary)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-05-19-eliminate-overlapping-dates",
        "content": "---\r\ndate: \"2014-05-19T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Eliminate Overlapping Dates\r\n---\r\n\r\nI was looking for an efficient way to eliminate overlapping days when provided with a historical table that provided events that could overlap. In my case, I had dates show the range of a process. However, the multiple start and end dates could overlap, and even run concurrently. To eliminate double counting the days the process truly was in play I needed a way to find eliminate the overlap, and eliminate duplicate days when running in parallel. I researched ways to complete this and found the solution through this post. Solutions to Packing Date and Time Intervals Puzzle \r\n\r\nItzik provided an excellent solution, though I had to take time to digest. The only problem I ran into, was his solution was focused on a single user and dates. For my purposes, I need to evaluate an account and further break it down by overlap on a particular process. Grateful for SQL MVP's contributions to the community as this was a brain bender!\r\n\r\n\r\n\r\n{{% gist 8c7235ecb75bb91833e1 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-05-19-finding-groups-consecutive-months",
        "content": "---\r\ndate: \"2014-05-19T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Finding Groups - Consecutive Months\r\n---\r\n\r\nA step by step explanation on one way to get a consecutive period of months, which could easily be adapted to days, years, or other values. I'll continue on this track and post a tutorial on eliminating overlapping dates soon.\r\n\r\n{{% gist e613ff857b5ae3ad9167 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-08-11-snippet-alert-useful-dates-(eom,-bom,-etc)",
        "content": "---\r\ndate: \"2014-08-11T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: 'Snippet Alert: Useful dates (eom, bom, etc)'\r\n---\r\n\r\nCommon date values you may need to reference that you may not want to write from scratch each time.\r\n{{% gist 5390417 %}}\r\n Hope this helps someone else!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-08-12-generate-random-date-with-starting-point",
        "content": "---\r\ndate: \"2014-08-12T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Generate Random Date With Starting Point\r\n---\r\n\r\nIf you want to create sample random samples when dealing with date calculations to test your results, you can easily create a start and end point of randomly created dates. This is a snippet I've saved for reuse:\r\n\r\nDATEADD(day, (ABS(CHECKSUM(NEWID())) % $Days Seed Value$), '$MinDate$')\r\n\r\nThis should let you set the starting point (Min Date) and choose how far you want to go up from there as a the seed value. Ie, starting point 1/1/2014 with seed of 60 will create random dates up to 60 days outside the min date specified. Stackoverflow Original Discussion: How to update rows with a random date asked Apr 27 '09\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-12-02-get-information-on-current-traces-running",
        "content": "---\r\ndate: \"2014-12-02T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Get Information on Current Traces Running\r\n---\r\n\r\nThis is just a quick informational query to save as a snippet to get some quick information on running traces, and provide the stop and close snippet quickly if you want to stop a server side trace you created.\r\n\r\n{{% gist 2f0c53641421a9825e6b %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-12-11-or-pattern-causing-indexing-scans-in-parameter-based-queries",
        "content": "---\r\ndate: \"2014-12-11T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: OR pattern causing indexing scans in parameter based queries\r\n---\r\n\r\nTl;dr\r\n\r\n(time constraints prevented me from reworking significantly)\r\n\r\nAn article on SQL-Server-pro was forwarded over to me to research by a friend who indicated that using a variable with an or pattern had historically caused table scans. This was a suprise to me as all previous queries with optional parameters I'd used in the past seemed to use index seeks. I had to dig into this a little deeper to see if had been missing this in my past work and needed to find an alternative method for optional parameters. Original test procedure copied from original article\r\n\r\n{{% gist 4f8446420776336f7478 %}}\r\n\r\n\r\nThe result from running the test1 procedure was to find a clustered index scan. SQL Server optimizer should be able to utilize the or conditions as long as an index covers the predicates, so I dug in deeper. When I ran a random query against a few tables I found this was creating table scans. Looking a little deeper I decided to evaluate the indexing on the tables to see if it was an issues with indexing, and not the pattern of using the OR against a variable.\r\n\r\n{{% gist ef2d134cdd3b47dcdf84 %}}\r\n\r\n\r\nTest 1: SCANS - except with option recompile Found index scans on all my versions of running this except when including option(recompile) inside the stored procedure statement text. This of course fixed the issue by allowing sql to build the plan based on the exact value passed in, however, this would be at the cost of increasing CPU and negating the benefits of having a cached plan ready.\r\nTest 2: Ran exec sys.sp_updatestats\r\n\r\n`text\r\nUpdating [dbo].[ortest]     [PKortest_3213E83F7953A2B2], update is not necessary...     [idxortestcol1], update is not necessary...     [idxortestcol2], update is not necessary...     [ixnc_CoveringIndex], update is not necessary...     0 index(es)/statistic(s) have been updated, 4 did not require update.\r\n`\r\n\r\nAfter researching for hours more, and reading many posts, I discovered I've been missing this in previous work, probably due to query plan caching. When utilizing the variable from the stored procedure, the parameters are \"sniffed\". This means the plan is not rebuilt for each execution, but instead first execution is cached with the values it utilized. Thereafter, the optimizer can reuse this plan. The difference is that if you provide a value you manually plug into your test such as \"declare @Value = 'FOO' \" then the optimizer has an actual value to use for each of your manually run executions. This means that if you have properly indexed the column, it would be sargable.\r\n\r\nHowever, the stored procedure is not passing in the actual value after the first run, it is trying to save the CPU demand the optimizer will need, and instead use the cached plan. This is likely the cause of my missing this in the past, as all my execution testing was based on commenting out the stored proc header and running manual tests. In this case, I'd correctly be seeing table seeks if indexed properly, because the optimizer was obtaining the actual values from my session. When executing the stored procedure externally, it looks to utilize parameter sniffing, which when working correctly is a good thing not a bug.\r\n\r\nHowever, when result sets can greatly vary, the problem of parameter sniffing can become a problem. In addition, if the OR statement is utilized as my original problem mentioned, the optimizer can decide that since parameter value is unknown at this time, that with an OR clause it would be better to run a table scan since all table results might be returned, rather than a seek.\r\n\r\nTo bypass this, there are various approaches, but all are a compromise of some sort. The common approach to resolving if truly various selections may be made (in the case of SSRS reports for example) is to utilize option(recompile). This provide the actual value back to the compiler at run time, causing higher CPU usage, but also ensuring the usage of indexes and reducing scans when the columns are properly indexed. Again, this is one solution among several, which can include building dynamic query strings, logic blocks, and other methods.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2014-12-24-dev-tools-x-yplorer-(review-1)-catalog",
        "content": "---\r\ntitle: 'Dev Tools: XYplorer (review 1) - Catalog'\r\nslug: \"dev-tools-x-yplorer-(review-1)-catalog\"\r\ndate: \"2014-12-24T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nramblings\r\n---\r\n\r\ncurrently on version 14.60 I'm a big fan of finding tools that help automate and streamline things that should are routine actions.Surprisingly, I've found it incredibly challenging to move away from the default Windows Explorer for file management, as the familiarity it offers makes it somewhat tough to be patient with learning an alternative, especially if the alternative offers more complication. That's where I stood with XYPlorer for sometime. It is a developer's tool first and foremost. It is complicated, but with this complication comes an extremely robust set of features. I honestly think that I'll never really know all of them, as this is more than just a swiss knife for file management.\r\n\r\nThis is almost like stepping from a text editor for editing code to a full blown visual studio IDE. There is just that that much to learn! Over time, I'm finding myself less frustrated by using it, and more amazed at the tweaks here and there that can be found that can greatly enhance one's file management and workflow, personal and professional. I won't cover all features, but I think instead of doing a full blown review on the product, I'm going to add some incremental reviews on features as I discover, otherwise the vast feature-set will end up causing nothing but writer's block and I'll never share anything (cause I'll be busy learning)\r\n\r\nCatalog\r\n\r\nReplaces Favorites with additional functionality The favorites section is one of my most used features in explorer. I setup the default locations I'm commonly navigating to, such as my SQL Query files location, cloud drives, temporary projects I'm working on, appdata folders I need access to occasionally, and more. XYPlorer Expands on this greatly by the concept of Catalogs. Instead of just having a shortcut, Catalogs allows one to expand the concept of shortcuts far beyond Windows Explorer (hereafter referred to as WE) and combines the favorites functionality with much more features.\r\n The Tree is an image of your computer's file system. It shows you all what's there. But, most of the time all is just too much...\r\n The Catalog is the answer: here you can grow your own personal tree. Your favorite locations are deep down in some heavily nested structures? Lift them to the surface! Side by side with locations from the other end of your hard disk. You can navigate by the Catalog (finally a one-click favorite solution!) and you can drop onto the Catalog's items.\r\n XYPlorer Help\r\n The catalog houses many categories.\r\n Each of these categories can provide various functionality beyond just linking to favorites.\r\n\r\n\r\n\r\nHere you can see applications listed directly. They provide functionality to open the app, open a file you drag onto it with the app (bypassing need to use \"open with\" dialogue)\r\n\r\n\r\n\r\nOpening the properties of a file allow one to futher edit the actions the application performs.\r\n\r\n\r\n .... to be continued.\r\n\r\nLots of functionality in the catalog to benefit from, but time is limited, I'm going to visit further in next post.\r\n\r\nnote: was given a license by developer to help me evaluate long term. This did not affect my review, as it wasn't solicited at all by XYPlorer developer.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-01-05-case-of-the-mondays...-causing-me-to-randomly-redefine-the-scope-of-global-temp-tables",
        "content": "---\r\ndate: \"2015-01-05T00:00:00Z\"\r\nslug: scope-of-global-temp-tables\r\ntags:\r\nsql-server\r\ntitle: Case of the Mondays... causing me to randomly redefine the Scope of Global\r\n  temp tables\r\n---\r\n\r\nToday, I was reminded that global temp tables scope lasts for the session, and doesn't last beyond that. The difference is the scope of the global temp allows access by other users and sessions while it exists, and is not limited in scope to just the calling session. For some reason I can't remember, I had thought the global temp table lasted a bit longer. Remembering this solved the frustration of wondering why my adhoc comparison report was empty..... #mondayfail SQLMag article I referenced\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-01-13-dev-tools-the-file-searcher-launcher-to-rule-them-all",
        "content": "---\r\ndate: \"2015-01-13T00:00:00Z\"\r\nslug: dev-tools-file-search\r\ntags:\r\nsql-server\r\ntitle: 'Dev Tools: The File Searcher/Launcher to rule them all'\r\n---\r\n\r\nWhy does this not have more recognition? In the experimentation of various file management and launching apps, I've tried several (Launchy, Listary, etc), but none have offered the speed and customization of Find and Run Robot. This app is a life saver for the power user! Here is an example of how you can have a hotkey to immediately launch a customized google search. The group alias gives you extensibility to filter the text you are typing to identify this alias of \"Google Me\" as the result to use since we typed ? as the first part of the string (that is the anchor ^).\r\n\r\n\r\n\r\nNote the encoding is handled by $$u1 for the websearch, automatically correctly encoding spaces, semicolons, and other characters.\r\n\r\n\r\n\r\nHere's the final result of what you'd start typing.\r\n\r\n\r\n\r\nThe cool part about this is the ability to not only match the initial regex, but also to filter inside this pattern to provide lists of options inside our match. In this example, I wanted to list favorite website by typing \"G\" at the beginning of the string followed by the keyword to filter my websites. This can be accomplished by anchoring the beginning of the regex filter to ^g, then filtering with the $$1.\r\n\r\n\r\n\r\nHere is the initial filtered match based only on \"G\"\r\n\r\n\r\n\r\nAnd finally the magic happens when the letters after \"g\" are parsed to get the website I want. This allows one to launch favorite websites easily, and you could even customize the url or more based on what regex magic you work!\r\n\r\n\r\n\r\nAll of these concepts apply to launching favorite apps and more. FARR2 has more customization than apps like launchy, symenu, and more, as it allows one to easily tweak the search \"score\" and add bonus points to items matching common folders or file types such as exe, xlsx, and more. Score model is pretty amazing.\r\n\r\n\r\n\r\nExample of customized options to boost certain valuable matches in search results.\r\n\r\n\r\n\r\nFinally, the killer feature for those fans of Everything search tool (Void) is the integration of the Everything search engine as an option to quickly search your entire computer in millseconds. You can easily setup a search filter with a space at the beginning so that all you have to do is type space and your search phrase and it will switch over to using the plugin search engine.\r\n\r\n\r\n\r\nWhy does this tool not get more recognization! What a life saver as you search through sql files, projects, and docs! Hope this helped point you in the direction of an amazing tool... post a comment if you try it out and tell me what you think! Find and Run Robot Help Find and Run Robot Download\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-01-16-dev-tools-farr2-launching-groups-of-files-or-apps-at-once",
        "content": "---\r\ndate: \"2015-01-16T00:00:00Z\"\r\ntags:\r\ncool-tools\r\ntitle: 'Dev Tools: FARR2 Launching groups of files or apps at once'\r\n---\r\n\r\nThere are probably a common number of apps you pull up when you pull up your system. For example, I pull up my Trello board, outlook, XYplorer, Sublime text 3, Sql server management studio, and ketarin (app updater). Found that you can easily setup a simply alias and launch a group of apps or files at anytime by simply typing the keyword.\r\n\r\n\r\n\r\nThis could easily launch a favorite group of files by adding to your startup or project list as you go. The right click on search results gives you this option on the fly.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-01-21-statistics-parsing",
        "content": "---\r\ndate: \"2015-01-21T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Statistics Parsing\r\n---\r\n\r\nNever really enjoyed reading through the statistics IO results, as it makes it hard to easily guage total impact when you have a long list of tables. A friend referred me to: http://www.statisticsparser.com/ This site is great! However, I really don't like manually copying and pasting the results each time. I threw together a quick autohotkey script that will detect your clipboard change event, look for \"scan count\" keyword, and then open a \"chrome app\", paste the results and submit. Note that I have the option \"window name enabled\" at the bottom of the textbox on the webpage. If you don't the tabcount navigation might be a little off, so tweak this if you want.\r\n\r\n{{% gist 01631b28176f71ce4789 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-04-28-restoring-a-database-that-doesn't-exist",
        "content": "---\r\ndate: \"2015-04-28T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Restoring a database that doesn't exist\r\n---\r\n\r\nWhen restoring a database that doesn't exist, say for instance when a client sends a database to you, you can't use the option to restore database, because there is no database matching to restore. To get around this you need to use the Restore Files and Filegroups option and then restore the database.\r\n\r\n\r\nAnother option I found interesting was the support for loading database hosted on a fileshare. Brentozar has an article on hosting databases on a NAS that I found interesting. I haven't tried it yet, but think it has a great usage case for dealing with various databases loaded from clients. If you haven't read any material by him... then my question is why are you reading mine? His whole team is da bombiggity.... stop reading my stuff and head on over there!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-04-28-sql-sentry-pro-explorer-is-worth-it...",
        "content": "---\r\ndate: \"2015-04-28T00:00:00Z\"\r\ntags:\r\nsql-server\r\nperformance-tuning\r\ntitle: SQL Sentry Pro Explorer is worth it...\r\nslug: sql-sentry-pro-explorer-is-worth-it\r\n---\r\n\r\n info \"Updated: 2017-04-21\"\r\n Another great bit of news from reviewing this older post I wrote... SQL Sentry Pro is now a free tool thanks to the generosity of the Sentry One team! It's a must have. Go download it for sure.\r\n\r\n info \"Updated: 2015-04-28\"\r\n I created a few autohotkey scripts and solved the problem of collapsing panes and a few other annoyances. This has improved my experience a little. - Also noted one major improvement that would help with tuning is aggregating the total IO, and stats, rather than only each individual statement. I've found the need to compare two very different plans to see the total writes/reads variation and the impact on IO, but I've having to utilize another tool for statistics IO parsing to run totals, and then come back to the SQL Sentry Plan explorer for other details. The SQL Sentry plan explorer tool could be improved by enhancing with totals/sums to better compare various runs of plans. I can make do without it, but it makes me have to do a lot of workarounds for now.\r\n\r\nI'll post more later, but after a full day of query tuning on a difficult view, I'd definitely say the cost for PRO is worth it. I'm a fan of sql sentry (free), and decided recently to push for a license at work on this tool. Turns out it was well worth it. The ability to measure variance in plans with small changes without cluttering up SSMS without 20 versions was incredibly helpful and time saving. There are a few quirks that really bother me, but not enough to negate the benefits of this tool. Perks - Save a continual session on troubleshooting a query - Evaluate Logical IO easily in the same view - Save comments on each plan version run to identify the changes you made and what impact it had Negatives - Not integrated with SSMS or preferred text editor so the text editor extremely sparse on features. - No ability to easily sum logical IO and COMPARE to another plan, really you have to open two tabs and eyeball them. That is the biggest frustration, no easy comparison side by side without opening the same session and eyeballing. - NO KEYBOARD SHORTCUTS. GEEZ is that frustrating as you are trying to power through some changes, copy cells/io, and more. Overall: Love the product. Hope they enhance the producivity and efficient aspect more as that's the only area I'm seeing it's short in. Here are some screenshots from my work with it today. I additionally compared the final IO with http://statisticsparser.com/index.html#\r\n\r\n\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-04-30-utilizing-the-power-of-table-parameters-to-reduce-io,-improve-performance,-decrease-pollution,-and-achieve-world-peace...",
        "content": "---\r\ndate: \"2015-04-30T00:00:00Z\"\r\ntags:\r\nsql-server\r\ndeep-dive\r\ntitle: Utilizing the power of table parameters to reduce IO, improve performance, decrease pollution, and achieve world peace...\r\nslug: utilizing-the-power-of-table-parameters-to-reduce-io,-improve-performance,-decrease-pollution,-and-achieve-world-peace\r\n---\r\n\r\nI was dealing with a challenging dynamic sql procedure that allowed a .NET app to pass in a list of columns and a view name, and it would generate a select statement from this view. Due to requirements at the time, I needed the flexibility of the \"MAIN\" proc which generated a dynamic select statement, while overriding certain requested views by executing a stored proc instead of the dynamic sql.\r\n\r\nDuring this, I started looking into the string parsing being completed for a comma delimited list of numbers to lookup (the primary key). I figured I'd explore the benefits of the user defined table and pass through the list of ids from the .NET application with a table parameter instead of using comma delimited list. Some great material\r\n\r\nI came across indicated the overhead might be a little more client side, but that the benefits to cardinality estimation and providing SQL Server a table to work with can far outweigh the initial startup cost when dealing with lots of results to join against. The main area I wanted to address first, that I couldn't find any clear documentation on was the memory footprint. I saw mention on various sources that a TVP can have a lower memory footprint in SQL Server's execution due to the fact as intermediate storage it can be pointed at by reference, rather than creating a new copy each time, like when working with parsing into another variable using comma delimited lists.\r\n\r\nI get that passing the stored proc a table variable means it's working with provided object, but what about the portability of this object? In my case, there are at least 2 levels being worked. The MAIN proc and the CHILD proc. The child proc needs access to the same list of ids. The dynamic statement in the MAIN proc also needs the list of ids. Currently it was creating the list of ids by inserting into a table parameter the delimited list of values.\r\n\r\nCould I instead consider passing the actual table parameter around since it's by a readonly object and hopefully keep referring to it, instead of having separate copies being created each time. This could reduce the IO requirements and tempdb activity by having a single TVP being used by the MAIN and CHILD procs.\r\n\r\n\r\n\r\n\r\nSummarized IO:\r\n\r\nThe footprint is reduced when dealing with IO from the child statement, because it keeps pointing to the same in memory object. I also validated this further by examining a more complex version of the same query that compares the comma delimited list against executing a nested stored procedure, which in turn has dynamic sql that needs the table parameter passed to it. The results of the review show successfully that it keeps pointing to the same temp object!\r\n\r\n\r\nIn summary, the table valued parameter can end up being pretty powerful when dealing with passing a list of values that may need to be referenced by several actions or passed to nested procs (not that this is the best practice anyway). Disclaimer: this is working with the constraints of what I have to release soon, so not saying that nested procs with dynamic sql in both MAIN and CHILD are a great practice, but sometimes you gotta do what you gotta do!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-05-04-upgrade-from-sql-2014-evaluation-to-developer-edition",
        "content": "---\r\ndate: \"2015-05-04T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Upgrade from SQL 2014 Evaluation to Developer Edition\r\n---\r\n\r\nCouldn't find documentation showing that upgrade from SQL 2014 evaluation version was possible to developer edition. I just successfully converted an evaluation version to developer edition.\r\n\r\nObtain the key for the developer edition (in my case I had to download the .ISO from MSDN downloads, and go through the installation wizard to get it)\r\nRun the installation center app for SQL Server 2014\r\nSelect edition upgrade\r\nPlug in your new serial from the developer edition. Pretty simply, but thought I'd post a confirmation for anyone wanting confirmation the upgrade path was an option for developer. Probably somewhere in the MSDN documentation, but I couldn't find it with a quick search.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-05-22-enabling-instant-file-initialization",
        "content": "---\r\ndate: \"2015-05-22T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Enabling Instant File Initialization\r\n---\r\n\r\nFound a couple good walkthroughs on enabling instant file initialization. However, I'm becoming more familar with the nuances of various setups and found it confusing in trying to map the correct user/group to enable this option. In my case, I had the SQL Service running under NT SERVICE/MSSSQLSERVER and as such this logic wasn't showing up when trying to find groups/users to add to the necessary permissions. Lo and behold...\r\n\r\nI typed it in manually and it worked. If time permits I'll update the article later with a more technical explanation, but as of now, this is just a quick functional post to show what resolved the issue. Add the service account or group (whatever you have sql server in) to the perform volume maintenance privileges in the local security policy.\r\n\r\n\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-06-24-some-simple-examples-of-querying-xml-with-sql",
        "content": "---\r\ndate: \"2015-06-24T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Some simple examples of querying xml with sql\r\n---\r\n\r\nXML is a beast if you've never tackled it. Here are some simple examples of what I discovered as I experimented and walked through obtaining values out of a XML column.\r\n\r\n{{% gist 7f66126b19a454920f2e %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-07-13-what-was-i-thinking-deleting-myself-from-localdb",
        "content": "---\r\ndate: \"2015-07-13T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: What was I thinking? Deleting myself from localdb?\r\n---\r\n\r\nWas testing a batch file to add a user to a localdb instance. Assumed that my user as admin on the machine wouldn't have an issue inserting myself back.... didn't think that one through too carefully. Executing any type of SQLCMD against it denied me. SSMS denied me. No SA had been setup on it, so I couldn't login as SA either. Looked for various solutions, and ended up uninstalling and reinstalling (localdb)v11.0 so that I'd stop having myself denied permissions.\r\n\r\nThis however, didn't fix my issue. The solution that ended up working from me came from dba.stackstackexchange.\r\n\r\nI ended up deleting everything in the v11.0 Instances folder and then issuing the following command sqllocaldb.exe c v11.0\r\n\r\nResulting in message: LocalDB instance \"v11.0\" created with version 11.0.3000.0.\r\n\r\nSuccess! This resulted in the instance being created successfully, and then I was able to login with SSMS. Apparently today was my day for learning some localdb permissions issues. What a blast..... Could have avoided this if I had simply used a test login, or had setup the SA with a proper password for logging in. #sqlfail\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-07-28-database-stuck-in-single-user-mode-due-to-botched-restore",
        "content": "---\r\ndate: \"2015-07-28T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Database Stuck in Single-User Mode Due to Botched Restore\r\n---\r\n\r\nWorking in a development environment, I botched up a restore. After this restore attempt to overwrite my database with the previous version, I had it stuck in single-user mode.\r\n\r\nSSMS provided me with helpful messages such as this:\r\nChanges to the state or options of database 'PoorDb' cannot be made at this time. The database is in single-user mode, and a user is currently connected to it.\r\nAdditionally, I was told I was the deadlock victim when attempting to set the user mode back to multi-user.\r\n\r\nGoing forward I looked at several articles from Stack Overflow and various other blogs, and followed the recommended steps such as\r\n\r\n{{% gist 97c73c8ef61c84e6adbb %}}\r\n\r\nI even added a step to kill the connections to it by using this statement, helpfully posted by Matthew Haugen\r\n\r\n{{% gist 252ca75b8e8ab4fe64fa %}}\r\n Finally went through and removed all my connections from master based on an additional post. No luck. Stopped my monitoring tools, no luck. At this point, it felt like a Monday for sure.\r\n\r\nSince I was working in a development environment, I went all gung ho and killed every session with my login name, as there seemed to be quite a few , except for the spid executing. Apparently, the blocking process was executing from master, probably the incomplete restore that didn't successfully rollback. I'll have to improve my transaction handling on this, as I just ran it straight with no error checks.\r\n\r\nVICTORY!\r\nWhat a waste of time, but at least I know to watch out next time, ensure my actions are checked for error and rolled back.\r\nI'm going to just blame it on the darn SSMS GUI. Seems like a convenient scapegoat this time.\r\n\r\nSuccessful pushed out my changes with the following script:\r\n{{% gist a3db2c337d8e5d4f67a7 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-05-running-very-large-scripts-is-not-a-strong-area-for-ssms",
        "content": "---\r\ndate: \"2015-08-05T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Running very large scripts is not a strong area for SSMS\r\n---\r\n\r\nout of memory, argggh!\r\n\r\nAm I the only one that has experienced the various out of memory issues with SSMS? Not according to google!\r\n\r\n lovingly crafted in the forges of.. well ... dbforge\r\n\r\nI've a huge fan of Devarts products. I've done a review in the past on their SQL Complete addin, which is the single most used tool in my SQL arsenal. It vanquishes nasty unformatted code into a standard lined up format I can easily read. The 100's of options to customize the formatting make it the most customizable formatter I've found for SQL code.\r\nThis SQL Complete however, is a plugin for SSMS. It is native in their alternative to Sql Server Management Studio, dbForge Studio. Highly recommend checking this out. It's affordable, especially if you compare against other products that offer less.... and they have a dark theme muaaah!\r\n\r\nexecute script that is far too large\r\n\r\nI'll post up more detail when time permits on some of the other features, but one noticeably cool feature is the \"execute large script\" option.\r\n\r\n\r\n\r\n\r\nYou can see the progress and the update in the output log, but the entire script isn't slowing down your GUI. In fact, you can just putter along and keep coding.\r\n\r\n\r\nOther options to accomplish the same thing include executing via SQLCMD, powershell, or breaking things up into smaller files. This just happened to be a pretty convenient option!\r\n\r\n Have I switched?\r\n\r\nI haven't switched to using it as my primary development environment because of 2 reasons. Extensions... I do have quite a few that work in SSMS like SSMS Tools, SSMS Toolpack, and some Red Gate functionality. I lose that by switching over to dbForge Studio. Also, some of the keyboard shortcuts like delete line and others I'm so used to aren't in there. Regretably, they don't support importing a color scheme from visual studio, so you lose out on sites like https://studiostyl.es/\r\nOther than a few minor quibbles like that I'm pretty happy with it. They've done a great job and the skinning of the app is great, giving you the option of dark or light themes.\r\n\r\nDevart apps provided to me for my evaluation, but are not biasing my recommendation.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-07-set-noexec-is-my-new-friend",
        "content": "---\r\ndate: \"2015-08-07T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: SET NOEXEC is my new friend\r\n---\r\n\r\nHave never really played around with the option: SET NOEXEC ON\r\nTurns out this can be a helpful way to validate larger batch scripts before actually making changes, to ensure compilation happens. If you choose, you can verify syntax by \"parsing\" in SSMS. However, this doesn't compile. Compilation checks more than your syntax. It actually validates the objects referenced exist.\r\n\r\n The execution of statements in SQL Server has two phases: compilation and execution. This setting is useful for having SQL Server validate the syntax and object names in Transact-SQL code when executing. It is also useful for debugging statements that would generally be part of a larger batch of statements. MSDN #188394\r\n I previously had done this basic verification by running an estimated execution plan. This had the benefit of finding compilation errors as well. However, with large batch jobs it could be problematic and slow, as it had to process and return all execution plans for the statements, which I didn't need.\r\n Having this function can be a nice resource for validation of scripts before running, without the overhead of estimated execution plans.\r\n Additionally, you can handle transactional rollbacks and prevent cascading problems from happening when running a batch that changes your database by setting NOEXEC ON when encountering an error. Red Gate SQL Compare does this elegantly:\r\n \r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-10-using-qure-profiler-to-benchmark-tuning-progress",
        "content": "---\r\ndate: \"2015-08-10T00:00:00Z\"\r\nlastmodifiedat: \"2018-03-24\"\r\ntags:\r\nsql-server\r\ntitle: Using Qure Profiler To Benchmark Tuning Progress\r\ntoc: true\r\n---\r\n\r\nQure Analyzer Benchmark Testing\r\n\r\n info \"Update\"\r\n Wouldn't recommend tool at this time. Development seemed to cease, resulting in me not being able to use with later versions of SQL Server. When I came back recently to check on it, the app was sold and it was with a new company. Didn't see to have progressed much based on a quick look and not really interested in it at this point. Instead, other solutions like the new DEA (Database Experimentation Assistant from Microsoft) would be where I'd spend more effort.\r\n\r\n warning \"Updated: 2018-03-18\"\r\n\r\n the problem of monster views\r\n\r\nI've been working with tuning an application that performs a lot of Entity framework calls, as well as stored procedure activity. The stored procedures could be processing a large amount of data, or a small amount, depending on the configuration of the client. The major issue was the source of the data for the client application was a view with 20+ joins involved. In breaking down the logic, most of the joins really functioned as just adding additional columns of data. The logical grouping mean that mostly 2-3 tables at a time joined to provide a set of columns based almost exclusively on the source table.\r\nI needed to get away from this pattern, as I was finding tremendous issues with cardinality estimation for SQL Server, resulting in sometimes 20GB nested loop join worktables by the optimizer.\r\n\r\nSimplify\r\n\r\n the incredible masterful... well... actually quiet simple solution... simplify!\r\n\r\nMy proposed solution was to break up the large view into small pieces, which I identified by logically grouping the joins/related columns. Instead of 20+ joins to return the columns needed, I'd instead provide 10 separate selects of 2 tables or so in each query. These would be processed as a dataset on the client, instead of returning a single large datatable.\r\nThe next issue was to identify the improvements based on evaluating the possibility of larger amounts of base accounts pulled at once with the new format, vs the smaller batch size the large view required to function at all.\r\n\r\nfirst approach was statistics io & Sql stress\r\n\r\nUsing the SQL Stress tool along with evaluating the statistics io, time was the my first course of action. However, the problem I ran across was really that I needed to run this dozens of times in a row, and evaluate the impact on the client performance as well. SQL stress provided a great way to run the query manually, but with the input variables from the app, I really wanted to run the app as the end user experience, and request the batch job on the largest amount of rows I could get. This way, I truly was matching all the input from the app, and getting timings from this.\r\nIn addition, I was looking for a good way to evaluate the workloads against each other.\r\n\r\n the right tool for the analysis = Qure Profiler\r\n\r\nI'd used ClearTrace before, and found it helpful, but hard to consume some comparison type reports easily. I needed something to help me identify the improvement or degradation of performance and Qure Analyzer solved this for me, turning something very messy into a much simpler proposition (which also helped with my end research presentation to the big kahonas).\r\nDbsophic Qure Profiler has had some great reviews for assisting in database profiling, but I haven't had a chance until recently to fully dive into using it. Since I was doing batch testing, I figured now would be a great time to re-review it. The numerous variables at play made this pretty complex to really quantify any improvement based on a single run alone.  So, I brought back up Qure Profiler to help me out.\r\nNecessity is the mother of experimentation.\r\n\r\nInitial Performance Evaluation Run (Batch level analysis)\r\n\r\nEvaluated runs on the largest current grouping. This was 1232 base accounts. I would later evaluate with larger batch sizes, but started simple.\r\n\r\n\r\n\r\n Actual Execution Details with Qure Profiler\r\n\r\nTested Again With QURE Profiler set to minimum batch info. I also ran DBCC free proc  cache to attempt to better ensure the plan was correctly rebuilt for the new counts, and that it was a fresh start in the comparison as far as impact on the disk IO.\r\n\r\n\r\n\r\nComparing 100 to larger batch sizes in the base account request\r\n\r\nThis final comparison shows 42% improvement by using 500 rows at a time. This seemed to be a good compromise at this point to increase batch sizes, while still maintaining lower logical reads. Next step was to test against a larger database to evaluate scalability.\r\n\r\n\r\n\r\n Actual Execution Results on Larger database\r\n\r\nEvaluating against a database with about 500GB of data, I found the best execution time seemed to be the base account count (tongue twister) seems to be the 1000 batch size at this time. It is returning the results in the shortest duration and the lowest impact on reads. FREE PROC CACHE COMPLETED ON EACH STEP\r\n\r\n\r\n\r\nQure Profiler workload comparison\r\n\r\nSet the baseline as the 100 batch size (which is the current way of processing the request). Qure provided an extremely powerful side by side comparison of both of the workloads. The best value is still the 1000 batch size, showing that the logical reads at point.\r\n\r\n\r\n\r\n Comparing to the original methodology of a huge view with 20+ joins\r\n\r\nAgainst two databases, both performed signficantly better with the simplified approach. One database was much faster, while another that was completely timing out with 20GB+ nested loop join worktables finally ran without incident. Major win!\r\nThe changes look to have positively improved the performance overall for both databases representing two much different usage patterns (and data stored in it)\r\n*When comparing new (1000) simplified procs vs 100 on original views it showed: *\r\n27% shorter per execution\r\n77% less cpu impact\r\n81% less logical read impact\r\n\r\n\r\n\r\nAdditional functionality I haven't even touched on\r\n\r\nQure Profiler offers additional functionality that I haven't even gotten a chance to touch on. It can normalize the count of events to compare an even 100 against another 100 even when the second trace might have run longer and caught the event more than the 100 times. Check out their page for more details.\r\n\r\n TL;DR Summary\r\n\r\nLong post. If you read this far, I commend you for either being a great page scroller, or for being tenacious and finding all my graph's super interesting. If that's you, you get #SqlCred\r\nI'm finding that with workload testing, Sql server workload comparison is one of the harder things to do right. There are a lot of variables to take into consideration, and even when doing a simple test on batch size changes like I did, aggregating the comparison results into usable statistics and in a presentable format can be daunting.\r\nHighly recommend the dbsophic product for a great utility to help save a lot of time in doing this comparison.  This tool goes into my top SQL server performance tuning tools for sure. I'm going to be evaluating their more advanced Qure Optimizer soon as well, as it might help identify other issues I've missed on tuning by evaluating adjustments against a test copy of a database.\r\n\r\nAre there any cons?\r\n\r\nI didn't run into any significant issues that impacted my ability to use it. I do think there were a few stability issues that I had to work around by restarting the app a few times, but for the time it saved, I wasn't complaining too much.\r\nNeed to offer option to use extended events. This is on their roadmap apparently. Xevents should help lower the impact from doing the profiling on the workload being tested.\r\nSome app errors related to closing workload comparison tabs. I reported this bug and hopefully they'll continue to improve the application.\r\nMaybe some GUI improvements to make it more in-line with modern UI standards? Just a personal preference. I didn't find the tab based comparison approach the most intuitive. Not terrible, but would be a nice improvement in the future.\r\n\r\n What could they improve?\r\n\r\nOffer comparison report option that wouldn't require as many screenshots, and instead summarize the selected items in a pane that you could copy to image. This would be slick!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-12-red-gate-sql-source-control-v4-offers-schema-locks",
        "content": "---\r\ndate: \"2015-08-12T00:00:00Z\"\r\ntags:\r\nsql-server\r\nredgate\r\ncool-tools\r\ntitle: Red Gate SQL Source Control v4 offers schema locks\r\n---\r\n\r\nRed Gate Documentation Update\r\nLooks like the rapid release channel now has a great feature for locking database objects that you are working on. Having worked in a shared environment before, this could have been a major help. It's like the poor man's version of checking an object out in visual studio except on database objects! With multiple developers working in a shared environment, this might help reduce conflicting multiple changes on the same object.\r\nNote that this doesn't look to evaluate dependency chains, so there is always the risk of a dependent object being impacted. I think though that this has some promise, and is a great improvement for shared environment SQL development that uses source control.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-13-stranger-danger...-the-need-for-trust-with-constraints",
        "content": "---\r\ndate: \"2015-08-13T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Stranger Danger... The need for trust with constraints\r\n---\r\n\r\nI ran into an issue with errors with an database upgrade running into a violation of a foreign key constraint. Don't know how it happened. Figured that while I'm at it, I'd go ahead and evaluate every single check constraint in the database to see if I could identify any other violations, because they shouldn't be happening.\r\n\r\nimprove the execution plan by checking the data\r\n\r\nIn my reading, I found out that checking the constraints can enable the constraint to be marked as trusted. The trusted constraints are then able to be used to build a better query plan execution.\r\nI knew that constraints could help the execution, but didn't know that they could have a trusted or untrusted trait.\r\n\r\n Brentozar to the rescue\r\n\r\nI'm serious, this guy and his team are awesome. This one single team and their web resources have single handled helped me gain more understanding on SQL server than any other resource. I love how they give back to the community, and their communication always is full of humor and good examples. Kudos!\r\nAnyway, commendation aside, the explanation from sp_blitz was fantastic at summarizing the issue.\r\n\r\n After this change, you may see improved query performance for tables with trusted keys and constraints. - Blitz Result: Foreign Keys or Check Constraints Not Trusted\r\n   As the site further mentions, this can cause locks and performance issues, so this validation might be better done off hours. The benefit might be worth it though!\r\n\r\nmy adaption of the check constraint script\r\n\r\nI appreciate the script as a starting point (see link above). I adapted to run this individually on each check constraint and log the errors that occurred. This runs though all FK and CHECK constraints in the database you are in, and then checks the data behind the constraint to ensure it is noted as trusted.\r\n\r\n{{% gist 2454ce9134eac225ce264c64adb331a9 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-18-monitoring-sql-server-on-a-budget",
        "content": "---\r\ndate: \"2015-08-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Monitoring SQL Server on a budget\r\ntoc: true\r\n---\r\n\r\nCheap ain't easy\r\n\r\nThere's a lot of tools out there, and very few that are polished, have a good UI, and some reasonable functionality to help monitoring, that don't cost an arm and a leg. One such tool I've recently begun to appreciate is MiniDBA . I was generously provided with a license to evaluate this and continue testing, and have recently had an actual chance to start using it more in my environment. The cost for MiniDBA is one of the most affordable I've found for a live monitoring tool with a good UI design (eye candy is critical for monitoring a server as we all know  )\r\nAt the time of this post's original date, there is a simple free version for monitoring a single instance on the machine running. This free version is awesome if you have a VM running full-time, as you could have it stay running and monitor the instance you care about.\r\nPaying $50 for developer and $100 for Enterprise gives you more flexible management with alerts, multiple servers, and a service to collect the data instead of having to run the GUI app the whole time.\r\n\r\n wait stats\r\n\r\nWait stats are the first place to typically go to when analyzing the delays a server may face. MiniDBA offers a few cool ways of looking at the data, including getting the diff on waits since the point in time you started looking at it, helping isolate the waits that really matter to you right now.\r\n\r\n\r\n\r\nget alerts on critical server issues\r\n\r\nI'd love to see this more extensible/customizable, but it's a good start. The time \"resolved\" would also be great when reviewing the history to be able to see how long before an issue was resolved.\r\n\r\n\r\n\r\n general healthcheck on \"best practices\"\r\n\r\nAgain, some really cool stuff in here. I'd love more customization opportunity to actually expand or customize these as I have a boatload of custom DMV's for evaluating best practice setup conditions on a SQL server. It would be great to extend this more.\r\n\r\n\r\n\r\nactive connections\r\n\r\nPretty straightforward, but one plus is it offers ability to view the execution plan for each SPID, potentionally helping save a few steps. Note the execution plans are not shown at the server level \"SQL tab\", but at the database level. This reminds me of a less thorough \"sp_whoIsActive\".\r\n\r\n\r\n\r\n other features\r\n\r\nThere are features to look at like:\r\ntable sizes\r\nindex sizes\r\nfiles in the database\r\nmemory\r\ndefault trace\r\nlast 3000 transaction log entries\r\nlocks on objects.\r\n\r\nvisual monitoring\r\n\r\nThe key of course for a great monitoring tool is not just a bunch of text data thrown at you, but a great visual representation of various facts so you can easily identify something wrong. I think the developer did a great job in providing a useful \"dashboard\". I think more customization or ability to look at a point in time more specifically would be great (like SqlSentry offers) but at the same time, the scope of the MiniDBA project seems to focus on simplicity, and not offering so much that it becomes complicated.\r\nI'd say for the price, the value is pretty good for a team looking for a simple tool with a few visual ways of looking at the performance, while still giving some active connection monitoring. Again, there's a lot of other options out there for monitoring, even the built in functionality. But for value, this is a pretty good option, as it seems to focus on simplicity, usability, and not being a $1000+ per server license.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-08-27-brentozar's-training-chocolate-&-cowboy-hats-included",
        "content": "---\r\ndate: \"2015-08-27T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: 'Brentozar''s Training: Chocolate & Cowboy Hats Included'\r\n---\r\n\r\nIt was entertaining to listen to a technical wizard fighting the obsession with waiting to eat chocolate with 3k viewers watching. Kendra wore about 4-5 cowboy hats in an effort to help those of us who wear many hats feel welcome.... Now that's the kinda of training I enjoy! No pretense, just honest real, and insightful training with enjoyable humor included for free\r\n\r\nHighly recommend attending the webex presentations occurring today and tomorrow with Brent Ozar and his amazing team. They are giving back to the community with some excellent training and Q&A. Attended the Shared Storage discussion yesterday and wasn't disappointed (I don't think they've ever disappointed with their training!)\r\nRegister for the training now, as it is the second of the 3 days, with the first training occurring here @ 10am CST.\r\nBrentozar Event Registration\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-09-10-multi-cursor-editing-sql-feels-like-the-movie-inception-just-became-real",
        "content": "---\r\ndate: \"2015-09-10T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Multi-Cursor Editing SQL feels like the movie Inception just became real\r\n---\r\n\r\nYes... multicursor editing is epic\r\nNo... SSMS doesn't support multi-cursor editing the way it should.\r\nYes... you can do some basic editing with multiple lines using alt-shift\r\nNo... it doesn't come close to what you can do with Sublime.\r\nCool thing is you can open text in Sublime synced w/SSMS cursor position and switch between the two with a shortcut. That will be a post for another day, I'm just telling you now to get your appetite going.\r\nIf you can't tell, I love shortcuts. Sublime + AHK pretty much covers most text editing needs you'll ever have.\r\nFeel free to send me a check for all the time you'll save from my revelation.\r\nI apologize in advance for the video quality. I plan on recording a better one in the future, after I wrap my head around the awesomeness of Camtasia Studio 8\r\n\r\niframe allowfullscreen=\"yes\" frameborder=\"0\" height=\"480\" src=\"https://www.youtube.com/embed/1YF0XphEd04?rel=0\" width=\"640\"/iframe\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-09-14-split-personality-text-eiting-in-ssms-with-sublime-text-3",
        "content": "---\r\ndate: \"2015-09-14T00:00:00Z\"\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: Split personality text editing in SSMS with Sublime Text 3\r\n---\r\n\r\nMy preview post showed a demonstration of the multi-cursor editing power of Sublime Text 3 when speeding up your coding with SQL server.There is a pretty straight forward way to setup sublime (or one of your preferred text editors) to open the same file you are editing in SQL Management Studio without much hassle. I find this helpful when the type of editing might benefit from some of the fantastic functionality in Sublime.\r\n\r\nExternal Tool Menu\r\n\r\nGo to Tools  External Tools\r\n\r\n\r\n\r\n Setup Sublime Commands to Open\r\n\r\ntable data-preserve-html-node=\"true\"\r\ntbodytr data-preserve-html-node=\"true\"\r\nth data-preserve-html-node=\"true\"Setting/th\r\nth data-preserve-html-node=\"true\"Value/th\r\n/tr\r\ntr data-preserve-html-node=\"true\"\r\ntd data-preserve-html-node=\"true\"Title/td\r\ntd data-preserve-html-node=\"true\"Edit in Sublime/td\r\n/tr\r\ntr data-preserve-html-node=\"true\"\r\ntd data-preserve-html-node=\"true\"Command/td\r\ntd data-preserve-html-node=\"true\"C:\\Program Files\\Sublime Text 3\\sublime_text.exe/td\r\n/tr\r\ntr data-preserve-html-node=\"true\"\r\ntd data-preserve-html-node=\"true\"Arguments/td\r\ntd data-preserve-html-node=\"true\"$(ItemPath):$(CurLine):$(CurCol)/td\r\n/tr\r\ntr data-preserve-html-node=\"true\"\r\ntd data-preserve-html-node=\"true\"Initial Directory/td\r\ntd data-preserve-html-node=\"true\"$(ItemDir)/td\r\n/tr\r\n/tbody/table\r\n\r\nLimitation: Unsaved temporary files from SSMS are empty when you navigate to them. If you save the SQL file you will be able to correctly switch to the file in Sublime and edit in Sublime and SSMS together.\r\n\r\nImportant:\r\nOne thing I personally experienced that wasn't consistent was the handling of unsaved files. If the file is SqlQuery as a temp file that hasn't been saved, then this opening didn't work for me. Once I had the file named/saved, it worked perfectly, even bringing the cursor position in Sublime to match what was currently in SSMS.\r\n\r\n\r\n\r\nRefresh File3\r\n\r\nTools  Options  Environment  Documents\r\nYou can setup the auto-refresh to be in the background if you wish, or manually select the refresh from SSMS when it detects the change. If the auto-refresh happens while you are editing sometimes it caused me to have redo some work (or control-z) in Sublime, but for the most part it's pretty seamless.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-09-21-xml-attribute-vs-element-assignment-when-working-with-sql",
        "content": "---\r\ndate: \"2015-09-21T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: XML Attribute VS Element Assignment when working with SQL\r\n---\r\n\r\nXML handling with aliases\r\n\r\nI find it interesting the difference in behavior with querying XML between column assignment, and quoted alias naming.\r\nIt's a generally understood best practice to not use the deprecated syntax of column aliasing using a quoted name. For example:\r\n\r\n`\r\nselect\r\n    [escapedWithBracketsIsGood] = case when raining then cats else dogs end\r\n    ,NoEscapeNeededPerGoodNamingPractices = case when rains then pours else friday end\r\n    ,case when writtenThisWay then ICringe else UseAssignmentWay end as NormalWayMostFolksDoIt\r\n    ,'BadNaming' = case when food then eat else hungry end\r\n    ,case when work then eat else JobLess end as 'VeryBadNaming'\r\n`\r\nYou can see the difference in naming. The bottom too are deprecated syntax from older naming standards, and should be avoided typically. Aliasing is pretty straight forward and the variance in doing your alias pattern doesn't normally have an actual impact on the results or way of handling...\r\nExcept when doing with XML apparently\r\n\r\n{{% gist aae7d80467d37b89dc15 %}}\r\n\r\n\r\n\r\n\r\n Further reading\r\n\r\nMSDN covers in more detail and precision in the this page\r\nThe handling of XML is a newer area to me, as previously I've avoided like the plague. However, in working w/apps sometimes creating XML configuration files might be useful so exploring this can helpful to understand how to manipulate and even obtain values from it.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-09-23-dynamic-sql-&-quotename",
        "content": "---\r\ndate: \"2015-09-23T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Dynamic SQL & Quotename\r\nurl: dynamic-sql-quotename\r\n---\r\n\r\nNot quite fineprint, but sure feels like it!\r\n\r\nQuotename can be a pretty cool function to simplify your dynamic sql, as it can ease some of the escaping of strings.\r\nHowever, I normally use it for table/column names, and so hadn't ran into a \"gotcha\" of this function until today.\r\nIt's limited to 128 characters, and if you pass in greater than 128 characters will yield a null.\r\nYep... you could be trying to track down that error for a null string somewhere in your concatenation for a while... only to find out this silent error is occurring.\r\nI'd like to thank NoSqlSolution for mentioning this and helping me go back to the other window I had open and rereading it.... I guess sometimes it pays to read the darn BOL.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-10-14-best-tools-for-taking-notes-in-development",
        "content": "---\r\ndate: \"2015-10-14T00:00:00Z\"\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: Best Tools for Taking Notes In Development\r\ntoc: true\r\n---\r\n\r\n info \"Updated: 2016-04-08\"\r\n Been working through some issues with Clarify app on my personal computer, something I messed up. @trevordevore with @clarifyapp has worked step by step through it with me providing some great personal assistance. In addition, he's given me some tips about my research in merging in a php script to automatically upload my images to cloudinary for hosting. I just finished producing some documentation with Clarify 2, and I have to say it's one of the best tools I can think of for producing a stellar documentation without requiring the user to spend time doing much formatting. I think the points in this post still remain, I wish they'd continue to expand it a little more. Maybe an extension that replicates a library of screenshots like Snagit, or something else for quick ad hoc annotation and pasting would be welcome. Overall, these are minor concerns with the great results the app already produces.\r\n\r\nDevelopers have a lot to juggle.\r\n\r\nDo a few of these things sound familiar:\r\n\r\nJuggle current sprint tasks\r\nResolve outstanding issues needing immediate attention\r\nResearch/Professional Development\r\nLong term development objectives that you can only get fragments of time to work on\r\nPerformance testing - and being able to remember numbers/figures with all the moving pieces\r\nWith all the time left blog, write an insightful blog post that will be hailed by millions\r\n\r\nI've been on a long quest to evaluate the best tool to help me personally document and stay up to date with some of these tasks. Documenting and task management can be easily overlapped. For the purpose of my notes, I'm going to focus primarily on the documenting aspect. Whether you use Onenote, Trello, word, or any other system, the most important thing is an easy way to recap and evaluate all the work you've done\r\n\r\nA few goals for a best fit\r\n\r\nScreenshots are a key for me (see next section if you wonder why)\r\nAnnotation of screenshot images are a must\r\nEasily able to copy and paste to reuse the material or export in a way that others could consume\r\nBasic authoring, nothing fancy needed, just basic bold/italics, etc. Something to help visually organize, as raw text can be hard to skim through if a lot of content.\r\n\r\n Why do care so much about screenshots?\r\n\r\nThe problem with solely using text for content is the variety of content we run across.\r\n\r\nFor instance, when I'm working on performance tuning a database, I will have text based details from SSMS, a grid of results that may be relevant. I may utilize a performance tuning tool like Qure Profiler, or find a diagnostic graph from perfmon that I want to preserve. The variety of formats is daunting to try and capture in a usable format into a documentation tool without considering screenshots. Since the other tools present information in a usable format, but not always exportable format I can use a screenshot to capture the data I reviewed without spending time trying to get things just right.\r\n\r\nI also find visually walking through a problem easier to remember when re-reviewing in the future. Your mileage may vary.\r\n\r\nTools I Evaluated\r\n\r\nDisclaimer: Please note that most of these tools I received a free license for indicating I'd review. I DO NOT base recommendations on these. A piece of crud is a piece of crud regardless of it it is free. I normally like to review products for an extended period before writing about any of them\r\n\r\nMy Final Review Rating after intense self examination, contemplation of my navel for hours on end, and the hope of lucrative contracts becoming a professional software reviewer....\r\n\r\nEpic: Highly Recommended\r\n\r\nClarify 2\r\nSnagit\r\nCamtasia Studio\r\n\r\nClose: Good product\r\n\r\nGreenshot1. Great free screenshot utility. Best general screenshot tool besides Snagit I think I've found.\r\nOneNote\r\nGreat for OCR scans of text, but basically useless for anything relating to annotations/commenting on an image.\r\n\r\nBenched: Not a good fit for me *not reflective of all users\r\n\r\nAshampoo Snap 8\r\n\r\n        UI was ok, but found it lacking with keyboard shortcuts and power user usage. The annotations were very dated and seemed very cartoonish for the most part, so I wasn't very happy with the end result. I think the app is a good one, just needs some redesign on a few elements for making it cleaner and modernized. The canvas expansion was very unintuitive as well, making it unfriendly for merging several screenshots together.\r\n\r\nScreenpresso\r\n\r\n        organizes by folder/project, which was a cool concept. Overall very limited and nothing that really stood out as a paid product.\r\n\r\nShareX\r\n\r\n        Very extensible. One of my favorite features was the on-screen annotations, which let you capture an area on the screen, and do basic annotations on it without any apparent GUI. Nice! Overall I can't recommend due to the learning curve and complexity. It's extremely powerful, but that comes at the cost of being very complex to configure and get working. Documentation was ok, but still for some more advanced setting up of custom share destinations I found it difficult to find help.\r\n\r\nActivePresenter (free version)\r\n\r\n        Great capturing of screenshots and mouse movements into training video based on detecting movement/actions. This was a great find, however, I can't recommend it for any but those with no work budget, as it's extremely clunky for usage relating to development/notes/personal workflow. It's very busy and hard to find settings to configure, but then again... free!\r\n\r\n Epic: Highly Recommended\r\n\r\nAll 3 of these products are commercial. All three have solid functionality, and I really find them to be beneficial to my development process. They each have a different scope in what they accomplish so I could see myself using each of them in various ways from this point forward.\r\n\r\nClarify 2\r\n\r\nThis was a gem I discovered and initially passed over. The site documentation focused a lot of documenting steps for tutorials/help, and I went ahead and tried to see what it would be like as it looked polished.\r\nSo far, my favorite discovery in documenting/notes for development and blogging. This tool has the polish of a well design tool, while still having some powerful extensibility. I think it focuses very well on one thing which is documenting step by step work. This ties directly in with blogging and tech documentation. In a blog, I'm covering specific areas, and organizing my thoughts in blocks.\r\n\r\nOn the tech side, when I'm evaluating a SQL performance problem or tuning effort, I systematically am going through comparisons of before and after, impact assessment, and identifying the improvements against baseline. This tool allows this workflow perfectly.\r\n\r\nThe export options are pretty polished. I did a quick walk-through for a issue at work and got several complements about the polished documentation I produced. Best part is this took minimal effort.\r\n\r\nPros\r\n\r\nAbsolutely intuitive and clean\r\nLack of over complication with unneeded features. Instead, it focuses on usability.\r\nThe annotation style is my favorite, very modern and minimal, not like some that use more cartoonish annotation styles.\r\nRounding of images on canvas as a default, looks polished.\r\nEasy manipulation of canvas, no complication in trying to add multiple images\r\nFantastic export options. Polished PDF, copy to clipboard as rich text, _EXPORT TO WORDPRESS _this article was written with this tool entirely*\r\nCons\r\nKeyboard shortcuts are lacking\r\nCould really use with an extension of functionality to just capture single screenshots and annotate like snagit. As it stands, the tool is focused more on the documentation aspect, but with some enhancements it would work as a screenshot annotation tool. Sort of like a single step in their documentation tool, without all the rest.\r\n\r\n\r\n\r\n Clarify 2 Image Annotation Editing\r\n\r\nThis image shows some of the useful annotation powers that clarify offers.\r\nThe canvas that images are placed on can easily be re-sized with a mouse drag and images pasted.\r\n\r\n\r\n\r\nSnagit\r\n\r\nI avoided this product for a while as I honestly thought it was so mainstream that it wouldn't really benefit a power user/developer. So far, I'm glad to say I'm wrong. It's a well designed product that I've come to start using as my primary screen capture utility, along with Clarify\r\n\r\nPros\r\n\r\nWealth of export functionality\r\nGood annotation capability with shapes, text, and so on.\r\nCan capture screen video for a quick tutorial or walkthrough, no extensive editing options on this however. This is the focus of Camtasia.\r\nAbility to tag screenshots (so bug screenshots could be easily re-reviewed)\r\n\r\nCons\r\n\r\nKeyboard shortcuts are a little lacking with annotations\r\nSome of the effects/annotations seems a little cheesy to me, but there are plenty of ways to find a good fit that formats in the style I like.\r\nNo rounded corner presets for the image canvas if you want that. There are workarounds, but nothing built in.\r\n\r\n\r\n\r\n Snagit Extensibility with output\r\n\r\nA wealth of export functionality is in Snagit, along with the additional plugins you can pull in.\r\n\r\n\r\n\r\nSnagit Profile Extensibility with scripted profiles\r\n\r\nAlong with keyboard shortcuts and powerful export options, I found you could create capture profiles, letting you with setup a shortcut (keyboard or just menu driven) to capture and save as a special format, basically scripting several steps you'd manually repeat together.\r\n\r\n\r\n\r\n Snagit Annotations\r\n\r\nAnnotations are pretty comprehensive with some functionality to splice out the middle of images (say a toolbar), merge multiple images, and more. Pretty well designed and along with Clarify 2 the best implementation from any utilities I've experimented with so far.\r\n\r\nI did like the ability to \"curve\" an arrow on the annotations, as it let you smoothly draw an annotation that could curve around content without obfuscating it.\r\n\r\n\r\n\r\nCamtasia\r\n\r\nCamtasia crosses over the screenshot territory into more of a screen capture area. This functionality can be very powerful however, as I've found the ability to walk through tech issues with a recording is powerful. I'll probably blog with Camtasia eventually as I get more time to do audio recordings. The primary benefit I've found is the easy gif creation with full annotations, blurring, and other effects that help when providing examples while protecting some sensitive connection/context information.\r\n\r\nWith Camtasia, I've found it tremendously intuitive, as I've already worked with NLE (Non-linear editors), and the process of putting together a video with it is very simple. Some products I've tried that compare somewhat would be ActivePresenter, which is great for creating tutorials with a lot of \"whitespace removed\" by detecting the activity via keyboard and clicks.\r\n\r\nHowever, I'd say it still has a hurdle of usability, and is not intuitive compared to the Techsmith offering, making it a great option if you have _zero _budget, but not a good option if you really want to create quickly a quality video.\r\n\r\n conclusion\r\n\r\nQuality tools can help the development workflow. I personally think a tremendous amount of value comes from utilizing images for varied capturing of work in addition to typed notes, as you can gain a lot of information this way. Hopefully, my suggestions might help spark a few new ideas for great tools to help with your workflow.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-12-09-documenting-your-database-with-diagrams",
        "content": "---\r\ndate: \"2015-12-09T00:00:00Z\"\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: Documenting Your Database with Diagrams\r\ntoc: true\r\n---\r\n\r\nDon't get too excited. I know you love documentation, and just can't wait to spend some time digging in to document your database thoroughly. I imagine you probably want to build visio charts manually, or draw whiteboard diagrams by hand and take pictures.\r\n\r\nFor the rest of us that are lazy, a tool to help document your database is a great idea. I'm a big fan of SQL Doc by Red Gate, and ApexSQL Doc . I ended up using ApexSQL doc to document the database at my work, though Red Gates is also a great option. Both provide a great schema documentation tool that can generate a helpful CHM file to browse through (which I much prefer to trying to sort through PDF/WORD docs)\r\n\r\nHowever, there is one thing that I was recently tasked with that made me appreciate a little more hands on documentation. In the case that you are deploying an application or any set of tables/structure that might end up being used by someone else, describing your data model can be a really helpful resource to someone trying to query or implement some custom implementation against your data. This might be helping document some data structures for someone building a report, or even developers trying to leverage some of the data in a separate implementation.\r\n\r\nUnderstanding the data model as a dba/dev is important to being able to architect improvements as well, so I've found that going through and diagramming some of the logical structures/groupings can be a tremendous benefit to better understanding a database that perhaps you didn't architect from scratch, or has legacy results.\r\n\r\nSome positives I see:\r\n\r\nForces a better understanding of how your data model is constructed\r\nCan help highlight weaknesses and flaws in your data model\r\nCan help explain the data model to folks who may not be living and breathing your business, so if terminology and fields sometimes are confusing on how things relate, the data model can help clarify some of these things.\r\n\r\nSome negatives:\r\n\r\nRequires a little elbow grease\r\nYou'll find things to fix\r\n\r\nVisio\r\n\r\nVisio 2010 is the last Visio version that provides the capability to reverse engineer a database into a diagram. This is unfortunate. I worked for a while with this, but ended up shelving Visio as a long term option because of it's unintuitive nature, and behavior at times. For example, refreshing your database schema to identify changes wouldn't help flag any new changes, you'd have tables that were in several diagrams suddenly being given new table names like TableTest, TableTest1, TableTest2, instead of reusing the same table in the model. Also, the layout and arrangement can be pretty messy at times. Working with data sources requires you to use GENERIC OLEDB from what I recall, otherwise I got errors using SQL 2014. Lots of little things that added up to a poor experience.\r\nGave it the old college try.... not worth it in my personal opinion\r\n\r\n Visual Paradigm\r\n\r\nVisual Paradigm Gallery \r\nThis tool has some amazing functionality, but unfortunately was very tedious and complex for the purposes of documenting an existing database. I think this aspect of Visual Paradigm's reverse database toolset needs some major productivity and UI tweaks to be usable. It may be great for someone working with diagrams for creating databases and models, but for a SQL dba working to document their database better, it wasn't a great option.\r\nEven though very unintuitive, I did like the customization options for layout and the arrangement. The problem was none of the changes performed in realtime, thereby making it an annoying guesswork game. Not a huge fan. Also, very difficult to easily identify tables that had related tables to add with a click, so not easy to navigate all the tables and related tables in an intuitive way.\r\nNot a big fan of having to tweak 10 settings via 10 trips to a right click context menu. Their presentation workflow needs some major improvements.\r\n\r\n\r\n\r\nDBVisualizer\r\n\r\nDbVisualizer\r\nBy far the best auto arranging layout tool I found. Amazing. It uses yEd as the backbone of the diagramming, so I also downloaded yEd, but for me to utilize, I'll definitely have to play around as the engine is powerful, but would take programming to get it to do anything similar to what DbVisualizer accomplished. It's more of an exploratory tool than a documenting one, but you can save the graph to .gml format and open in yEd to change. A little extra work, but possible if you like the results. It doesn't display foreign key columns, so it's more of looking at the referring/relationship side of things, without a full column list.\r\nI'm pretty much sold that this is bar none the best auto-laid out representation of tables I've ever seen with any tool. I'll be keeping the free DbVisualizer as a tool for reviewing adhoc data models for sure!\r\nThe negative for documentation purposes is that none of these graphs are saved, so it's good for adhoc or one time, but not to regenerate.\r\n\r\n\r\n\r\n Aquasoft Data Studio\r\n\r\nThey have a great output, but unfortunately they don't allow you to only show key columns, therefore your diagrams get pretty bloated. It's pricey, has some great other features, and I'd be a big fan, except the diagramming options aren't as robust as the others for customization. If you are documenting a data model, I like to only show key columns (PK/FK) to ensure there is not excessive noise.\r\nI did reach out to company on this option, and they said:\r\n\"You can't show the PK and FK without the entity. So, you can't show them be themselves.\"\r\nThe lack of any extra options to submit feedback, or anything as a potential buyer put me off to this product for now.\r\n\r\n\r\n\r\nOther tools that were way too complex and pricey to fit documentation needs\r\n\r\nOracle SQL Developer - very difficult to get the SQL connection up and running. Not unintuitive, not visually pleasing output\r\nToad Data Modeler - same as above. Not visualizer pleasing, too many steps. Targeted at design rather than documentation.\r\nERStudio... didn't even consider, much too pricey for documentation purposes\r\nSQL Architect - much too unintuitive and complex\r\n\r\n Final Winner: DbSchema\r\n\r\nDBSchema\r\nThis is the winner. I switched all our diagrams over to this tool.\r\nA few major pros:\r\n\r\nAll diagrams saved as \"layouts\" in your project. This means I can come back at anytime and fix/tweak.\r\nI can refresh the database schema imported and it will flag all the changes, and then the diagrams will automatically get updated.\r\nIt generates really nice HTML interactive documentation. Major win!\r\nThe major pro I found for this is with a little upfront work in arranging better, I could refresh all diagrams (maybe a new fk gets added), and then bulk export all these as HTML interactive database diagrams with full detail. You can hover over the FK lines and it will highlight the relationship etc. This is great for usability! Also, removes a lot of manual work in refreshing database column diagrams if something gets added/changed.\r\nOne of the most significant points to this tool is the functionality + price! The price for a commercial licenses is extremely reasonable (as of 2015-12-09 the pricing was under $200)\r\nSome things that could use improvement:\r\nImproved autoarrangement - Allow preset option of only showing FK/PK ( i have an autohotkey script I built to do this, contact me if you want it, helped simplify)\r\nAllow optional layouts like Tree, Hierarchical\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2015-12-15-model-needs-exclusive-lock",
        "content": "---\r\ndate: \"2015-12-15T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Model needs exclusive lock\r\n---\r\n\r\nRan into an issue where a developer was trying to create a database and was denied due to no ability to obtain exclusive lock on model. After verifying with other blogs, I found that creating a database required exclusive lock to use model as a template for the new database creation.\r\n\r\nIn my case I had connected with SSMS directly to model for some queries instead of master. In this case, SQL Complete (Devarts's excellent alternative to SQL Prompt) was querying the schema had this open session was blocking usage of model to create a new database. After killing this low priority query session, no issues were experienced.\r\n\r\nGood to remember! Don't connect directly to model unless you have a specific reason to do so. Otherwise, you might be the culprit on some blocking errors.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-05-transaction-logging-&-recovery-(101)",
        "content": "---\r\ndate: \"2016-01-05T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-09\"\r\ntags:\r\nsql-server\r\ntitle: Transaction Logging & Recovery (101)\r\ntoc: true\r\n---\r\n\r\nLogging & Recovery Notes from SQL Server Logging, Recovery, and the Transaction Log (pluralsight) Paul Randal\r\nGoing to share some key points I've found helpful from taking the course by Paul Randal at pluralsight. I've found his knowledge and in detail presentation extremely helpful, and a great way to expand my expertise. Highly recommend pluralsight.com as well as anything by Paul Randal (just be prepared to have your brain turned to mush... he's definitely not writing \"how to write a select statement\" articles\r\n\r\nLogging & Recovery\r\n\r\nWhy is it important to know about this?\r\n\r\nOne of the most critical components to how SQL server works and provides the Durability in ACID (Atomicity, Consistency, Isolation, Durability) is the logging and recovery mechanisms.\r\nIf log files didn't exist we couldn't guarantee after a crash the integrity of the database. This recovery process ensures after a crash even if the data file hasn't be changed yet (after a checkpoint) that SQL server can reply all the transactions that had been performed and thereby recover to the consistent point, ensuring integrity. This is critical and why we know even with a crash that no transactions will be left \"halfway\", as we'd require the transactions to be harden to the log file before SQL server would allow the data file to be changed. If this was done in the reverse order of writing to the data file, then if a crash happened, the log file might be out of sync, and you couldn't reply actions that might not have been fully made to the log file as the data file and log file wouldn't be in sync with the transactions noted.\r\nThe logging & recovery mechanism is actually the backend architecture driving most of the disaster recovery options like Mirroring, Replication, Availability Groups and more. They each have different ways of handling/processing, but underneath, they all rely on utilizing the transaction logs. - Logging = the backbone of sql server.\r\n\r\n Data Files Aren't Changed Immediately\r\n\r\nUnlike what you might think initially, data files are actually note being written to realtime with transactions. This would be inefficient. Instead, the log is written to, hardened, and then periodic checkpoints (default I believe is 1 minute) take these changes that have been hardened and ensure the changed pages in the buffer (dirty pages at this point) are updated as well.\r\n\r\nCan't get away from transactions\r\n\r\nNote that all actions occur in transactions. If not explicitly stated, then an implicit transaction is gathered by SQL server in how it handles the action (when dealing with DML). For example, if we alter 2 tables, we could manually set a transaction for each, so if one fails, both were rolled back to the original state, allowing us to commit one table changes even if the other experienced a failure. If we combined both of these statements without defining a transaction then SQL server would imply a transaction, and this might result in a different behavior.\r\n\r\nFor example:\r\n\r\n`sql\r\n    alter table foo\r\n    alter table bar\r\n    GO\r\n    -- if foo failed, then both tables would not be changed, as the transaction itself failed\r\n`\r\n\r\non the other hand the statement below would be promised in SSMS as a batch separator and SQL server would have two separate transaction for each. If one had an error, then the other would still be able to proceed.\r\n\r\n`sql\r\n    alter table foo\r\n    GO\r\n    alter table bar\r\n`\r\n\r\n Everything gets logged!\r\n\r\nEverything has some logging to describe changes in log file (even in simple recovery)\r\nVersion store & workfile changes in tempdb\r\n\r\nCommitment Issues\r\n\r\nTransaction Has Committed\r\n\r\nBefore the transaction can commit, the transaction log file has to be written through to disk. - Once the transaction log is written out to disk, the transaction is considered durable\r\nIf you are using mirroring, the system will stop and wait for the replica/mirror to harden the transaction to the mirror db log file on disk, and then can harden the transaction log to the disk on the primary.\r\nThe Log file basically represents an exact playback of what changes have been made, so even if the buffer was cleared (removing the pages that were changed in buffer), SQL crashed, or your server went down, SQL server can recover the changes that were made from the log file. This is the \"description\" of the changes that were made.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-08-transaction-logging-&-recovery-(part-2)",
        "content": "---\r\ndate: \"2016-01-08T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-09\"\r\ntags:\r\nsql-server\r\ntitle: Transaction Logging & Recovery (part 2)\r\ntoc: true\r\n---\r\n\r\n... Continuation of some notes regarding the excellent content by Paul Randal in Pluralsight: SQL Server: Logging, Recovery, and the Transaction Log. Please consider supporting his excellent material by using Pluralsight and subscribing to his blog. He's contributed a vast amount to the SQL server community through SQLSkills\r\n\r\nTransaction Log File\r\n\r\nThe initial size of the log file is the larger of 0.58 MB or 25% of the total data files specified in the create database statement. For example, if you create a database with 4 separate files, it would increase the initial size the log file is set to.\r\nThis would be different if you've changed MODEL database to set the default log and database size.\r\nThe log file physically created must be zero initialized. Note that the benefits of instant file initialization apply to the data file, but the log file still has to be fully zero initialized, so a large log file creation doesn't benefit from instant file initialization. Previous Post on Enabling File Initialization\r\n--- Examine the errorlog (after you've enabled trace flag 3605,3004) EXEC xp_readerrorlog; GO\r\nWhen examining the results, you can see the zeroing of the log file, but not the datafile if you have instant file initialization enabled.\r\n\r\n\r\n\r\n Virtual Log Files\r\n\r\nThe transaction log is divided into virtual log files. This helps the system manage the log file more efficiently.\r\nNew VLF's are inactive & not used.\r\nActive VLF's contain the log record activity and can't be reused until they have been noted as available by SQL server.\r\n_My seque based on the fun experience of giant log files. _\r\nNote: In Brentozar Office Hours Brent talked about the common misconception of SIMPLE VS FULL logging. Most folks (guilty) think that SIMPLE reduces the amount of logging SQL server performs, thereby improving the overall performance. However, in a general sense this is a misconception. Logging, as previously discussed from my previous post on (101), is the core of SQL server, and required for transaction durability. The difference between SIMPLE and FULL is mostly to do with how the transaction log space is marked as available for reuse.\r\nSIMPLE: after data files are updated and the data file is now consistent with the changes the log has recorded, the transaction logs are now marked as free and available.\r\nFULL: all the transaction log records, even after hardened to the data file, are still used. This is what can cause the common issue of exponential log file growth with folks not aware of how it works. 300 GB log file on a small database due to now one watching? Been there? This is because the log file will keep appending the log entries overtime, without freeing up space in the existing transaction log, unless some action is taken to let SQL server know the transaction log file space is available for reuse.\r\nMarking the space as available is done by ensuring you have a solid backup solution in place that is continually backing up the transaction log in the backup set) thereby letting SQL server know that the transaction log has been backed up and space can be reused in the existing log file.\r\nThe normal process would be to ensure you have a full backup, incremental backups, and transaction log backups running on a schedule. Under the full recovery model or bulk-logged recovery model, if the transaction log has not been backed up recently, backup might be what is preventing log truncation. If the log has never been backed up, you must create two log backups to permit the Database Engine to truncate the log to the point of the last backup. Truncating the log frees space for new log records. To keep the log from filling up again, take log backups frequently.\r\n MSDN Troubleshooting a Full Transaction Log\r\n\r\nMy past experience was running into this challenge when performing a huge amount of bulk transactions. I ran the space out on a drive because the log files continued to grow with no backups on the log file running. The solution in my particular situation was to take a full backup, change the database recovery to Bulk-logged or SIMPLE, perform the massive changes, then get right back to full-recovery with backup. This helped ensure the log file growth didn't keep escalating (in my case it was the appropriate action, but normally you want to design the size of the transactions to be smaller, and the backup strategy to be continual so you don't run into this issue)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-13-verifying-instant-file-initialization",
        "content": "---\r\ndate: \"2016-01-13T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Verifying Instant File Initialization\r\n---\r\n\r\nRan into a few issues verifying instant file initialization. I was trying to ensure that file initialization was enabled, but found out that running the xp_cmd to execute whoami /priv could be inaccurate when I'm not running it from the account that has the privileges. This means that if my sql service account has different permissions than I do, I could get the incorrect reading on if it is enabled.\r\n\r\nPaul Randal covers a second approach using the sysinternals tool Accesschk, which seems promising. However, in my case, I didn't have permissions to run in the environment was I was trying to check. I found a way to do this by rereading original article in which Paul Randal demonstrates the usage of trace flags 3004,3605. This provided a very simple way to quickly ensure I was getting the correct results back. For even more detail on this, I highly recommend his Logging, Recovery, and Transaction Log course. I adapted pieces of his script for my quick error check on this issue.\r\n\r\nSuccessfully Verifying\r\n\r\nSuccessfully added instant file initialization should mean when you review the log you will not have any MDF showing up in the error log for zeroing. I adapted the sql script for reading the error log in a more filtered manner from this post: SQL Internals Useful Parameters for XP Reader (2014)\r\n\r\n{{% gist f2dddfc8187f6676cd76 %}}\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-20-seeker-&-servant-fantastic-music-with-incredible-dynamics",
        "content": "---\r\ndate: \"2016-01-20T00:00:00Z\"\r\ntags:\r\nramblings\r\nmusic\r\ntitle: 'Seeker & Servant: Fantastic Music with incredible dynamics'\r\n---\r\n\r\nJust recently discovered this artist after being exposed to an article from Worship Leader magazine. Fantastic dynamics. The dynamics and beautiful harmonies are pretty darn close to what I'd love experimenting with if I had a group of folks playing those instruments. Interestingly, the vocal harmonies are very simple, but I found very beautiful. It's compromised of a tenor and a baritone range, and is a fresh change stuff like Shane and Shane which both have incredibly high ranges. I found it very approachable to enjoy singing with. The power of the musical dynamics and beautiful lyrics was a major win. I'll be following them closely.\r\n\r\nGet their latest album for free here: Seeker & Servant\r\nSimilar Artists/Albums:\r\n\r\nFleet Foxes\r\niframe data-preserve-html-node=\"true\" src=\"?uri=spotify%3Aalbum%3A3l7iMXJ0jqFnIYZRyCUewC\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\"/iframe\r\n\r\n\r\nDustin Kensrue: Carry the Fire\r\niframe data-preserve-html-node=\"true\" src=\"?uri=spotify%3Aalbum%3A01k7Oz3hfoG0HFPsZ7MUIT\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\"/iframe\r\n\r\n\r\nWould Like If:\r\n\r\nYou like post-rock\r\nLong Vamping Dynamics\r\nMinimalist Arrangements\r\nTight 2 part harmonies\r\n\r\nHere's the album on spotify for those who want to check it out!\r\n\r\niframe data-preserve-html-node=\"true\" src=\"?uri=spotify%3Aalbum%3A4ZbX2MIrXRrTviMGDGsHpv\" width=\"300\" height=\"380\" frameborder=\"0\" allowtransparency=\"true\"/iframe\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-20-transaction-logging-&-recovery-(part-3)",
        "content": "---\r\ndate: \"2016-01-20T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Transaction Logging & Recovery (part 3)\r\ntoc: true\r\n---\r\n\r\nContinuation of some notes regarding the excellent content by Paul Randal in Pluralsight: SQL Server: Logging, Recovery, and the Transaction Log.\r\nPlease consider supporting his excellent material by using Pluralsight and subscribing to his blog. He's contributed a vast amount to the SQL server community through SQLSkills This is my absorbing of key elements that I never had worked through\r\n\r\nJackalopes Are Real....so are Virtual Log Files\r\n\r\nEver seen a picture of a jackalope?\r\nImage by Mark Freeman (Jackalope, Grand Canyon North Rim, Oct 07) Creative Commons License\r\n\r\nThis is how I used to feel about Virtual Log Files. Folks were saying things like\r\n\r\n\"Your server may be slowing down because of those darn VLF's\".....\r\n\"Have you checked your VLF count\"...\r\n\"My VLF count was x\" and more\r\n\r\nFinding clarification on VLF (Virtual Log Files) can be difficult, as what is considered a high count for some may be contradicted by another with another \"target VLF count\" claim.\r\nPaul Randal unpacks this excellently in his class, providing some great transparency.\r\n\r\n\r\n\r\n Why Should I Care About VLFs?\r\n\r\nIn an excellent article regarding the performance impact analysis of VLF's, Linchi Shea provides some valuable insight into the impact.\r\nFor more detailed analysis & graphs please look at this great article:\r\nPerformance impact: a large number of virtual log files - Part I (2009)\r\n\r\nInserts were about 4 times as slow\r\nUpdates were about 8 times slower\r\nDeletes were about 5 times slower\r\nRecovery time can be impacted Slow recovery times and slow performance due to large numbers of Virtual Log Files (2008)\r\nTriggers & Log Backups can be slowed down Tony Rogerson article (2007)\r\n\r\nVirtual Log Files\r\n\r\nAt the beginning of each log file is a header. This is 8kb header that contains settings like autogrowth & size metadata.\r\nActive VLF's are not free for usage until they are marked as available when clearing the log (see previous post about backups)\r\nWhen you create a db you have one active VLF file, but as you progress more VLF's will be used.\r\nToo few or too many VLF's can cause problems.\r\n\r\n VLF Count\r\n\r\nYou cannot change the number and size of VLF's in a new portion of the transaction log. This is SQL server driven.\r\nThe VLF size is determined by a formula.\r\nFor detailed breakdown of the changes that SQL 2014 brought for the VLF algorithm, see this excellent post by Paul Randal: Important change to VLF creation algorithm in SQL Server 2014 \r\nSince I'm working with SQL 2014, I found it interesting as the increased VLF count issue that can be impacting to server performance has been greatly improved. Paul's example cited that the number of VLF's in his example would result in 3192 VLF prior to 2014, but with SQL 2014 it decreased down to 455, which is a substantial improvement. Paul indicated that the prior algorithm was designed primarily for around 1997-1980's, when log files wouldn't be sized as large.\r\nAlso note a critical question that he answers: _COMPATIBILITY LEVEL IS IGNORED BY THE STORAGE ENGINE PROCESSOR_\r\nThis is great information he's shared, as I've found it confusing at times to separate out the Query Engine impact from compatibility level, and understanding this scope of impact can help with assessing possible impact.\r\n\r\nMore Detail than You Ever Wanted to Know on VLF's\r\n\r\nVLF's internally contain log block sizes. 512-60KB.\r\nWhen the log block is filled it must be flushed to disk.\r\nWithin the log block are the log records.\r\nVLF's contain a header. This indicates whether or not the VLF is active or not, LSN, and parity bits.\r\nVLF log records support multiple concurrent threads, so the associated transaction records don't have to be grouped.\r\nLSN. I've heard the term used, but until you understand the pieces above, the term won't make sense. - Log Sequence Number = VLF Sequence Number : Log Block Number : Log Record\r\nThey are important as the LSN is stamped on the data file to show the most recent log record it reflects, letting sql server know during crash recovery that recovery needs to occur or not.\r\n\r\n Number of Log Files\r\n\r\nThis is determine by a formula that has been updated for 2014.\r\n\r\nDifferent size growths have different number of VLFs.\r\nVLF's don't care about the total size, but instead about the growth.\r\nFor instance, Above 1 GB growth events on log file will split into 16 new VLF's, 1/16.\r\n\r\nFAQ (I've asked and looked for some answers!)\r\n\r\n*Create small log and then expand or create larger log initially? *\r\n\r\n Paul Randal answered:  No. If I was creating, say a 64 GB log, I'd create it as 8GB then expand in 8GB chunks to 64GB to keep the number of VLFs small. That means each VLF will be 0.5 GB, which is a good size.\r\n What is the ideal l number of VLFs?\r\n Some key articles I've found for detailed answers on understanding proper VLF count:\r\n\r\n 1.  Transaction Log VLFs - too many or too few (2008)\r\n 2.  8 Steps to better Transaction Log throughput (2005)\r\n 3.  A Busy/Accidental DBA's Guide to Managing VLFs (2009)\r\n Resources\r\n 4.  Brentozar SP_BLITZ will check VLF counts\r\n How do I ensure my log file gets marked as available for reuse when in full recovery?\r\n Full recovery is required for point-in-time recovery after a failure. This is because every change to data or to database objects are written to the transaction log prior to being committed. These transactions are then written to the data file as SQL Server sees fit after this initial write to disk. The transaction log is a rolling history of all changes in the database and will allow for redo of each transaction in case of failure to rebuild the state of the data at failure. In the case of Full Recovery, the transaction log continues to expand until a checkpoint is issued via a successful transaction log backup. Top 13 SQL Server Mistakes and Misteps (2012) This great article by Tim Ford should be reviewed, as it's one of the best simple breakdowns of growth issues and prevention that I've read.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-25-diff-all-files-reviewing-changesets-quickly",
        "content": "---\r\ndate: \"2016-01-25T00:00:00Z\"\r\ntags:\r\ncoding\r\ncool-tools\r\nsql-server\r\ntitle: Diff All Files - Reviewing Changesets Quickly\r\ntoc: true\r\n---\r\n\r\nDiff all files\r\n\r\n info \"Updated: 2017-07-14\"\r\n Still find this incredibly awesome! Developer just updated for Visual Studio 2017 after a lot of hard work. Github repo here for any issues. Big thanks to DeadlyDog for this great tool and putting in the effort to update for VS2017. deadlydog/VS.DiffAllFiles: Visual Studio Extension to make comparing files before and after committing them to Git and TFS faster and easier\r\n\r\n info \"Updated: 2016-01-25\"\r\n I started using the option to compare a single file at a time, since it picks up the syntax highlighting then. The other way (merge files into one) can't handle syntax highlighting with the various files types all mixed into the same window. Diff All Files extension handles this beautifully with proceeding one by one and automatically opening the next file to compare or allowing you to hit next to close and reopen with the next file to compare. I still enjoy using this due to the \"context\" lines that reduce the noise to just the actual section being changed.\r\n\r\nIf you need to do code comparison on a lot of files in Visual Studio, I ran across an extension Diff All Files that is really helpful for merging down the individual file changes into more consumable format.\r\n\r\nIn the changeset view you can select diff all files and whatever settings you've setup in the Tools  Options  Diff all Files settings will then feed through automatically.\r\n\r\n\r\n\r\nYou then click on diff all files in the changeset viewer\r\n\r\n\r\n\r\nAll the non-excluded items will then be merged into a single file, if this option was selected, or separate windows. Personally, I've found the merged view really helpful to have one single pane to scroll through.\r\n\r\n\r\n\r\n Third Party Comparison Perks\r\n\r\nTo go a step beyond this you could use a third party comparison tool. My preferred option is Araxis Merge (disclaimer: they provided me with a license for evaluation, which doesn't impact my assessment). I really like the fact it merges down similarities into a concise comparison. This tool gives you the option to provide only the different lines, with a X number of lines before and ahead for context. This could reduce thousands of lines of code to just a few hundred that have variances. Win! Highly recommend you check them out, as if you are doing open source they have a free license for that. If you do a lot of code comparisons, some of the additional functionality in their app might be worth it, otherwise the built in viewer in TFS is pretty good.\r\n\r\nIn my test comparison, I reduced 3245 lines of code in the comparison window to 25 lines, which was the actual changes + 10 lines above/below for each change to have context. This resulted in only a few hundred lines to scroll through. Made my life easier!\r\nThe only con for me with Araxis is no dark theme.... but I'll live.\r\n\r\n\r\n\r\nconfiguring third party tools\r\n\r\n\r\n\r\nI saved this snippet from working through Araxis documentation for setting up their tool with Visual Studio.\r\n\r\n{{% gist 8f46d34d2c3c83a4c18e5d70e92e5d3d %}}\r\n\r\n\r\nI also just ran across another site that had a nice resource list of configuration settings for different comparison tools.  Diff All Tools - Visual Studio Extension\r\n\r\n Other comparison tools\r\n\r\nThird party tools can be great for some extra perks like File versioning, comparing folders, and more. You can get by without them, of course, but if you do a lot of change review consider some of the options. A few others I've reviewed (if pro/paid they provided license for me to evaluate)\r\n\r\nDelta Walker (pro): Great UI, does image similarities comparison tool. Didn't seem to have a \"show only changed lines with context\" like Araxis at the time I reviewed. Mac app!\r\nDevart Code Compare (pro/free): I love most programs Devart puts out. This is a good comparison app. Wasn't my preferred option, and at times struggled with large files, but overall was good in comparison accuracy.\r\nDiffMerge: Brief overview, solid tool, just didn't have specific improvements I needed (focused on changeset/sql comparison)\r\nBeyond Compare: didn't dive into this extensively. They have a devoted following with a lot of loyalty, so check them out if you want to evaluate their software.\r\nBuilt in TFS diff - great for a built in tool, I just wanted to have some enhancements in what to ignore and summarize.\r\nSQL Compare: my favorite for reviewing changsets that are directly in TFS. I wish they'd add a module for just pulling up the diff view without having to open SQL Compare to setup a comparison though. Sounds like a good user voice feedback item :-)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-01-29-easy-way-to-test-log-shipping-or-availability-groups-setup",
        "content": "---\r\ndate: \"2016-01-29T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Easy way to test log shipping or availability groups setup\r\n---\r\n\r\nHave been working through the fantastic training resources from Brent Ozar's Everything Bundle and on the recommended resources they mention after all the training on log shipping and availability groups that you can use a lab environment from TechNet to actually get going on familiarizing yourself with the technology more.\r\n\r\nThis is great! Of course, it's not the full deal, but this gives a tangible way to get moving on familiarizing yourself with this complex technology.\r\nTechNet Virtual Labs Availability Groups\r\nSide note: Fantastic training resource with great roi with Brent Ozar Unlimited \"Everything Bundle\" if you apply the current podcast review discount of 78%. Great value with immediate return. I also like pluralsight, but find it takes more time investment to get the immediate value. Their courses are short, compact, and full of great material.\r\n\r\nRecommend you add to your training budget asap. Podcast Review Discount\r\nCombine this with a subscription to Office Hours with the app Pocket Cast, and you'll have an easy way to keep up to date with some great tips.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-03-18-calculating-some-max-mirror-stats",
        "content": "---\r\ndate: \"2016-03-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Calculating Some Max Mirror Stats\r\n---\r\n\r\nThis turned out to be quite a challenge. I couldn't find anything that made this very clean and straight forward to calculate, and in my case I was trying to gauge how many mirroring databases I could run on a server.In my scenario, I wasn't running Expensive Edition (@BrentO coined this wonderful phrase), so was looking for the best way to assess numbers by doing mirroring on a large number of databases, in my case  300 eventually.\r\nThe documentation was... well.... a bit confusing. I felt like my notes were from the movie \"A Beautiful Mind\" as I tried to calculate just how many mirrors were too many!\r\nThis is my code snippet for calculating some basic numbers as I walked through the process. Seems much easier after I finished breaking down the steps.\r\nAnd yes, Expensive Edition had additional thread impact due to multi-threading after I asked about this. Feedback is welcome if you notice a logical error. Note that this is \"theoretical\". As I've discovered, thread count gets reduced with increase activity so the number mirrored database that can be mirrored with serious performance issues gets decreased with more activity on the server.\r\n\r\n{{% gist 1335ab60accc21b95ece %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-03-18-failover-all-databases-to-other-server",
        "content": "---\r\ndate: \"2016-03-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Failover all databases to other server\r\n---\r\n\r\nQuick snippet I threw together to help with failing over synchronized databases to the other server in bulk. No way I want to click that darn Fail-over button repeatedly. This scripts the statements to print (i commented out the exec portion) so that you can preview the results and run manually.Note that it's also useful to have a way to do this as leaving databases running on the mirror server for an indefinite period can violate licensing terms on the secondary server when it's a fail-over server and not meant to be the primary.\r\n\r\n{{% gist b753658689b40b2883c5 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-03-18-previewing-the-new-ssrs-2016-portal",
        "content": "---\r\ndate: \"2016-03-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\nssrs\r\ntitle: Previewing the new SSRS 2016 portal\r\n---\r\n\r\nRan into an issue with the \"Preview New Reporting Portal\" link on a fresh install of 2016 giving me a not found error.\r\n\r\n\r\n\r\nChanging the virtual directory in the Report URL tab for SSRS configuration fixed this invalid link. In my case, I changed /Report to /Reporting.\r\nThanks to Adam on Stack Overflow for providing the solution and saving me a lot of time!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-03-18-sql-2012-sp3-and-entity-framework-conflict",
        "content": "---\r\ndate: \"2016-03-18T00:00:00Z\"\r\ntags:\r\nentity-framework\r\nsql-server\r\ntitle: SQL 2012 SP3 and entity framework conflict\r\n---\r\n\r\nthe problem\r\n\r\nAn issue with SQL Server 2012 SP3 was identified that impacted EF4/5 due to additional datatypes in the dll.\r\n\r\n    System.EntryPointNotFoundException: Unable to find an entry point named 'SetClrFeatureSwitchMap' in DLL 'SqlServerSpatial110.dll'\r\n\r\n diagnosing\r\n\r\nTo easily identify the available dll versions of sql server, I ran a quick adhoc bat file.\r\n\r\n{{% gist 88ff6ce9caa927a27804 %}}\r\n The output returns a simple text file like this:\r\n\r\n\r\nA post in technet mentioned that the DLL shipped with SP3 could cause these conflicts and if the uninstall didn't clean up the GAC correctly, problems could occur with Entity Framework calls.\r\n\r\n Can confirm in my case it was due to dll shipped in SQL Server SP3.  I had to uninstall the patch but the newer dll was still in the gac so I had to overwrite with the older version using gacutil. ( Edited by snowcow Thursday, January 14, 2016 12:41 PM )\r\n\r\nThe Fix\r\n\r\nIn my case, I still needed the current SP3 version, but we wanted to make sure that the app was pointing to the older version to avoid this error.\r\nI apparently needed to point backwards to:  C:\\Windows\\assembly\\GACMSIL\\Microsoft.SqlServer.Types\\10.0.0.0_89845dcd8080cc91\r\nStack Overflow, the golden mecca of programming knowledge, saved the day with a solid answer\r\n\r\n EF Cannot Update Database\r\n This forces the EntityFramework to use the version 10 of the SqlServer.Types.dll, which doesn't have the Geometry type apparently. - KdBoer\r\n When the fix was applied to map the application config to the older version of the Microsoft.SqlServer.Types.dll (in this case 10). Apparently the 2012 SP3 provided some additional functionality in the dll and this had a conflict with Entity Framework 4 for my situation (and according to online posts EF5 also had some issues)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-03-18-tfs-work-item-fancy-filtering",
        "content": "---\r\ndate: \"2016-03-18T00:00:00Z\"\r\ntags:\r\ndevelopment\r\nsql-server\r\ntfs\r\nramblings\r\ntitle: TFS Work-Item Fancy Filtering\r\n---\r\n\r\nIf you want to create a TFS query that would identify work items that have changed, but were not changed by the person working it, there is a nifty way to do this.The filtering field can be set to  another field that is available, but the syntax/setup in Visual Studio is not intuitive. It's in the dropdown list, but I'd never noticed it before!\r\n\r\n\r\n\r\n`sql\r\nAND ' Changed By '  [Field] ' Assigned to\r\n`\r\n\r\nNote that you don't include brackets on the assigned to field, and that the  [Field] is not a placeholder for you to type the field name in, it's actually the literal command for it to parse this correctly.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-04-01-failed-to-initialize-sql-agent-log...-not-worthy",
        "content": "---\r\ndate: \"2016-04-01T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Failed to Initialize SQL Agent Log... not worthy\r\n---\r\n\r\nMoving system databases in SQL Server takes a bit of practice. I got that again, along with a dose of SQL humility (so tasty!), today after messing up some cleanup with sql agent server log files.\r\n\r\n`text\r\nFailed to initialize SQL Agent log (reason: Access is denied).\r\n`\r\n\r\nI was creating a sql template when this came about. SQL Server Agent wouldn't start back up despite all the system databases having very little issues with my somewhat brilliant sql commands.\r\nI had moved all my databases to the new drive location, and changed the advanced startup parameters for sql server and SQL Agent... or so I thought.\r\n\r\n\r\n\r\nI apparently missed the order of operations with SQL Server Agent, and so it was unable to start. MSDN actually says to go into the SQL agent in SSMS to change this, and I thought I was smarter than msdn....\r\n\r\n MSDN\r\n\r\n *   Change the SQL Server Agent Log Path\r\n From SQL Server Management Studio, in Object Explorer, expand SQL Server Agent.\r\n *   Right-click Error Logs and click Configure.\r\n *   In the Configure SQL Server Agent Error Logs dialog box, specify the new location of the SQLAGENT.OUT file.\r\n *   The default location is C:\\Program Files\\Microsoft SQL Server\\MSSQL\r\n version data-preserve-html-node=\"true\".\r\n instance_name data-preserve-html-node=\"true\"\\MSSQL\\Log.\r\n Found the registry entry and changed here... all fixed!/instance_name/version\r\n\r\n\r\nI also updated the WorkDirectoryEntry to ensure it matched new paths.\r\n\r\nThanks to this article I was saved some headache. I also learned to read directions more carefully :-)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-04-14-cool-tools-powershell-ise-steroids",
        "content": "---\r\ndate: \"2016-04-14T00:00:00Z\"\r\ntags:\r\ncool-tools\r\npowershell\r\nsql-server\r\ntitle: 'Cool Tools: Powershell ISE-Steroids'\r\n---\r\n\r\nDisclaimer: I have been provided with a free license because I am reviewing. This doesn't impact my assessment of the tool. I have a passion for finding tools that help developers, and even more specifically SQL DBA/Developers improve their workflow and development. I don't recommend tools without actually using them and seeing if I'd end up adding to my roster of essentials!\r\n\r\n\r\n\r\nCool Tool: ISE Steroids\r\n\r\nExpanding Powershell ISE\r\n\r\nPowershell ISE is simple. Not much fluff, but it gets the job done. You already know you have it on most of your machines, so expanding ISE seems like a logical step if buying something like Sapien's powershell studio or installing Idera's/Dells/etc studios are not something you want to do. I ran across Powershell ISE-Steroids as a recommended \"must have\" and decided to investigate further.\r\n\r\n Intelligent Code Parsing\r\n\r\nFor those of us that don't live daily in powershell, having some explanations on best practice and some guidance is a welcome change.\r\n\r\n\r\n\r\nVariable Explorer\r\n\r\nThis is a great implementation of a variable explorer that can dramatically help when debugging scripts.\r\n\r\n\r\n\r\n Diving into variable explorer\r\n\r\nI was having trouble finding the right property and objects with Amazon Powershell SDK for AWS. So I broke up a query to get instances into several steps and then explorered the objects. This made things much easier to explore.\r\n\r\n\r\n\r\nIntelligent Error Checking\r\n\r\nHelpful description on error when I clicked on the warning icon\r\n\r\n\r\n\r\n Help\r\n\r\nI need powershell help a lot. I don't work enough it in it to have it all memorized, so having a helpful syntax reference guide is a great plus.\r\nI'm a big fan of more help...that is easily accessible. I'll take as much \"powershell for dummies\" guidance as possible. I know you can find help with built in powershell functionality, but again, it's the combination of all the little shortcuts and pieces together that help so much.\r\nSmall benefit is context menu selection of help. With regular ISE, you can run UPDATE-HELP  and then press f1 over a cmdlet to get details, this is just a shortcut on the context menu. However, it's a helpful reminder for those newer to working with Powershell ISE.\r\n\r\n\r\n\r\nMore help\r\n\r\nHelp as you've seen it is triggered on executing help against specific cmdlets. However, ISE-Steroids has a context sensitive help that's pretty cool.\r\n\r\n\r\n\r\n example of contextual help\r\n\r\n\r\n\r\nRefactoring Utility\r\n\r\nLots of great best practice refactoring can be automatically applied.\r\n\r\n\r\n\r\n refactoring\r\n\r\nThis is a very small example of what it would do. A better case would be a more complex powershell query.\r\n\r\n\r\n\r\nVersioning of powershell scripts\r\n\r\nInstead of having to constantly comment and uncomment for fear of losing work, you can keep things nice and clean with the versioning built in. This is great, I wish more editors had it.\r\nIn this case there is a shortcut to pull up a compare and you can open up the code comparison quickly to see what variations in the script have occurred. Additionally, commenting is possible.\r\n\r\n\r\n\r\n thoughts\r\n\r\nIf you are working with powershell then something like ISE Steroids can provide some smart help, ensuring better script quality, and hopefully saving some time. It's a recommended tool for my #cooltools list.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-04-27-automating-ssms-2016-updates-&-install",
        "content": "---\r\ndate: \"2016-04-27T00:00:00Z\"\r\nexcerpt: simplify your ssms install\r\ntags:\r\nautomation\r\ncool-tools\r\nsql-server\r\ntitle: Automating SSMS 2016 Updates & Install\r\n---\r\n\r\n info \"update 2016-04-27 11:20 - Red Gate SQL Toolkit\"\r\n This also is a great help for folks using Red Gate SQL Toolkits. It can help ensure all items are up to date. When a new bundle installer is identified, it would download the new one and you could then trigger the updates of each of the apps you desire, without having to keep run the download later through Red Gate's tool.\r\n\r\n\r\nFigured I'd share a way to automate the SSMS 2016 updates until it gets it's own fancy self updater. I love staying up to date, but with Power BI, SSMS, and others updating monthly or more and not having any automation for keeping up to date, this is something I find a waste of time that I'd rather automate.\r\n\r\nssms 2016 install\r\n\r\nI think this gets a win, as it's by default in a dark theme. If contains the future possibility of dark theme just like Visual Studio, it gets my stamp of hearty approval. According to some social media posts I've read, it's not yet implemented, but bringing the theming and extension capabilities to SSMS is a goal, and some of it should be here soon.\r\n\r\n\r\n\r\n currently using 2015 shell\r\n\r\n\r\n\r\nUpdates applied seperately from sql service packs\r\n\r\nOf course, the main benefit to having the SSMS install as it's own installer/update is we can get regular updates and improvements without it having to align with sql server service packs. This should allow Management Studio to have more rapidly developed and improved product with more frequent releases.\r\n\r\n\r\n\r\n changelog\r\n\r\nFinally have a changelog to easily review Sql Management Studio updates. As I recall, previously you had to sort through all the changes with sql bug fixes to find what was updated.\r\nSQL Management Studio - Changelog (SSMS)\r\n\r\n\r\n\r\nKetarin to the rescue\r\n\r\nKetarin is one of my favorite tools for automating setup and maintenance of some tedious software products. It takes a little practice to get the hang of it, but it's pretty awesome. It's sort of like a power user version of Ninite. You can automate setup and install of almost anything. The learning curve is not too bad, but to fully leverage you want to benefit from the regex parsing of the webpage to get the download link that changes with version, such as what we might deal with on version changes with SSMS.\r\n\r\n Download latest SSMS Version\r\n\r\nMSDN Installer Location\r\nHopefully, they'll improve the process soon by trimming the size and allowing ssms to autoupdate. Just like Power BI, you have to download the installer for the new version and run the installer to upgrade.\r\nAs a solution in the meantime, you could leverage the power of *Ketarin** *\r\nI created a installer package for running the update automatically, so you could have this setup to check upon startup, and then when a download is detected, download the update, and run silent install. Perhaps this will help you if you want to stay up to date.\r\n\r\n\r\n\r\nKetarin passive install\r\n\r\nThe version parsing I added into this means you shouldn't need to download the installer unless it detects a new version applied.\r\n\r\n\r\n\r\n Update ready to download and apply\r\n\r\nThis is what you'd see on computer startup with a fresh update ready and waiting for you.\r\n\r\n\r\n\r\nLast setup note\r\n\r\nIf you setup Ketarin, to make the app portable, copy the jobs.db from appdata folder, into the application folder and restart. This will make it portable so you can actually put this on a USB, clouddrive, or however you want to make it easily usable on other machines.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-04-27-google-search-only-results-from-the-last-year",
        "content": "---\r\ndate: \"2016-04-27T00:00:00Z\"\r\ntags:\r\nramblings\r\ntech\r\ntitle: Google Search Only Results from the last year\r\n---\r\n\r\nTech changes quick. Reading google postings from something in 2009 is not my first choice.I found an option after digging through some google discussion posts on how to setup the default search in chrome (also applies to other browsers) to automatically apply the advanced filter option for \"results in last year\".\r\n\r\nGo to chrome settings menu\r\n\r\n\r\nNavigate to manage search engines\r\n\r\n\r\nEnter the new search engine option\r\n.\r\nHere is the snippet that sets the new default.\r\n\r\n{{% gist 200fb5c17e0d7455699bd07830942704 %}}\r\n\r\n\r\nSet the entry as default\r\n\r\n\r\nReview the great results of your hard work :-)\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-04-28-red-gate-dependency-tracker-making-databases-into-moving-art",
        "content": "---\r\ndate: \"2016-04-28T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-09\"\r\ntags:\r\ncool-tools\r\nphotography\r\nsql-server\r\ntitle: Red Gate Dependency Tracker - Making Databases Into Moving Art\r\n---\r\n\r\ndata structures are art\r\n\r\nIf anyone thinks that working with complex data structures is boring... I don't know what world they live in. The problem is often that sql tables and data structures are just script files and lists of tables in an object explorer.\r\niframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1jOUyjgO0_A?rel=0&controls=0&showinfo=0&autoplay=1&modestbranding=1&rel=0&autohide=1&loop=1\" frameborder=\"0\" allowfullscreen/iframe\r\n However, once you crack open the visual aspect of database diagramming and data architecture, you can see some interesting patterns emerge.\r\n\r\n red gate dependency viewer tracker\r\n\r\nI've long enjoyed playing around with Red Gate Dependency Tracker. I've found it a great tool for interacting with dependencies and visually working through their relationships.\r\nI figured I'd share with others a more artistic side of database architecture, as I've never seen it done, and I found it uniquely beautiful.\r\n\r\nvisual review of data structures\r\n\r\nThis is the architecture of the application I work on everyday. I inherited care of a system that shows the attention and design that was put into it. The order of the key relationships is obvious. I've worked on some systems that this diagram would be completely different story without the order and structure.\r\nThe interesting part about the visual dependency view is you can often see orphaned objects that aren't correctly linked to their parent objects, so it can serve as a useful tool to help improve existing designs. My previous article about tools I've use for this is here...Documenting Your Database with Diagrams\r\nEnjoy!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-05-15-the-mysterious-black-box-of-r-for-the-sql-server-guy",
        "content": "---\r\ndate: \"2016-05-15T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: The Mysterious Black Box of R - For the SQL Server Guy\r\n---\r\n\r\nTook a class from Jamey Johnston @ SQLSaturday #516 in Houston. Lots of great information covered. Follow him for a much more detailed perspective on R. Jamey Johnston  on Twitter @StatCowboy. Did a basic walkthrough of running an R query, and figured I'd share it as it had been a mysterious black box before this. Thanks to Jamey for inspiring me to look at the mysterious magic that is R....\r\n\r\nSetup to Run Query\r\n\r\nSimple-Talk: Making Data Analytics Simpler SQL Server and R\r\nThis provided the core code I needed to start the process with R, recommend reading the walkthrough for details.\r\nTo get started in connecting in RStudio to SQL Server run this command in the RStudio console.\r\n\r\n`r\r\ninstall.packages(\"RODBC\")\r\n`\r\nVerify the library is installed by running from the console\r\n\r\n`r\r\nlibrary()\r\n`\r\n\r\n Running Select from View\r\n\r\nThis was run against StackOverflow database\r\n\r\n`r\r\nlibrary(RODBC)\r\nstartTime1\r\n<- Sys.time() cn <- odbcDriverConnect(connection=\"Driver={SQL Server Native Client 11.0};server=localhost;database=StackOverflow;trustedconnection=yes;\") dataComment <- sqlFetch(cn, 'vwtestcomments', colnames=FALSE,rowsattime=1000) View(dataComment) endTime1 <- Sys.time() odbcClose(cn) timeRun <- difftime(endTime1,startTime1,units=\"secs\") print(timeRun)\r\n`\r\n\r\n I created a simple view to select from the large 15GB comments table with top(1000)\r\n\r\n`sql\r\nUSE [StackOverflow]\r\nGO\r\nSET ANSI_NULLS ON\r\nGO\r\nSET QUOTED_IDENTIFIER ON\r\nGO\r\n    CREATE view [dbo].[vw_testcomments] as\r\n    select top(10000) * from dbo.Comments as C\r\nGO\r\n`\r\n\r\n\r\n\r\nviewing the results of basic query in r studio\r\n\r\n\r\n\r\n running R script in PowerBi\r\n\r\n\r\n\r\nexecute r script\r\n\r\n\r\n\r\n results preview\r\n\r\n\r\n\r\nVisualized in Power Bi\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-05-16-continual-deployment-of-visual-studio-sql-proj",
        "content": "---\r\ndate: \"2016-05-16T00:00:00Z\"\r\ntags:\r\nsql-server\r\ncool-tools\r\ntitle: Continual Deployment of Visual Studio SqlProj\r\n---\r\n\r\nUnveil the inner workings of the esoteric build system...\r\n\r\nAs a data professional, I've never worked extensively with msbuild or other pipelines. I'd been mostly focused on just running schema comparisons and publishing. However, I've had the needed to try and deploy a database project from visual studio automatically, and this is my process through it.\r\n\r\nThere are benefits for those who don't necessarily want to run this against production, but instead for those who want to continually deploy check-ins and run tests, documentation, or other tasks against the deployed schema. This was my personal goal.\r\n\r\nThere are other solutions that plug into this that can make the process easier, such as Red Gate's DLM Automation with SQL Continuous Integration. For this case, since migrating to a new project format wasn't possible until later, I worked with the native sql project from the SQL Server Data Tools in Visual Studio 2015.\r\n\r\n Terminology\r\n\r\nBuild Controller: This is like Service broker. It handles the queuing and assignment of the defined builds to the appropriate agents. It doesn't do any actual building, but instead handles the delegation of the work.\r\nBuild Agent: This is the \"wrapper\" for msbuild. It does the actual work of building whatever you pass to it.\r\n\r\nInitial Setup & Install\r\n\r\nConfigure Build Service Wizard\r\nChoose the configure option of just the build service\r\n\r\n\r\nConfigure collection \r\n\r\nChoose configuration options for the build services\r\n\r\n\r\nSetup build service account\r\n\r\n\r\nFinished with configuration. After running checks and resolving any issues (I thankfully had none for this simple install of just build controller/agent; you can proceed\r\n\r\n\r\nCreate Build Agents\r\n\r\n\r\n Future Configuration of Build Agents\r\n\r\nOnce you've finished the install of the build service and agents, you need to configure them. After completing the install the Team Foundation Server Administration Console should open, if not open manually (start menu)\r\n\r\nReview build service configuration\r\n\r\nBuild Configuration Pane\r\n\r\nReviewing Build Agent Properties\r\n\r\n\r\nSetup of Build\r\n\r\nCreate new build\r\n\r\n\r\nset trigger to continous integration\r\n\r\n\r\nmap your working folder to the database project\r\n\r\n\r\nsetup the process details\r\nMake sure to the map the project to the .sqlproj file to only build the sqlproj. You can adjust other items as you desire, but this should cover the core settings.\r\n\r\n\r\nCopy Local\r\nMake sure to have the new profile copied locally as part of the build or it won't have any publish profile copied when the build is triggered, and therefore might result in hours of you wondering why it is ignoring your profile target settings (true story). You can configure to not do this of course, if you want to have a fixed file on your drive instead of copying from source control each time. Recommendation - Prepare a Publish Profile file\r\n\r\n\r\nSetup Parameters for Build\r\nThe arguments I choose to use were:\r\n(note the publishprofile parameter which wasn't in older versions of SSDT)\r\n\r\n    /t:Build /t:Publish /p:SqlPublishProfilePath=foobar.publish.xml\" /p:PublishScriptFileName=foobar.publish.sql /p:TargetDatabaseName=\"foobar\";TargetConnectionString=\"Data Source=localhost;Integrated Security=True;Pooling=False\" /p:PreBuildEvent= /p:PostBuildEvent=  /p:VisualStudioVersion=14.0\r\n\r\nThe infuriating thing about working through this is that when it doesn't find the publish profile, it defaults to the \"Deploy\" settings, so in my case it kept trying to deploy the changes to a local database named the same thing as my project. Please don't waste the same amount of time grinding your teeth at MSBuild as I did, and watch out for the paths to be correct! Make sure you have it set to copy the publish profile over or it will not exist and default to the deploy settings.\r\n\r\nWhen I omitted the VisualStudioVersion option, it had an error \"unable to connect to target server\"...... 2 days of work later I realized the version parameter was critical for it to use the right msbuild version and deploy. I didn't go into more research on this, so feel free to comment if you more details on this.\r\n\r\n\r\n\r\n Running MSBuild Manually\r\n\r\nA few tips from my work with launching the process to build the database locally, if you experience an issues. This is work I was doing with MSBUILD 14 (visual studio 2015). I would trigger msbuild and it would deploy the database, but never exit the process to report success to powershell allowing my script to continue.\r\n\r\n`text\r\n/p:UseSharedCompilation=false               -- (Roslyn compiler bypassed http://bit.ly/1WmMVzx)\r\n/m:4                                        -- (limit to only 4 cores http://bit.ly/1VSl9uF)\r\n/nr:false                                   -- same link above\r\n/verbosity:quiet\r\n/p:Configuration=Release                    -- don't need debugging for this output, so just output release\r\n/p:DebugSymbols=false                       -- no need for extra debugging, potential improvement in timing to get rid of this\r\n/p:DebugType=None\r\n/t:Build;Deploy                     -- optional. Could rebuild if you want to\r\n/p:PreBuildEvent=                   -- bypass any prebuild/post build events if you have something doing copying of files around (optional)\r\nupdate after got everything working:\r\n/p:VisualStudioVersion=14.0 -- ensure you match the version of msbuild you need\r\n`\r\n\r\nOptional: If you have further issues with 2015 and want to disable globally the node reuse settings then you can do this with a registry entry. Following the directions from TechDocs I did this.\r\n\r\n\r\n\r\nResources\r\n\r\n MSDN Documentation Tasks\r\n\r\nCreating tasks is documented in\r\n\r\nTask Writing: Visual Studio 2013.aspx)\r\nMsBuild Tasks.aspx)\r\n\r\nThe MSBuild XML project file format cannot fully execute build operations on its own, so task logic must be implemented outside of the project file.\r\nThe execution logic of a task is implemented as a .NET class that implements the ITask interface, which is defined in the Microsoft.Build.Framework namespace.\r\n\r\nThe task class also defines the input and output parameters available to the task in the project file. MSBuild Tasks.aspx#Anchor_0)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-06-14-omni-compare-a-free-tool-to-compare-sql-instances",
        "content": "---\r\ndate: \"2016-06-14T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nsql-server\r\ntitle: 'OmniCompare: A free tool to compare SQL Instances'\r\ntoc: true\r\n---\r\n\r\ncomparing instances\r\n\r\nWhen working with a variety of instances, you can often deal with variances in configuration that might impact the performance. Without digging into each instance you wouldn't know immediately that this had happened. There are fantastic tools, like Brent Ozar's SP_Blitz, but this doesn't focus on every single configuration value and cross instance comparison. To supplement great material like that a tool like OmniCompare is great.\r\n\r\n side by side comparison\r\n\r\n\r\nI am definitely adding to my list of great sql tools. OmniCompare provides a side by side comparison of various configuration and system related values in a side by side format so you can easily see variances in basic configuration.\r\n\r\nrelease post\r\n\r\nI read the post about this by Phil Grayson here . He's got some great examples of what you could use it for, such as auditing, performancing tuning, synchronizing server settings, and more. The link for OmiCompare to try it out\r\n\r\n synchronization\r\n\r\nApparently, it's got the ability to synchronize some of the configuration settings so you could use a template to help setup a new configuration of sql server quickly. I haven't had a chance to try that piece out, but I will be exploring it for sure! It might be something that is in the works, as I couldn't find the options to synchronize currently in the tool. Sounds a lot more elegant than my homebrewed scripts that a nest of code needing some cleaning.\r\nWell done Aireforge team!\r\n\r\nEasy Visual Summary of differences\r\n\r\nQuite a few ways to filter and sort down to the information you care about.\r\n\r\n\r\n\r\n Listing of the servers you want to compare\r\n\r\nAdd more or import a list of them to easily do a comparison on configuration in the environments.\r\n\r\n\r\n\r\nConfiguring A Server\r\n\r\nSimple and quick to add a server, as well as tag them so you can easily compare based on whatever grouping you see fit. For example, you could compare all common versions, all UAT type environments, etc.\r\n\r\n\r\n\r\n Configuration Differences\r\n\r\nMakes it very easy to immediately just view configuration scoped differences between each\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-06-17-ssms-tools-pack-a-handy-tool-for-generating-crud",
        "content": "---\r\ndate: \"2016-06-17T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nsql-server\r\ntitle: SSMS Tools Pack - A Handy Tool for generating CRUD\r\n---\r\n\r\nSo I've had this tool around for a while, but never found much usage out of it to be honest. I didn't end up writing a review as I had other tools that did text replacements, and history/session saving. I've always considered this tools implementation of SQL History/Tabs saver the best period, even over Red Gate Tab History, SSMSBoost, etc. However, recommending the tool solely based on it's fantastic history saver wasn't really something I was going to do.However, having to generate some CRUD procs lately I found a new reason to appreciate this tool. I dusted it off, updated to the latest, and looked for the CRUD option I remember it having. Sure enough I ended up saving myself a lot of time and generated procs that were all standard with what I wanted to create. This gets my hearty approval to avoid tedious grunt work on creating procs. Since the tool throws in a great history/session saver to avoid losing work, it's even more of a recommended tool!\r\n\r\nCRUD Generator\r\n\r\nDisclaimer: I was provided with a license to give me time to fully review. This doesn't impact my assessment of the tool. I don't recommend tools without actually using them and seeing if they'd actually benefit me in my work\r\n\r\nSSMS Tools Pack\r\nFirst, I know there are some great stored procs/scripts that people have written to do this. I appreciated those, but found I was going to spend a lot of time trying to customize to get the error handling and other scripted pieces in, so I revisted SMSS Tool Pack.\r\n\r\n create CRUD from context menu for entire database\r\n\r\n\r\n\r\ngeneral options\r\n\r\n\r\n\r\n replacement text options\r\n\r\nThis has some potential to be very helpful! You could generate the user, date and time, and more to generate some comment headers and more.\r\n\r\n\r\n\r\nreplacement text example\r\n\r\n\r\n\r\nselect template\r\n\r\n\r\n\r\ninsert template\r\n\r\nI prefer a begin try and error catch output syntax. I was able to encapsulate the CRUD generator statements with the syntax I preferred, and no longer had to manually manipulate each file to get it where i wanted it. This was a lot of time saved!\r\n\r\n\r\n\r\n update template\r\n\r\n\r\n\r\nwarning - case sensitive parameters\r\n\r\nMake sure to keep the case correct on the variables. This is case sensitive.\r\n\r\n\r\n\r\n saving you a lot of work...\r\n\r\nOnce you've clicked the generate CRUD the magic happens.\r\nThe results were a large list of prebuilt stored procedures for doing all the CRUD operations needed, with no extra work required. Win!\r\n\r\n\r\n\r\nOther\r\n\r\nSearching the history of previously executed queries, versions of files edited, and sessions of tabs is all excellently handled in the SQL History Search extension. My favorite part is the useful status messages such as\r\n\r\n\r\n\r\n History of Execution\r\n\r\n\r\n\r\nupdates\r\n\r\nSome recent updates were released with version 4 that might be beneficial for your workflow such as support for SSMS 2016, renaming of tabs, better insert generator, and some other things. Check out the website for more details.\r\n\r\n its not CRUD.... it's quite nice!\r\n\r\nWhile it doesn't really do full formatting or other things, the organized query execution, CRUD generator, and some other features make it a nice tool if you have the budget to purchase. It's a especially a good tool for those who want to generate CRUD procs easily.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-06-26-sql-compare-12-initial-look",
        "content": "---\r\ndate: \"2016-06-26T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nsql-server\r\ntitle: 'SQL Compare 12: Initial Look'\r\ntoc: true\r\n---\r\n\r\nI know there have been a few other folks going into more detail on SQL Compare 12 (beta), but I thought I'd share just a few looks at the new design. Looks pretty slick, and I like where the design is going. Just a quick look, as I'm sure there will be more to cover when it's finally released. Until then....\r\n\r\nSome GUI Improvements\r\n\r\nVery clean. Not too drastic of a change to mess with current workflows. Still, nice to see some clean design like this.\r\n\r\n\r\n\r\n Options dialogue\r\n\r\nSame options, just slightly different navigation/ui design to get there.\r\n\r\n\r\n\r\nComparing databases\r\n\r\nComparing running... saving me a massive amount of work in not creating scripts by hand :-)\r\n\r\n\r\n\r\n Comparison matches\r\n\r\n\r\n\r\nFinal thoughts\r\n\r\nAgain, this was just a quick peek at what I've looked at. Overall, I like the design choices.\r\nUI Design improvements are nice. I would say the next stage would be seeing this better design roll-out into better report styling & formats, making it consistent. Additionally, while the UI improvements are nice, the code diff viewer at the bottom looks pretty similar, and I'd love the ability to have this improved a little. Perhaps even the option to use \"external diff viewer\" and pipe it into my preferred diff viewer (Araxis Merge for me).\r\nOfficially, I'm part of the Friend's of Red Gate community now, so I'll probably be posting a bit more on their tools as I've got the chance to work with more of them now. I've always been a big fan of their products, and now I get a chance to be part of their community and help with testing, feedback, and advocating their great stuff when it would help folks out.\r\nFinal thought... why can't all great tools have a full Solarized Dark theme? ;-)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-07-01-glasswire-(giveaway-included)-networking-monitoring-even-a-caveman-could-use",
        "content": "---\r\ndate: \"2016-07-01T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nsql-server\r\ntitle: 'Glasswire: (Giveaway Included) Networking Monitoring even a caveman could\r\n  use'\r\ntoc: true\r\n---\r\n\r\nGiveaway details at the bottom for those interestedDealing with development & sql servers, I like to know what type of network traffic is happening on my machine. What is the overhead of monitoring on network bandwidth, what is communicating across servers or even externally?\r\n\r\nWhat is phoning home?\r\n\r\nYou can create perfmon counters, but realistically sometimes I just want a easy quick overview of network traffic with minimal overhead. I have been using a utility I came across called Glasswire, and found this tremendously helpful. I'd highly recommend taking a look at this. I've installed on a couple of the sandbox sql servers I have worked with and found it really great for evaluating the network traffic occurring from the monitoring services running on them.\r\nDisclaimer: I have been provided with a free license to allow me to review the product in detail. This doesn't impact my assessment of the tool. I just love great tools and try to help other developers find them.\r\n\r\n Network traffic monitoring\r\n\r\nSounds boring, but Glasswire changed my perspective on this. I always want to know what's \"phoning home\" and using up my bandwidth, but until I tried this app, I never have found something that tracked and reported on it in a clean user friendly way. Process Hacker is my preferred task manager, and it can provide some metrics when running, but not a long term history, and not in a user friendly format for analysis.\r\n\r\nComparing timeline\r\n\r\nI found this great as a simple way to compare two SQL server monitor tools traffic against each other. I wanted to know the network traffic load they were generating, and this was a great way to get some quick transparency on the network impact. In this example, the test wasn't perfect as they had slightly different detail level tracking configuration, so just take this as an example.\r\n\r\n\r\n\r\n Comparing Usage Metrics\r\n\r\n\r\n\r\nSee activity\r\n\r\nI really like the transparency of seeing what network activity is occurring on my system. I've found myself evaluating why apps would need to connect rather than just allowing everything through.\r\n\r\n\r\n\r\n Easiest firewall tweaking I've seen\r\n\r\n\r\n\r\nWould I recommend\r\n\r\nCompletely! I've found this to give me a transparency into the network ativity in a great way.\r\nPROS\r\n\r\nBeautifully thought out design\r\nCreates a great awareness of network activity, allowing you to be more proactive on what you allow to send data out\r\nCONS\r\nSome \"power user\" functionality would be nice, such as being able to customize or get details from the desktop widget, add special alerting on specific apps whenever they request network access, etc. These are small things though. I think the overall design is very elegant and well thought out\r\nPrice. For normal users this is really expensive for the pro version. However, if you are just interested in some basic monitoring without a long history and desktop toast alerts, you can get the free version and still get great value from it.\r\n\r\n Interview with Developer\r\n\r\nI thought the app was unique enough in design and function that it would be great to get a short interview with the owner to share a bit about his development approach, goals, and overall story.\r\n\r\nTell me a little bit about yourself and your company.\r\nBefore GlassWire we made a webcam virtual driver software that allowed you to use your webcam with multiple applications simultaneously.  That company was acquired a couple years ago.  Making a sophisticated driver gave us experience with making drivers, so we used that experience to make our network monitoring driver.\r\nSince launch we have been surprised by how many people use GlassWire to keep their data usage low and save money and resources.  For example some of our customers have boats and they are on very strict data plans out on the ocean, so they use GlassWire to see what's wasting their data and also block apps they don't want to use while out at sea.\r\nAfter we launched we were surprised by how feedback was so positive right away.  I was worried that nobody would want the software at all, but people seem to like it and we have now had close to 4 million downloads.\r\n\r\n What made you want to build a network application like Glasswire?\r\nI always felt that I couldn't see what was happening with my PC's network usage so I built GlassWire for myself so I could instantly see what was happening.  I also had some relatives who lived in a remote area and could only use Satellite Internet access.  Satellite only gives you a little data, then throttles you so it's very useful to see what apps are wasting your data. GlassWire also has a built in bandwidth overage monitor to help with that.\r\n\r\nWhy was QT chosen as your graphs/design?\r\nI tested a lot of different tools to see what was happening on the network but I found them difficult to use, plus I couldn't find any that could go back in time and show me past network usage so our team worked together to build GlassWire. QT allows us to build a beautiful UI and make changes easier over time.\r\n\r\n What's been some of your hardest decisions in designing the application?\r\nAfter launching we found that Bittorrent users were causing GlassWire to use too much memory/resources on their PC because Bittorrent communicates with so many hosts simultaneously in such a short time period.  We had to redesign GlassWire to use less resources for these users, and I blogged about it here https://blog.glasswire.com/2016/03/29/how-glasswire-1-2-saves-your-memory-and-resources/.  There were a lot of different hard decisions we had to make, like adding \"loading...\" in some places in the UI to take the load off of GlassWire for users who had too many hosts.  I was worried users may find these short delays annoying bit fortunately nobody seemed to be upset about it and we are continuing to grow, and GlassWire now uses significantly less resources for everyone.\r\n\r\nDo you have a design philosophy that helps balance more features with simplicity?\r\nI try to look at other popular applications and study what makes them successful.  Currently we are working on our Android application and the work on Android has helped me come up with some ideas on how to improve yet simplify our GlassWire desktop software.\r\n\r\n What's your thoughts on implementing user feedback vs bringing design choices that no one even thought about?\r\nWe love to get user feedback in the forum and on Twitter, etc...  For me it's easy to come up with feature/design choices because I want GlassWire for myself.  I think I'm a pretty normal person and usually the things that I want others want too.\r\n\r\nAny future projects you want to accomplish (not a roadmap, but general things you'd love to tackle)\r\nWe're excited about bringing GlassWire to mobile and Mac, but it's not easy so we're trying to make sure our Mac/mobile versions have the same high quality as our desktop software.  We don't want to cut corners, so I hope our fans will be patient!\r\n\r\n Can you tell me a few of your design philosophy decisions that drove some of those simple UI differences that are a bit uncommon? (like the graph refreshing smoothly without 1 second gaps)\r\nSince we built the software for ourselves we tried to create a simple layout that we preferred, kind of like a web browser.  We tried some 1 second intervals but it made the graph look jerky and it hurt the eyes so the team spent a lot of time making the smooth graph we ended up with. One of the main things I wanted myself was to be able to see what caused a spike on the graph and with GlassWire you can just click the spike, then see what hosts/apps were involved in the spike.\r\n\r\nGiveaway Details\r\n\r\nGlasswire was kind enough to provide me with a license for their Elite version (currently $199) which is a onetime fee. You can get the hookup!\r\nTo get the hookup on this... I'm making it technical since this is not for sweepstake surfers but my techy SQL friends....\r\nReminder: This is a Windows application, not Mac.\r\nDrawing will occur end of July and winner name will be posted here and notified in Twitter direct message\r\n\r\n Giveaway Result 2016-08-11\r\n\r\nCongrats to Tim! I'll be sending you the license details.\r\nHope you enjoy and thanks to Glasswire for sponsoring this. Give the free version a shot, even if you aren't planning on going pro. It's a great tool for anyone to increase transparency on what's really happening with their system.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-07-12-improvements-with-ssms-2016",
        "content": "---\r\ndate: \"2016-07-12T00:00:00Z\"\r\nexcerpt: Improves to SSMS (Sql Server Management Studio) continue. Use the latest\r\n  version instead of using the version bundled in older versions of SQL Server Installation\r\n  media to ensure the latest features are available.\r\nlastmodifiedat: \"2018-03-30\"\r\ntags:\r\nsql-server\r\ntitle: Improvements with SSMS 2016\r\n---\r\n\r\n info \"Updated: 2019-01-24\"\r\n Improved options to install through Chocolatey package. Use command choco upgrade sql-server-management-studio and you'll simplify the installation process greatly.\r\n Also for servers, consider Azure Data Studio as much smaller download and might provide what you need to do basic management without a length install and download.\r\n\r\n info \"Updated: 2018-03-30\"\r\n Use SSMS 2017 when possible now. It has continued to be improved. Current download page for SSMS 2017\r\n If you want a shortcut to install, check out this post: Update SSMS With PS1\r\n\r\nThe staple of every SQL Server developer's world, SSMS has been impossible to usurp for the majority of us. However, it's also been behind the development cycle of visual studio, and didn't have continual updates. That changed recently, as I've previously posted on. SSMS (Sql Server Management Studio) now being decoupled from the SQL Server database releases.I've been enjoying some of the improvements, especially as relatest to the built in execution plan viewer. I use SQL Sentry Pro, but for a quick review, any improvements to the default viewer is a welcome addition!\r\n\r\nLive Statistics View\r\n\r\nYou can see the statistics update as it's running.\r\n\r\n\r\n\r\n Side by Side Comparison of Plans\r\n\r\nThis is something that is fantastic. A good step in the right direction for helping compare plans quickly. This is a feature I'd love to see added to other tools like SQL Sentry Plan Explorer.  When plans don't vary significantly in their structure, this type of view is great for quickly viewing variances.\r\n\r\n\r\n\r\nUsability\r\n\r\nYou can actually drag your mouse to pan a plan... enough said. This should have been there a long time ago.\r\n\r\n comparison of properties\r\n\r\nThe properties pane also has an overhaul with some really useful comparison information, helping you identify what is now different.\r\n\r\n\r\n\r\noverall\r\n\r\nReally liking the improvements I've seen. There are a lot of things about SSMS I'd like to see improved, and with a regular release cycle the future for SSMS looks promising!\r\nI'll be really happy once the Visual Studio dark theme has made it's way over... I swear everything just runs faster with a dark theme ;-)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-08-11-regex-with-sql-server-sql-sharp",
        "content": "---\r\ndate: \"2016-08-11T00:00:00Z\"\r\ntags:\r\nregex\r\nsql-server\r\ncool-tools\r\ntitle: Regex With SQL Server - SQLSharp\r\ntoc: true\r\n---\r\n\r\nIn the context of my developer machine, I had log files I wanted to parse through. I setup a log library to output the results on a test server to a sql table instead of text files. However, this meant that my \"log viewers\" that handled regex parsing weren't in the picture at this point. I wanted to parse out some columns from a section of message text, and thought about CLR as a possible tool to help this.Ideally, I wanted to feed the results for analysis easily into power bi, and avoid the need to create code to import and parse out fields. Since I knew the regex values I wanted, I thought this would be a good chance to try out some CLR functionality for the first time with SQL Server 2016 + CLR Regex parsing.\r\nI ran across SQL# and installed. The install was very simple, just downloaded a SQL script and ran it, adding a final \"reconfigure\" statement to ensure everything was good to go.\r\n\r\nSQLSharp (SQL)\r\n\r\nI used the free version which provided great regex parsing functionality.\r\n\r\n\r\n\r\nSimple to use\r\n\r\nConstructing the following query parsed the results easily, with no extract coding/import process required.\r\n\r\n{{% gist b067b6d87cda11d70c608298cff8c0d4 %}}\r\n\r\n\r\n\r\n Performance\r\n\r\nThis was just an isolated 1000 record test, so nothing exhaustive. I compared it to a table function that parsing strings (could probably be optimized more). For the purpose of running a simple log parsing search on 1000 rows it did pretty good!\r\nFor better work on parsing of strings, there are detailed postings out there by Aaron Bertrand, Jeff Moden, and others. My scope was specifically focused on the benefit for a dba/developer doing adhoc-type work with Regex parsing, not splitting delimited strings. The focus of most of the articles I found was more on parsing delimited string. However, I'm linking to them so if you are researching, you can be pointed towards so much more in-depth research on a related topic.\r\n\r\n\r\n\r\nThoughts\r\n\r\nThe scope of my review is not covering the proper security setup for CLR with production, CLR performance at high scale, or anything that detailed. This was primarily focused on a first look at it. As much as I love creative SQL solutions, there are certain things that fit better in code, not SQL. (heresy?) I believe Regex/advanced string parsing can often be better handled in the application, powershell, or other code with access to regex libraries.\r\nIn the case of string parsing for complex patterns that are difficult to match with LIKE pattern matching, this might be a good resource to help someone write a few SQL statements to parse out some log files, adhoc ETL text manipulation, or other text querying on their machine without having to add additional work on importing and setup.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-08-15-does-sp-rename-on-a-column-preserve-the-ms-description",
        "content": "---\r\ndate: \"2016-08-15T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Does sprename on a column preserve the msdescription?\r\n---\r\n\r\nDid some checking as couldn't find help in the MSDN documentation. My test on SQL 2016 shows that since the column_id isn't changing, the existing mapping of the description for the column is preserved.\r\n\r\n{{% gist bf8fc1a0b0c3200da6dd95f2bdeb3314 %}}\r\n\r\n\r\nI know it's probably pretty obvious, but I had someone ask me, so figured proving the mapping for ms_description is maintained would be a good thing to walk through. Score another point for Microsoft, for design practices\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-08-18-install-ready-roll-via-command-line",
        "content": "---\r\ndate: \"2016-08-18T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\ncool-tool\r\nredgate\r\nsql-server\r\ntitle: Install ReadyRoll via Command Line\r\n---\r\n\r\ncommand line install options\r\n\r\nReadyRoll has some great features, including the ability to use without cost on a build server. If you want to ease setup on multiple build servers you could create a simple command line install step against the EXE.\r\n\r\n future changes\r\n\r\nReadyRoll was recently acquired by Redgate, so the installer options may change in the future to be more inline with the standard Redgate installer. For now, this is a way to automate an install/updates.\r\n\r\nautoupdating via Ketarin\r\n\r\nI personally use Ketarin to help me manage automatically updating apps like SQL Server Management Studio. I've uploaded a public entry for ReadyRoll to automate download and install of the latest ReadyRoll version when available. For more detail on how to use Ketarin see my earlier post on Automating SSMS Upgrades\r\n\r\n command line options\r\n\r\nFind the path of the installer\r\nRun ReadyRoll.msi /exenoui /qn for a silent install.\r\n\r\n\r\nFor automated setup and install use the following code with Ketarin\r\n\r\n{{% gist bfde8f5846555183e3abd4e7575bc2a9 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-08-19-sql-2016-brief-overview-on-some-new-features",
        "content": "---\r\ndate: \"2016-08-19T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: SQL 2016 - Brief Overview on some new features\r\n---\r\n\r\nThese are notes taken from the Houston SQL Pass User group from July. This presentation was given by John Cook, (Data Platform Solution Architect Microsoft) who did a great job with limited time on providing some great details on the new functionality with SQL 2016. To follow him, take a look at sqlblog.com where he posts or follow him on twitter. Thanks to him for the overview.\r\n\r\n ####\r\n JohnPaulCook (@JohnPaulCook) on Twitter\r\n\r\n Microsoft Data Platform specialist and Registered Nurse\r\nscript data-preserve-html-node=\"true\" async=\" src=\"platform.js\" charset=\"UTF-8\"/script\r\n\r\n ####\r\n John Paul Cook\r\n\r\n SQL Blog - Blogs about SQL Server, T-SQL, CLR, Service Broker, Integration Services, Reporting, Analysis Services, Business Intelligence, XML, SQL Scripts, best practices, database development, database administration, and programming\r\nscript data-preserve-html-node=\"true\" async=\" src=\"platform.js\" charset=\"UTF-8\"/script\r\n\r\n\r\ncloud first\r\n\r\nMost of the new features included in 2016 have been tested in the cloud. They are implementing cloud-first with features. Therefore, most on prem features have been throughly tested in the new world, sometimes even up to months.\r\n\r\n dynamic data masking\r\n\r\nIf you know a specific value you could get the results back in a specific query by putting in where clause. This would retrieve the row masked, but you still knew the results due to this \"brute force attack\". Dealing with security means you'd prevent adhoc queries anyway. You want to ensure that that scenario doesn't happen.\r\nThis would be categorized more as obfuscation. This is not the same as encryption.\r\nNew grant permission for UNMASK\r\n\r\nencryption\r\n\r\nEncryption at column level\r\nDeterministic: need this for being able to search/join among different tables\r\nRandom: Good for increasing the difficulty of breaking the encryption.\r\nEncryption increases to the size of the data, taking up more size\r\nHas some limitations on Collation. The example was COLLATE LatinGeneralBIN2\r\nThis is offloaded to the client which converts the value with ado.net 4.6.1. This means a certain compatibility would need to be maintained to use this with legacy applications. This is done on the client. Unless you give the key to the dba, they can't see the information.\r\nAdditional connection string value is required, per SSMS has to convert and interpret this value.\r\n\r\ncolumn encryption setting=enabled\r\n\r\n\r\n\r\n stretch\r\n\r\nStretch is more \"stretch table\" to Azure. This means you'd bind a function to your sql server with the logic to archive. This would let you store very cold data without having to maintain locally.\r\nAnother positive to this is that each table is contained as it's own \"database\" in Azure. They maintain the backups for you, so your backup windows are not impacted. You only have to backup the local data.\r\n\r\nTemporal Database\r\n\r\nMicrosoft keeps track of all your changes in a table. You have to enable on each table individually. This functionality stores the history of all changes to ensure this history is tracked. This used to require a lot of coding.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-08-23-ssms-connection-color-with-sql-prompt-&-ssms-boost",
        "content": "---\r\ndate: \"2016-08-23T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nredgate\r\nsql-server\r\ntitle: SSMS - Connection Color with SQL Prompt & SSMSBoost\r\n---\r\n\r\nIf you haven't explored the visual color coding of tabs based on pattern matches with SQL Prompt, I'd suggest you check this out. Earlier iterations of Red Gate's SQL Prompt did not change tab color immediately when the connection was changed. Red Gate's tab color could get out of sync occasionally, so I stopped depending on it.\r\n\r\nApparently this has been improved on and my testing now shows that the tab recoloring for connections is changing when the connection is updated immediately. This is a great visual indicator of what your query window is connected to.\r\n\r\n\r\n\r\n\r\n\r\nI'm still a fan of SSMSBoost combined with SQLPrompt, but I'm finding myself needing less addins as SQLPrompt has been enhanced. The ease of configuration and setup is a major factor as well.\r\nSSMSboost Preferred connections are great for when needing to quickly context switch between different connections in the same window (and sync with object explorer)\r\n\r\n\r\n\r\nResulting textbox overlay\r\n\r\n\r\n\r\npossible improvements for SQLPrompt\r\n\r\nI think that SQLPrompt would have some great productivity enhancements by implementing something similar to SSMSBoost preferred connections. Here is a UserVoice item on it. Add your vote\r\nSQLPrompt enhancements to synchronize object explorer connections based on the current query window would be another great option. I created a user voice item on this here.\r\n\r\nOverall, I'm finding myself depending on SQLPrompt more. As a member in the Friend of Redgate program, I've had access to try some of the new beta versions and find the team extremely responsive.\r\n\r\nDisclaimer: as a Friend of Redgate, I'm provided with app for usage, this doesn't impact my review process.\r\n\r\n \r\n [\r\n SSMSBoost add-in - productivity tools for SSMS 2008 / 2012 / 2014 (Sql Server Management Studio)](http://www.ssmsboost.com/)\r\n\r\n Productivity SSMS add-in packed with useful tolls: scripting, sessions, connection management. Plug-in works with SSMS 2008 and SSMS 2012 SQL Server Management Studio: SSMS add-in with useful tools for SQL Server developers\r\nscript data-preserve-html-node=\"true\" async=\" src=\"platform.js\" charset=\"UTF-8\"/script\r\n\r\n ####\r\n Code Completion And SQL Formatting In SSMS ' SQL Prompt\r\n\r\n Write and format SQL with SQL Prompt's IntelliSense-style code completion, customizable code formatting, snippets, and tab history for SSMS. Try it free\r\nscript data-preserve-html-node=\"true\" async=\" src=\"platform.js\" charset=\"UTF-8\"/script\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-08-29-remote-desktop-workflow-improvements",
        "content": "---\r\ndate: \"2016-08-29T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nsql-server\r\nramblings\r\ntitle: Remote Desktop Workflow Improvements\r\ntoc: true\r\n---\r\n\r\nRemote server management is a fact of life for folks involved with sql server. Since we work so often with remote machines I looked into a few tools that provided a better workflow than the default Microsoft tools. This one came out as the winner by far.\r\n\r\nA better remote desktop manager\r\n\r\nFirst off, if you are using RDC.... why? At least move up to RDCMan, a microsoft tool that allows for much quick context switching between machines, inherited password settings for a group of servers, and more.\r\nNext, if you are using RDCMan... and perhaps you work with a lot of remote machines, perhaps there is an even better option? There are some great alternatives out there, and in my experience, with a little learning curve, the one I've worked on with Devolutions is fantastic!\r\nLink to app: Devolutions Remote Desktop Manager \r\n\r\n Devolutions Remote Desktop Manager\r\n\r\nDisclaimer: I have been provided with a free license because I mentioned I might review the product. This doesn't impact my assessment of the tool.\r\nI'm a big fan of this app after having used it for months to get a feel for it. I've found it helpful when working with remote desktop machines, as well as some basic credential management. For folks dealing with lots of remote machines, I think this tool is well worth the investment and it has my stamp for one of my top essential tools in my cool-tools toolkit.\r\nThere is a free version that is less powerful, but a still a good upgrade from the standard RDM from microsoft. The free version comparison is located HERE\r\n\r\nSynchronizer... some potentially powerful automation here for managing connections to remote machines\r\n\r\nHaving to deal with a lot of remote machines, especially ones that change IP addresses periodically can be annoying for a DBA trying to remotely connect again. Devolutions Remote Desktop Manager (RDM) has some really cool functionality that can help automate refreshing these lists from a variety of sources.\r\n\r\n\r\n\r\n Synchronized Session Listing\r\n\r\nYou can setup a synchronized session listing based on csv, activedirectory, spiceworks, and more. Eventually, I believe they'll have an amazon ec2 synchronizer as well. In the meantime, with some powershell magic we can create a synchronized listing of remote machines to work with, no longer having to update ip's manually in a Amazon EC2 system.\r\n\r\n\r\n\r\nOutput Results from Powershell into CSV Source\r\n\r\nsetup a powershell script that would obtain EC2 instances and output into a csv file.\r\nI found pieces of the needed code from various sources and modified to work for me. It's not elegant, and much better ways are available I'm sure. This was helpful to me though and got the job done!\r\n\r\n{{% gist f38807512bbbc14a5aab0680dccd4fba %}}\r\n\r\n\r\n\r\n\r\n Automatically keeping things up to date\r\n\r\nSynchronizing automatically gives us the flexibility to have a scheduled script to run the powershell command to get a new list of machines, and have the synchronized list run automatically maintain the latest connection information. In my case, I setup the powershell script to run every X hours so my connection information was always up to date.\r\n\r\n\r\n\r\norganized results\r\n\r\nThanks to the synchronizer, I know have two folders with separated instances for production and QA, allowing me to quickly access with minimal effort!\r\nThis would allow me to set credentials as well, to reduce the effort in logging in for each of the sets of instances\r\n\r\n\r\n\r\n Other Cool Stuff\r\n\r\nMultiple Monitor Support\r\n\r\nMicrosoft remote desktop connections have support for multiple monitors by spanning display. I tested this out with an unconventional setup. I have 3 24inch monitors, with 1 landscape in the middle surrounded by 2 in portrait mode. It had some problems with this as it's trying to create a spanned clone, however, if I had a typical setup, I think this would work fine, (such as a dual screen setup with the same orientation).\r\n\r\n\r\n\r\n Continual Updates\r\n\r\nI'm continually getting updates on this product. In any inquiries on their forum, I've seen helpful responses from staff within the day, with great support help (such as powershell tips on accomplishing what I needed with synchronizers)\r\n\r\n\r\n\r\nBetter Local Password Management\r\n\r\nI'm a big fan of Lastpass, but as I use it for personal password management, I wanted to keep my work related passwords entirely separate. Devolutions RDM offers some nice password management options and credential inheritance setup.\r\n\r\n\r\n\r\n Jump\r\n\r\nIf you have a scenario where you need to remote into one machine and then remote from that machine to another, things can get very confusing with copying/pasting/navigating. RDM solves this by having a \"RDM Jump Agent\", basically a service that allows you to set a remote desktop connection as a \"jump point\" and then connect through that connection to the next destination, while using one remote desktop window in the app. For those scenarios, I found it incredibly helpful.  Best scenario... just avoid having to jump in the first place :-)\r\n\r\nHandles Remote Desktop Connections + The Kitchen Sink\r\n\r\nHandles a breadth of different types of remote connections, such as Chrome remote desktop manager, Hyper V, remote command line, powershell sessions, Amazon S3, Amazon AWS console, Citrix ICA/HDX, and more.\r\nCan wrap up the trick of running SSMS (Sql Management Studio) with \"RunAs\" as a different domain and user, allowing locally run SSMS to be connected to AWS, or other environments.\r\n\r\n\r\n\r\n Other Odds and Ends\r\n\r\nLots of features, so I'm just covering some of the highlights that are of interest to me.\r\n\r\nQuickly ping and get status\r\nView event logs from remote machine loaded in your local event viewer\r\nList services\r\nRun powershell script remotely with RDM-Agent (executes the script as if running locally, and could do this in parallel with other instances)\r\n\r\n\r\n\r\nSummary\r\n\r\nI review quite a few apps, but this one is really difficult to review in detail as it covers such a range of functionality I've never even touched. The only con to the app I'd say is it can have a bit of a higher learning curve than using the plain old Remote Desktop Manager from windows due to the breadth of functionality it covers. However, once using this app, and discovering little pieces of functionality here and there, this is in my permanent \"essential tools\" toolkit.\r\n\r\n Free version\r\n\r\nThey have a free version that will suffice for many people. The pro version has more enterprise level focus, so give the free one a shot if you are looking for a basic improvement to your RDM workflow\r\n\r\n ####\r\n Remote Desktop Manager - Remote connection and password management software\r\n\r\n Remote Desktop Manager is an all-in-one remote connections, passwords and credentials management platform for IT teams trusted by over 270,000 users in over 120 countries.\r\nscript data-preserve-html-node=\"true\" async=\" src=\"platform.js\" charset=\"UTF-8\"/script\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-09-14-lack-for-nothing",
        "content": "---\r\ndate: \"2016-09-14T00:00:00Z\"\r\ntags:\r\nmusic\r\nramblings\r\nworship\r\nfollower-of-Jesus\r\ntitle: Lack for Nothing\r\n---\r\n\r\nThe message of this song is one that that has been driving itself home as more and more important to me.\r\nWe are not viewed in our frailty as we come before God to worship Him. We are not judged and condemned. He doesn't look and tell us to come back when we are in a better place.\r\nInstead, just as the author of Hebrews writes (Heb 10), we can come with bold confidence, knowing that the way has been made for us in Christ, with us being viewed through the \"curtain\" of Christ. This is critical to every person wanting to draw closer to God, as without a realization that we should be able to approach with confidence and no judgement, we will hold back from fully diving into our relationship with Him.\r\n\r\n Hebrews 10:19-22\r\n   We have, then, my friends, complete freedom to go into the Most Holy Place by means of the death of Jesus. He opened for us a new way, a living way, through the curtain---that is, through his own body. We have a great priest in charge of the house of God. So let us come near to God with a sincere heart and a sure faith, with hearts that have been purified...\r\n   This has shaped my paradigm a lot. It's hard for me to hear about others feeling unworthy in coming before God, like they have to get things together before coming to Him. This is so antithetical to the nature of God and to all of what we see Christ exemplifying in His life. Christ never demonstrated superiority, judgement against those who don't demonstrate perfection. When He did bring out anger and judgement, it was focused on those who, in their misguided concepts of God, put barriers between God and His people.\r\n   So, just a friendly reminder that you \"Lack for Nothing\". Cheers!\r\n\r\n\r\niframe width=\"853\" height=\"480\" src=\"https://www.youtube.com/embed/TZgXnO1MtGU?rel=0\" frameborder=\"0\" allowfullscreen=\"yes\"/iframe\r\n\r\n\r\n\r\n{{% gist 03a51d7ea231974bb3ce10feb0d2ad6c %}}\r\n\r\n\r\n_disclaimer_\r\n   This is my first full production attempt with Ableton. I used Ableton Live Lite, with EzDrummer 2 to help generate some drums. Due to running out of time (and patience :-) I just wrapped this up instead of worrying about fine tuning every little piece. I've got some errant vocal parts, guitar parts, and all, but hope you can just look past this. I ran out of time and .... and patience to keep reworking.\r\n   I actually am working on a series to help fellow worship leaders and musicians just getting into recording learn more about utilizing VSTs, DAW's (like Ableton, Studio One 3), and more. It's bit confusing getting started, so hopefully as I share a little of my journey it might save you some work if you are wanting to try your hand at some creativity.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-09-17-ableton-live-&-lemur-setup-(from-a-windows-user)",
        "content": "---\r\ndate: \"2016-09-17T00:00:00Z\"\r\ntags:\r\nmusic\r\nramblings\r\ncool-tools\r\ntitle: Ableton Live & Lemur Setup (From a Windows User)\r\n---\r\n\r\nHad a chance to look at this program thanks to the generosity of the developer. They had a promotion that ended prematurely and they sent me a license as a goodwill gesture. Pretty fantastic service, and thanks to them for this.The documentation was a little sparse for the Windows setup, so I ran into some complications getting it to work.\r\n\r\nI have used TouchOSC, and had some difficulty with configuring it for windows. True to other's postings, Lemur didn't make this easier. Especially as a windows user, this is not a simple plug and play and go type of tool. If you are looking for that then you'd be better served by looking a Touchable, Conductr, or something similar that provides the full package.\r\nThe difference is that Lemur offers a complete \"development\" environment for templates and automation. The sky is the limit. You can even have a single ipad controlling multiple computers based on the midi being mapped on one controller to various destinations.\r\nTo get Lemur talking to Ableton Live (9.6), this is pretty much what worked for me after experimenting.\r\n\r\nInstall Lemur\r\nInstall Live Control 2 (basically just a set of template and automation scripts)\r\nCopy Live Control Template onto ipad (you can drag the file directly to the app in Itunes and hit sync)\r\nInstall LoopBe. Note I tried rptMidi as well as LoopMidi, and this is the only one that worked without issue for me on Windows 10.\r\nSetup Lemur Daemon service as shown in screenshot.\r\n\r\nIn ableton I setup as the following:\r\n\r\nIf you drag a drum track into a channel and activate for recording, you can tap in the \"play\" tab and change \"key\" to \"drum\". Then you can find the appropriate drum machine field by scrolling up or down.\r\nThis got the initial setup going so I was able to drop a drum track in, and tap out a rhythm. I think I'm going to have invest more time into it to get what I want, so the jury is out for me as a casual user whether or not I'll be able to leverage this vs TouchOSC or touchable. It's pretty powerful, just pretty complex!\r\nAs a person primarily focused with Live for DAW and home recording/experimentation, I'm probably not going to leverage fully at this time like others would. If I gain some more traction with it, I'll try and post some more useful updates. Both TouchOSC and Lemur are designed for customization. If you want to get going with limited setup, look at Touchable, Conductr, or another similar to that.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-09-19-setting-dbcc-1222-on-startup",
        "content": "---\r\ndate: \"2016-09-19T00:00:00Z\"\r\ntags:\r\nsql-configuration\r\nsql-server\r\ntitle: Setting DBCC 1222 on startup\r\n---\r\n\r\nThe following command is run to gain details on deadlocks.\r\n\r\n    DBCC TRACEON (1222,-1)\r\n\r\nHowever, once the SQL instance is restarted this flag is set back to disabled.\r\n\r\n\r\nTo enable it on the instance upon startup:\r\n\r\nOpen SQL Configuration Manager\r\nServices  Sql Service Instance  Properties  Startup Parameters\r\nAdd the following statement: -T1222\r\nConfirm the change by navigating to Advanced  Startup Parameters. This should be grayed out and display the new value that was added at the end with a delimited semicolon.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-09-20-daw-dive-1-getting-started-studio-one-3",
        "content": "---\r\ndate: \"2016-09-20T00:00:00Z\"\r\ntags:\r\nmusic\r\nramblings\r\ncool-tools\r\ntitle: DAW Dive 1 - Getting Started - Studio One 3\r\n---\r\n\r\nI'm a geek, software nerd, and a musician. As I've been stagnating lately in my musical progression I decided to dive into trying to produce some recordings of some of my music, hoping to spice up the old brain.I have a lot to learn. It's pretty humbling, but despite being a musician and a developer with some decent tinkering ability, I've found getting into recording and production a bit challenging on certain things. If you come from a background in which you understand terms such as pre/post, bus, gain structure, compression ration, and other terms then your learning curve will be much easier. For me, my extent of dealing with these terms was focused on guitar pedal settings and an analogue mixing board.\r\nThis has made things much tougher, as I've been learning terminology along with methodology.\r\nI've also been pretty astonished with some of the technology solutions I had never explored that allow a much richer musical creation for a musician.\r\nI figured I'd share my progress as I work through some of these things in case it helps some others on the road. This is not attempting to be a profound treatise, full step by step walkthrough, but more focusing on sharing some of the problems as well as things I've discovered. Maybe you'll save some time, or get some new ideas if this is a new venture for you as well!\r\n\r\nPresonus Studio One 3 Professional\r\n\r\nThis is probably going to be my primary DAW. My initial impressions are extremely positive. The only competitor in my current focus is Ableton since I enjoy live looping, and the session/arrangement integration. However, for pure DAW focus, I'm not sure I'd be as comfortable long-term with Ableton since I'm not primarily an electronic loop/sample based writer.\r\nThe details I'll be posting are on the Professional Edition, so some of the functionality might not be available in the other versions. See their version comparison for exact details. \r\nDisclaimer: Presonus generously provided a license for my evaluation. However, please note that I am not biased in my review, as I believe in providing unfiltered feedback.\r\n\r\n Getting Content Installed\r\n\r\nI ran into some serious download issues when using the Presonus content installer. In reading forum posts, I see that this has occasionally cropped up. If this happens to you, run just go to the presonus account page and download directly with your internet browser, as it ran much faster for me. This was the only major hitch in my install. Maybe a better CDN is in order?\r\n\r\nfirst impressions\r\n\r\nSo far, very intuitive interface. I'm not a DAW expert by any means, so I've been looking for something that had a natural feel to someone familiar with the basics of recording, yet more powerful functionality that will help me create and arrangement more complex pieces. So far, it feels like Presonus Studio One 3 is living up to the reputation I've found online for being getting out of the way and enabling creative workflow. I haven't even dived into the scratchpad functionality, but even my first project has been a very pleasant experience, with none of the \"stumped\" moments I had in getting this to work with Ableton Live (though I still prefer this for live looping right now).\r\n\r\n Novation Launchkey 49 Setup\r\n\r\nSetting up a midi device has been a learning experience. Working with Studio One required a bit of experimentation, as it wasn't preset, but I finally got it to work. It would be nice if they offered more presets, but at least I got it working! Not all the controls seemed to be mapped, but Launchkey has \"InControl\" functionality, so I plan on evaluating this soon.\r\nTo find the midi port a device is working on with Windows, go to device manager, and you can see the midi port displayed so you know what to configure.\r\n\r\n\r\n\r\n\r\n\r\nClean interface\r\n\r\nThe interface is clean, designed to be a single screen view without a lot of navigation required. The context menu options are kept organized and clean. For me, Reaper was a great DAW, had a lot of functionality for the price, but was very context menu driven and this was a lot to get used to. I think Presonus has done an admirable job of organizing the many options to keep things from getting too messy.\r\n\r\n\r\nOne thing I liked was the focus on \"songs\", as it offered the creation of a new \"song\" instead of \"Files/projects\". This felt like such a great design decision as it focused on the creation aspect of new songs.\r\n\r\n\r\n\r\n Presence XT and Virtual Instruments\r\n\r\nA package of instruments are included in Studio One. I enjoyed the Cello and Pizzicato string sections in particular. I'll probably post more on this later as I discuss Melodyne integration.\r\n\r\n\r\n\r\nVST configuration\r\n\r\nNice and easy to find. No problems here. Was happy to see it supports 32, 64, VST3 and more. Ableton Lite didn't support 32bit, so I had a lot of VST's downloaded that I couldn't use.\r\n\r\n\r\n\r\n other stuff for the future\r\n\r\nI plan on discussing the Arrangement functionality as it helps to bridge the gap of organizing complex tracks, as well as scratchpad, which was the primary function I was excited to try, as it seems a win for someone experimenting with various versions.\r\nI've plan on showing how to convert guitar riffs into a cello, quantizing notes for more precision on a fast run, folding/unfolding multiple takes, and more. It's crazy how much is packed in here, but I'm completely loving it.\r\n\r\nany cons?\r\n\r\nSo far, the biggest thing I'm missing is a drum generator such as EzDrummer\r\nI had planned on doing a review of this, but my trial ran out and I haven't heard from them if they do any licenses for press reviews. Addictive drummer seems ok, but the trial is so limited, I'm really not able to leverage, and the single few tracks I was able to try didn't hit what I needed (demo removes all cymbals). EzDrummer 2 was pretty darn good in helping me create some believable tracks. Superior Drummer also seems promising but I couldn't find any trial for it.\r\nIf Presonus had something equivalent to EzDrummer 2 to generate believable drum tracks and rhythms based on building the drums that would be awesome. As it is, it seems to focus on some drum samples and loops, but not a full built engine like EzDrummer 2 offers. If I end up hearing back from Toontracks on their products and obtain copies, I'll be sure to post up more details at that point for the aspiring non drumming arrangers among us.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-09-21-sql-2016-configuration-manager-not-showing-in-start-menu",
        "content": "---\r\ndate: \"2016-09-21T00:00:00Z\"\r\ntags: [\"sql-server\"]\r\ntitle: SQL 2016 - Configuration Manager Not Showing in Start Menu\r\n---\r\n\r\nDidn't see SQL 2016 Configuration manager in the start menu. Ran a quick search to see if this was a common issue and found an article: Quick Trick Where is SQL Server for SQL 2012I looked and found the SQL Configuration Manager for 2016 in the same location: C:\\Windows\\System32\\SQLServerManager13.msc\r\n\r\n\r\n\r\nAs I'm running windows 10, the location for the start menu entries were located here: C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Microsoft SQL Server 2016\r\n\r\nCreate a shortcut for SQLServerManager13.msc in the start menu folder and you'll be good to go!\r\n\r\n\r\n\r\nThanks to @marnixwolf for providing that previous walkthrough that helped me resolve this so quickly.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-09-fixing-non-deterministic-error-when-creating-indexed-view",
        "content": "---\r\ndate: \"2016-10-09T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Fixing non-deterministic error when creating indexed view\r\n---\r\n\r\nI discovered a bit of info on working with float values, while creating a hash value that contained a float value, and a date value.\r\n\r\n    create unique clustered index ixclusteredViewKCatfoodK\r\n    Msg 1901, Level 16, State 1, Line 1517\r\n    Cannot create index or statistics 'ixclusteredViewKCatfoodK' on view 'compareCatfood' because key column 'ViewK' is imprecise, computed and not persisted. Consider removing reference to column in view index or statistics key or changing column to be precise. If column is computed in base table consider marking it PERSISTED there.\r\n\r\nAnd...\r\n\r\n    Msg 2729, Level 16, State 1, Line 38\r\n    Column 'Hash' in view 'compare.Catfood_test' cannot be used in an index or statistics or as a partition key because it is non-deterministic.\r\n\r\nStack Overflow to the rescue... The issue is with float values.\r\nhttp://stackoverflow.com/a/19915032/68698\r\n\r\n Even if an expression is deterministic, if it contains float expressions, the exact result may depend on the processor architecture or version of microcode. To ensure data integrity, such expressions can participate only as non-key columns of indexed views. Deterministic expressions that do not contain float expressions are called precise. Only precise deterministic expressions can participate in key columns and in WHERE or GROUP BY clauses of indexed views. MSDN\r\n Restrictions also apply to formatting dates when you are calculating a checksum. This is because every region has variations on the way the date may be displayed. This makes dates non-deterministic in a hash, unless the convert format is explicitly defined.\r\n\r\nensuring date is converted with style\r\n\r\n    isnull(convert(nvarchar(max), do oe.Somedate ,102), ''') + N'''\r\n    isnull(convert(nvarchar(max),la.SomeDater ,102), N''') + N'''\r\n    isnull(cast  (ft.ToBeOrNotToBe as nvarchar(max)),''') + N'''\r\n    isnull(cast  (t2.Fooey as nvarchar(max)),''') + N'''\r\n\r\nIf you can resolve these issues then you are on your way to resolving the other thousand restrictions on indexed views.... :-)  Good luck!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-09-powershell-ise-updating-theme",
        "content": "---\r\ndate: \"2016-10-09T00:00:00Z\"\r\ntags:\r\ndevelopment\r\npowershell\r\ntech\r\ncool-tools\r\ntitle: 'Powershell ISE: Updating Theme'\r\n---\r\n\r\nFor all the dark theme aficionados, or those who just want a better theme than the default, here's a quick set of directions to update your ISE.1. Go to download a theme from Github  PowerShellISEThemes\r\nUnzip\r\nGo to ISE  Tools  Options  Colors & Fonts  Manage Themes\r\nImport selected theme\r\nFor consistency, adjust the background and forecolor of the console pane as well as the text background to match if you want to. In my case I took the RGB values from the theme on the script pane background and applied to the console pane below it.\r\nIf you want the background for the error, warning, and other output streams to match, update the RGB background as well.\r\n\r\n\r\nYour eyes will now thank you, especially if you are jumping from Visual Studio dark theme to ISE with it's previously glaring white screen.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-09-syncovery-&-arq-syncing-&-backup",
        "content": "---\r\ndate: \"2016-10-09T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nramblings\r\ntitle: Syncovery & Arq - Syncing & Backup\r\n---\r\n\r\nSyncovery & Arq 5I've tried a lot of file sync/backup utilities.\r\n\r\nThe primary definition to get right is that there are two main functions people try to achieve with this type of software.\r\n\r\nFile Syncing: Syncing Files Between Cloud and Local\r\nFile Backup: Preserving Files, sometimes with versioning, in order to protect against loss.\r\nThese two approaches require a diffferent solution many times, as focusing on file syncing means you are more at risk, as handing conflicting sync scenarios might cause loss. However, file backup doesn't give you flexibility to grab files to sync to another machine in many cases (for example CrashPlan does a great job of being quiet and backing things up, however, it is not designed for syncing, rather a single machine archive).\r\n\r\n Syncovery\r\n\r\nDisclaimer: They provided a license for me to evaluate and provide feedback. This doesn't bias me, as I just love finding great software!\r\n\r\nI actually obtained Syncovery  back at the beginning of the year and so have had quite a bit of time to utilize.\r\n\r\nTL;DR\r\n\r\nPros\r\n\r\nIt's a powerhouse of customization, which provides an incredibly customizable set of options\r\nTypically well documented and easy for a power user to figure out the options\r\nIt can solve backups and file syncing inside the same app.\r\nCons\r\nIt's a powerhouse of customization. This isn't something I'd recommend for a non-technical user.\r\nI ran into errors syncing with Amazon Cloud Drive with deletes. Wasn't able to figure that piece out completely, but for the most part everything ran smoothly.\r\nSo, would I recommend? If you are looking to solve some file syncing options between multiple systems, as well as backup files/folders and are willing to deal with tweaking it to get it just like you want, it's awesome. If you want something like plug and play, then you need to look at Arq instead. Arq provides an incredibly simply alternative for those focused on backup, and not on file syncing.\r\n\r\n Profile Overview\r\n\r\nThis provides an overview of all activity. Even though it's not necessarily a styled gui, and has a lot of detail, I think it's well designed for the information it's providing. Having tried some other apps I think I found the majority of what I needed pretty quickly here.\r\nI had a huge backup to Amazon Cloud Drive of my entire lightroom catalog (600-800GB) and Syncovery handled the majority of this backup with no issues. I have run into some issues as you can see.\r\n\r\n\r\n\r\nProfile Settings\r\n\r\nI won't go into every option, read their documentation for the full details.\r\nAt a high level, some of the powerful options I appreciated where the exact mirror vs smart tracking. Smart Tracking looks try and resolve the conflicts that can happen when syncing on several machines by choosing which version wins.\r\n\r\n\r\n\r\n Running backup in attended mode\r\n\r\nShows the current progress. I have used some backup apps that froze when running large backups in the past. So far, I've had good experinces with Syncovery's stability.\r\n\r\n\r\n\r\n\r\n\r\nDetailed Logs\r\n\r\nThe app provides the output via powershell console, or in your native editor. Since I prefer Sublime Text 3, this was perfectly fine with me. Nice detailed logs give me a chance to figure out any issues.\r\n\r\n\r\n\r\n A few other thoughts\r\n\r\nAgain, the options are too massive to cover them all. However, a few stood out to me.\r\n\r\nVersioning deletes: Could have deleted files archived into a relative root folder, or a main archive folder, and then removed after a certain period. This provides a safety net for deletes to be reviewed.\r\nSafety Checks: Deletes or overwrites over a certain percentage of the files will require manual run. This ensures something accidental doesn't cause a mass deletion of files in your cloud storage.\r\nZip versioning. If you want, you can version your backups with zipped contents\r\nRemote agent: If you are using another computer and syncing between them, you can setup the destination to have an agent so a zipped copy could be unzipped locally on the system by the agent, or file scans could be run locally by this agent instead of a remote agent having to do all the work.\r\nRun as a Service: I enabled to run as a service to ensure this always was running in the background\r\nChange Detection: You can setup near real-time file sync based on monitoring a folder. With additional customization, you can tell it to batch up the changes if over a certain number are detected and do them in a batch run instead.\r\n\r\nArq 5\r\n\r\nDisclaimer: Arq also provided me with a license to evaluate.\r\nArq is on the app list for Amazon Cloud Drive supported applications.\r\n\r\n Arq approaches things differently\r\n\r\nThis tool is focused on file backup, so the options are going to be much different in focus. However, the approach reminds me a lot of the \"Apple\" approach with simplifying things.\r\n\r\n\r\n\r\nFiltering Backup Selection\r\n\r\nThis is pretty straight forward. However, I was happy to see the folder filter options actually provide Regex matching as well for power users.\r\n\r\n\r\n\r\n Advanced Options\r\n\r\nAgain, the options are much more limited... and less daunting\r\n\r\n\r\n\r\nArq approaches as encrypted backup\r\n\r\nOne big different to note is that if you are focused on backing up files like photography/video, then you probably want the cloud drive to have those files in their native format, to ensure they are usable to view from the cloud drive. Arq approaches things from a different standpoint. Your cloud drive will have encrypted blocks that this app can download and interpret. For privacy, this is fantastic. For media not so much. You have to decide if you want everything encrypted or \"open\".\r\n\r\n Windows User\r\n\r\nThis probably is just me, but I've had some issues with Arq 5 on Windows 10. They could probably use some better error messages, as this error detail wasn't very user friendly.\r\nWhen I reached out to support in the past, I got an answer in 2 days, so their support has been responsive.\r\n\r\n\r\n\r\nWhat Would I Recommend?\r\n\r\nFor the power user wanting to implement file sync and backup in a single utility, Syncovery all the way.\r\nFor anyone looking to do pure backups, with no configuration or tweaking, and ok with it being completely encrypted, then Arq.\r\nThey both have different focuses. For me, I've migrated to a hybrid approach. For personal code snippets I use Gists, as I can version control them. For media and settings I use Syncovery because I like the customization options. If I was focusing on something like CrashPlan for simplicity and simple configuration, I'd probably go with Arq for that.\r\nOne last one that I hope eventually is supported by Amazon Cloud Drive is Stablebits Cloud Drive. It has a lot of promise, but my tests a while back had it peforming really slow (not due to them, but due to Amazon's throttling) Another similar to that was ExpanDrive. I wasn't able to contact them for a license to evaluate in my review, but my short trial seemed promising, as it tries to add the cloud provide as a drive, allowing you to manage in explorer... or Xyplorer (yes I still use it!)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-10-migrating-database-collation-the-red-gate-way",
        "content": "---\r\ndate: \"2016-10-10T00:00:00Z\"\r\ntags:\r\nredgate\r\nsql-server\r\ncool-tools\r\ntitle: Migrating Database Collation - The Red Gate Way\r\n---\r\n\r\nI had some cross database comparisons that I wanted to simplify, but ensuring the collation matched. The amount of objects that I would have had to drop and recreate was a bit daunting, so I looked for a way to migrate the database to a different collation.Using the Red Gate toolkit, I was able to achieve this pretty quickly. There are other methods with copying data built in to SSMS that could do some of these steps, but the seamless approach was really nice with the SQL Toolbelt.\r\n\r\nFirst I created the database with the collation I wanted to match using SQL Compare 12.\r\nI deployed the original schema to the new location.\r\nRan SQL Data Compare 12 and migrated all the data to the new database.\r\nSince the new database was created with the desired migration, I was good to go!\r\n\r\nNote: I'm a member of Friends of Redgate program, and am provided with licenses for testing and feedback. This doesn't impact my assessments, as I just love finding good tools for development, regardless of who makes them!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-11-daw-dive-02-bfd3-drumming-for-the-rest-of-us",
        "content": "---\r\ndate: \"2016-10-11T00:00:00Z\"\r\ntags:\r\ndaw\r\nmusic\r\nramblings\r\ncool-tools\r\ntitle: DAW Dive 02 - BFD3 - Drumming for the Rest of Us\r\n---\r\n\r\nCreating tracks at home can be very intensive in time, so I'm always looking for a better way to bring a larger sound to a track with less effort. One of the big gaps for me has been the drum parts. I've evaluated a few options, and finally dived into BFD3 for this latest project that is a post-rock style track. This was my first time diving into BFD3 for a full song, so I had a bit to learn.\r\nDisclaimer: BFD3 generously provided me with a copy to evaluate. This doesn't affect my reviews, as I just love dig in and recommend good software!\r\n\r\nBFD3 - Drumming Plugin Extraordinaire\r\n\r\nI was using this in Presonus Studio One. I created a drum track and then started to explore. I started my exploration based on using the Groove Editor, as I wanted to benefit from the library of preset grooves that were already created, and use these with some modifications to be the drum parts for me song, allowing me to hopefully create the track I wanted quickly.  There is a lot of functionality I'm not going to dive into, such as the vast array of microphone adjustments, compression options, and other mixing options. My focus is going to be as a songwriter, how could I use this to help me generate a believable drum track for my song without it being an ordeal.\r\n\r\n Modifying Existing Kit\r\n\r\nOne thing that was really easy to do with BFD3 was modify the kit configuration. I liked the sound of Pork Pie Kick for this song, so I just played a groove, and then switched it over with the checkbox (on hover).\r\n\r\n\r\n\r\nGrooves\r\nThe Grooves section provides a preset set of associated rhythms to allow you to easily prepare a song based on related rhythms. This is ideal for helping you create a song quickly by finding associated rhythms.\r\n\r\n\r\n\r\n\r\n\r\nCopy and Replicate a Groove to Tweak\r\n\r\n\r\n\r\nChange Velocity for Realism\r\n\r\nThis is really important to a natural sounding drum part. You definitely do not want everything at the same velocity as no real drummer would do this. There are some easy ways to adjust the velocity as well as the humanization of the rhythm in both Presonus Studio One 3 and BFD3 plugin. In this example, I adjust it in BFD3 directly.\r\n\r\n\r\n\r\n Humanization of Rhythm\r\n\r\n\r\n\r\nPractice Your Rudiments\r\n\r\nOne other cool feature for quickly pumping out some grooves is the ability to \"paint\" the rudiment you've selected. This means you could easily pick some drag paradiddles on the snare without having to click each point in time. I found this incredible helpful for creating some interesting grooves.\r\n\r\n\r\n\r\n Quick Note on Versions\r\n\r\nI tried the BFD3 Eco, but due to wanting flexibility to edit the grooves and do more tweaks, the Eco version was not a good fit for for me. If, however, you are looking for an experience that is more groove oriented, with less tweaking/adjustments, then you should consider starting with the Lite version. If you are looking for the full flexibility then you might want to evaluate the full one instead.\r\nFXpansion indicates on their support page in the comparison on the differences that:\r\n\r\n What are the differences between BFD Eco and BFD2/BFD3?\r\n BFD Eco is optimized for ease of use and fast results - it is deep enough for deeply sculpting drum sounds into all manner of shapes but is streamlined enough so that you won't be overwhelmed with options.\r\n It has less detailed sounds than BFD2 and BFD3 but is much more light on system resources.\r\n BFD2 and BFD3 contain far more control over each aspect of each kit-piece as well as a configurable mixing engine with custom aux channels and much more. They also feature full editing of Grooves, more control over exports and many other features too numerous to list. FXpansion Support Article\r\n\r\nFinal Thoughts\r\n\r\nThe power and flexibility of this software is pretty amazing. I've only touched on a small fraction of what it is supposed to be capable of, as I'm using a lot of the simple functionality to \"paint\" some rhythms.\r\nI was a little disappointed in the initial groove library as far as the single 6/8 groove test goes, as it felt more difficult to get a groove I liked than when I demoed Ez Drummer\r\n\r\nHowever, the flexibility in editing with the easy humanization and editing of the kit made up for this. I think the vast sound library of plugin packs is what is designed to expand this, so I'll probably cover some of those later to evaluate if they fill in the gap on providing a even larger variety of preset grooves for the wannabe composer. Note that the library is around 40GB for the initial load, so my isolated 6/8 groove test isn't reflective of the rest of the available library. There is a lot to work with, and the expansion libraries seem to be pretty vast in number.\r\n\r\nAnother area that would be awesome to enhance is providing something similar to \"song finder\" that EzDrummer 2 had, where a certain rhythm was tapped, and related matches found. EzDrummer 2 then provides something similar to Grooves where the song structures for verse, chorus, bridge, etc are laid out. I'm not partial to the format that EzDrummer used, I actually prefer the Groove library format of BFD3, but I sure would love seeing the search capability added to help quickly match grooves fitting the feel you are trying for.\r\nI'm pretty happy with the documentation and support, as they have a wide range of well made videos showing demonstrations on the product, how to use, etc. I look forward to incorporating this into my workflow and going through some more training videos to better understand a solid workflow, as I'm just starting to wrap my head around it, along with learning how to navigate Presonus Studio One\r\n\r\nCheck out a trial if you want to give it a shot and post some feedback below on what you think. Remember, I'm not an affiliate, or getting paid for anything on this, I just like reviewing and referring folks to good software.\r\nGood luck!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-16-centralized-management-server-101",
        "content": "---\r\ndate: \"2016-10-16T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Centralized Management Server 101\r\ntoc: true\r\n---\r\n\r\nI've used Central Management Server registered servers in SSMS for primarily one purpose, saving connections. :-)  This is definitely not the intended usage. As I've explored the benefits of using this a little more, I put a few notes together to help share the concepts I came across. I was brand new to this feature in SQL server, and found some of the functionality pretty powerful, especially if you are in an environment that has a lot of servers to manage and ensure consistent configuration among all of them.\r\n\r\nMoving CMS Entries to a new CMS server\r\n\r\nIf you need to move your Central Management Server (CMS) entries to a new CMS, then use the export and import functionality located under the CMS  Tasks context menu.\r\n\r\n\r\n\r\n Run queries across multiple instances at once\r\n\r\nRight click on the CMS group and choose new query. This tab will now execute the same query in parallel across all the selected instances, allowing quick adjustments.\r\nOne creative use of this is to register two databases in the server group, then click new query. They could be on the same server if you wish. Once you start a new query on this group you could run the same query on two separate databases with no extra effort.\r\nAn alternative to using this is Red Gate's SQL Multiscript which offers a bit more customization in the behavior and project file saving for multi-server, multi-database query running.\r\nYou can identify a multiserver query at the bottom identified by\r\n\r\nHighway to the danger zone\r\n\r\nIt's easy to forget you are running a server group query. Use some type of visualization and don't leave the query window open longer than you need to, especially in a production environment. One hint can be setting up the Red Gate tab color if you have SQL Prompt. You can see the connection details on the tab are a little different, listed with the CMS server group name + database, such as the image below\r\n\r\n\r\n\r\n Create a policy\r\n\r\nIn reviewing technet article on Policy-Based Management \r\n\r\nAdministration Functionality\r\n\r\nFrom the CSM context menu you can perform some nice functionality such as start, start, restart of SQL services, view error logs, and even pull up the SQL configuration manager for that instance! Take advantage of this to easily adjust settings across instances without having to log into remote machines.\r\n\r\n Policies\r\n\r\nThere are a few different types of policy behaviors to know about. From MSDN article Administer Servers by Using Policy-Based Management I found that there were a few ways the evaluation of a policy is handled.\r\n\r\nOn Demand\r\nOn Change: Prevent\r\nOn Change: Log Only\r\nOn Schedule\r\nOne interesting comment from MSDN indicating that:\r\n\"IMPORTANT! If the nested triggers server configuration option is disabled, On change: prevent will not work correctly. Policy-Based Management relies on DDL triggers to detect and roll back DDL operations that do not comply with policies that use this evaluation mode. Removing the Policy-Based Management DDL triggers or disabling nest triggers, will cause this evaluation mode to fail or perform unexpectedly.\"\r\n\r\nCreate Policy\r\n\r\nThis suprised me a little. The policy functionality wasn't available in the CMS registered server tab. Instead, go to the server in Object Explorer and expand Management  Policy Management  Policies\r\n\r\n\r\n\r\n creation dialogue\r\n\r\nAdd new condition, there is a large list of policies to evaluate. You can detailed information on them on MSDN here.\r\n\r\n\r\n\r\nConfigure the rules\r\n\r\nYou'll see a huge list of Facets to evaluate and then you can easily setup logic to evaluate this.\r\n\r\n\r\n\r\n Description Details on Policy\r\n\r\nIn this case, I linked back to my favorite resource for server configuration... the Ozar! Providing some detail back on this could be great for quickly providing details later back to someone reviewing the results.\r\n\r\n\r\n\r\nAll your hard work\r\n\r\nFor all this hard work, you'll get two fancy new icons in object explorer. With this work, I'm thinking saving your policies for backup with scripts would be a great idea.... scripting this would be much faster than all these steps to check one setting. I wish the dialogue had been focused on setting up multiple conditions quickly instead of all that work for a single Fact to be evaluated.\r\n\r\n\r\n\r\n Evaluate Policy Results\r\n\r\nStart the evaluation\r\n\r\n\r\n\r\n Results were not what I expected\r\n\r\nMy first run gave me a failure, despite to my eyes being the right match. I had to change my approach from being @FillFactor != 0 to being Policy should be @FillFactor = 0 and it would pass, else it would fall. I was thinking in reverse.\r\nMSDN indicates to be aware that:\r\nIMPORTANT! The functions that you can use to create Policy-Based Management conditions do not always use Transact-SQL syntax. Make sure that you follow the example syntax. For example, when you use the DateAdd or DatePart functions, you must enclose the datepart argument in single quotes.\r\n\r\n\r\n\r\nPrebuilt Best Practice Rules\r\n\r\nThankfully, I found that there were a lot policies already presetup by Microsoft. The default location I had for the 2016 installation was C:\\Program Files (x86)\\Microsoft SQL Server\\130\\Tools\\Policies\\DatabaseEngine\\1033 . You can navigate to these by right clicking on the Server Group you want to evaluate, and then  Evaluate Policies  Choose Source  Files  SQL Server Best Practices folder  Database Engine  1033  Rule to explore\r\n\r\n\r\n\r\n Some final thoughts\r\n\r\nI can see the value for enforcing policies across a vast number of servers, and monitoring for compliance. For my environment, primarily dealing with a lot of developer sandboxes, the effort this requires is a bit too much. For my scenario, I'll probably stick with some home grown queries, powershell SMO checks, and the awesome OmniCompare tool that is getting better and better each iteration. A previous article I wrote discussed the functionality of this tool here: OmniCompare: A Free Tool to Compare SQL Server Instances\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-17-cannot-generate-sspi-context",
        "content": "---\r\ndate: \"2016-10-17T00:00:00Z\"\r\ntags:\r\nsql-server\r\nmysteries\r\ntitle: Cannot Generate SSPI Context\r\n---\r\n\r\nTroubleshooting\r\n\r\nI ran into an error: The target principal name is incorrect.  Cannot generate SSPI context. (Microsoft SQL Server, Error: 0)\r\n\r\nI evaluated the sql server configuration manager protocols for sql server and saw that named pipes was disabled. I tried ensuring that this wasn't causing the issue, but enabling but it didn't fix. Thankfully, Andrew on StackOverflow had the answer here:\r\n\r\n First thing you should do is go into the logs (Management\\SQL Server Logs) and see if SQL Server successfully registered the Service Principal Name (SPN). If you see some sort of error (The SQL Server Network Interface library could not register the Service Principal Name (SPN) for the SQL Server service) then you know where to start.\r\n We saw this happen when we changed the account SQL Server was running under. Resetting it to Local System Account solved the problem. Microsoft also has a guide on manually configuring the SPN.\r\n Andrew 3/19/2014\r\n\r\nWhen I went into the configuration manager I changed the format from the DOMAIN\\USER to searching with advanced and matching the user. The username was applied as USER@DOMAIN.COM instead. When I applied, and restarted the sql service, this still didn't fix.\r\n\r\nI read some help documentation on this on smatskas.com but it didn't resolve my issue as I had the correct permissions, and I verified no duplicate SPN by running the command setspn -x\r\nI ran gupdate /force to ensure was properly in sync with the policies and it did get the time updated. However, the problem persisted. I went back to checking for a specific conflict by running\r\n\r\nStill no luck....\r\n\r\nFinally, I switched the account to use LocalSystem (this was in a dev environment) following the directions by Bob Sullentrup  and this allowed it to successfully register the SPN.\r\n\r\nI'll update my blog post when I have a better understanding on exactly why this occurs, but for now, at least I was able to proceed.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-18-easy-sql-maintenance-with-minionware",
        "content": "---\r\ndate: \"2016-10-18T00:00:00Z\"\r\nexcerpt: Review of using Minionware sql maintenance solution\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\npowershell\r\nsql-server\r\ntitle: Easy SQL Maintenance with Minionware\r\n---\r\n\r\n info \"Updated 2017-01-25\"\r\n While I think the minionware solution is pretty awesome, I think it takes more work for the value, and can be a bit confusing to correctly setup, vs the Ola Hallengren solution, esp since you can install this quickly with dbatools now. I'd lean towards Ola Hallengren for simple implementations, and consider MinionWare's option if you are looking at the their flexibility in the table based configuration. The learning curve seems higher to me, but more for those looking to tweak options a lot. Both are great solutions, just be aware MinionWare will require a little more digging to leverage it fully.\r\n\r\n\r\nHere's my personal tweaked settings for deploying Minionware's fantastic Reindex & Backup jobs. In the development environment, I wanted to have some scheduled jobs running to provide a safety net, as well ensure updated statistics, but there were a few default settings I wanted to adjust. In particular, I tweaked the default fill factor back to 0/100. I also installed all the objects to a new \"minion\" database instead of in master, as I'm beginning to be a fan of isolating these type of maintenance jobs with logging to their own isolated database to easy portability. I also adjusted the default retain days on backups to 30.\r\n\r\n\r\n\r\nYou can use this template as a guide to help you adjust the default backup settings to fit your environment a little better.\r\nThere has been various forms of discussion on the adjustments of Fill Factor for example on the defaults. For more detailed explanation, see Brentozar.com post An Introduction to Fillfactor in SQL Server. For my usage, I wanted to leave the fill factors as default, so the install scripts flips these back to my desired settings. I also run the sp_config command to ensure backup compression is enabled to save some space.\r\n\r\nMaybe this will help you get up to speed if you want to try out this great solution, but tweak a few defaults.\r\nThe ease of installation across multiple instances makes this my current favorite solution, followed by the fantastic Ola Hallengren solution.\r\n\r\n{{% gist 2fee8ab97c0210918e8fb10719fca3f5 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-20-an-internet-with-less-ads-adguard",
        "content": "---\r\ndate: \"2016-10-20T00:00:00Z\"\r\ntags:\r\nramblings\r\ncool-tools\r\ntitle: An Internet With Less Ads - Adguard\r\n---\r\n\r\nAds...I know they employ people, make the world go round, gave us google.... but seriously I hate almost all ads.\r\n\r\nIf my ranking in google search drops to the end because of this post... well.... I'm ok with that.\r\n\r\n Adguard\r\n\r\nI've been using this for over a year as a beta tester (they provided license for me to test and use latest versions). I had a perfect case to demo the craziness of some sites with ads vs using Adguard the other day and figured I'd share it if you... like me... hate the clutter they provide on many sites.\r\nNow, note that other factors come into play here. For example, the site I hite should have been using some optimization for images presented. This site was pretty insane on the ad content, so others aren't as dramatic. I just saw this as a Google Now recommended article and checked it out.\r\n\r\nSite With Ads\r\n\r\nWITH NO ADBLOCKING\r\n679 Requests\r\n11.3 MB for a single webpage load\r\nFinish Time 1.1min\r\n\r\n\r\n\r\n Site With Less Ads\r\n\r\nWITH ADGUARD\r\n116 Requests\r\n2.2 MB for a single webpage load\r\nFinish Time 7.23sec\r\nThis was loaded with the mobile emulator. If you are paying for data (for instance I'm on Project Fi), this could be a huge difference in your browsing bandwidth.\r\n\r\n\r\n\r\nSide by Side\r\n\r\nThis blew my mind. According to Adguard metrics it had saved me over 2GB. Now, even taking this with a grain of salt, I was still pretty impressed by the results. I can immediately tell when I'm not running Adguard on android as the ads are everywhere.\r\nAdguard android has some additional functionality that provides the ability to create a local VPN and filter apps as well, so if you are using some app that has a annoying banner add right near the menu, this will most likely eliminate it.\r\nThis is a pretty big image, and I blurred out text/ads to avoid any issues. Any guess at which one was the one with Adguard running?\r\n\r\n\r\n\r\n Final Thoughts\r\n\r\nThere are options to allow some \"acceptable\" ads. Not interested in this personally, but those who are should know it's offered. Cost can be a little higher than some options due to yearly cost, but the reward of a constantly developing product seems worth it, especially for folks that browse a lot.\r\n\r\nOnly con I've come across with this is custom filtering options are a bit confusing for a non-technical user. Hopefully this will improve in the future to offer a much easier ad editing experience like some other similar toolkits. Lastly, a better notification of potentially blocking issues would be nice. I've come across a few sites that Adguard has blocked on various scripts or other \"needed\" actions that prevent the site from working. Disabling temporarily is acceptable for me, as this is quick with the chrome extension (pause for 30 secs). I'd say a better notification system, if even possible, on potentially site disrupting scripts/cookies blocked would be great enhancement.\r\nOverall, highly recommend this cross platform solution if you are looking for a better way to browse... with less ads!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-22-fixing-untrusted-foreign-key-or-check-constraint",
        "content": "---\r\ndate: \"2016-10-22T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Fixing Untrusted Foreign Key or Check Constraint\r\n---\r\n\r\nUntrusted constraints can be found when you alter/drop foreign key relationships and then add them back without the proper syntax.If you are deploying data through several tables, you might want to disable foreign keys on those tables during the deployment to ensure that all the required relationships have a chance to insert their data before validation.\r\n\r\nOnce you complete the update, you should run a check statement to ensure the Foreign Key is trusted.\r\nThe difference in the check syntax is actually ridiculous....\r\nThis check would not ensure the actual existing rows are validated to ensure compliance with the Foreign Key constraint.\r\n\r\n`sql\r\nalter table [dbo].[ChickenLiver] with check constraint [FK_EggDropSoup]\r\n`\r\n\r\nThis check would check the rows contained in the table for adherence to the foreign key relationship and only succeed if the FK was successfully validated. This flags metadata for the database engine to know the key is trusted.\r\n\r\n`sql\r\nalter table [dbo].[ChickenLiver] with CHECK CHECK constraint [FK_EggDropSoup]\r\n`\r\n\r\nI originally worked through this after running sp_Blitz and working through the helpful documentation explaining Foreign Keys or Check Constraints Not Trusted.\r\n\r\nUntrusted Check Constraints and FKs can actually impact the performance of the query, leading to a less optimal query plan. The query engine won't know necessarily that the uniqueness of a constraint, or a foreign key is guaranteed at this point.\r\n\r\nI forked the script from Brent's link above and modified to iterate through and generate the script for running the check against everything in the database. This could be modified to be server wide if you wish as well. Original DMV query credit to Brent, and the the tweaks for running them against the database automatically are my small contribution.\r\n\r\nNote: I wrote on this a while back, totally missed that I had covered this. For an older perspective on this: Stranger Danger... The need for trust with constraints\r\n\r\n{{% gist 2454ce9134eac225ce264c64adb331a9 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-28-data-compare-on-temporal-tables",
        "content": "---\r\ndate: \"2016-10-28T00:00:00Z\"\r\ntags:\r\ncool-tools\r\nredgate\r\nsql-server\r\ntitle: Data Compare on Temporal Tables\r\n---\r\n\r\nI hadn't seen much talk on doing data comparisons on temporal tables, as they are a new feature. I went through the exercise to compare current to historical to see how Red Gate & Devart handled this. I'm a part of the Friends of Red Gate program, so love checking out their latest updates, and I'm also a regular tester on Devart which also provides fantastic tools. Both handled Temporal Tables with aplomb, so here's a quick walk through on how I did this.\r\n\r\nSSMS 2016 View Of Temporal Table\r\n\r\nWith the latest version of SSMS, you can see the temporal tables labeled and expanded underneath the source table.\r\n\r\n\r\n\r\n Red Gate SQL Data Compare 12\r\n\r\nTo begin the comparison process, you need to do some custom mapping, which requires navigating into the Tables & Views settings in SQL Data Compare\r\n\r\n\r\n\r\nUnmap the existing options\r\n\r\nTo remap the Customers to Customers_Archive, we need to select this in the tables and choose to unmap the Customer and the Customer-Archive Tables from each other. This is 2 unmapping operations.\r\n\r\n\r\n\r\n Setup Compare Key\r\n\r\nGo into the comparison settings on the table now and designate the key as the value to compare against. For the purpose of this example, I'm just doing key, you can change this however you see fit for your comparison scenario.\r\n\r\n\r\n\r\nRemove any columns from comparison desired\r\n\r\nIn this example, I'm removing the datetime2 columns being used, to instead focus on the other columns.\r\n\r\n\r\n\r\n Compare results\r\n\r\nIf you run into no results coming back, look to turn off the setting in compare options for Checksum comparison, which helps improve the initial compare performance. With this on, I had no results coming back, but once I turned off, the comparison results came back correctly.\r\n\r\n\r\n\r\nConflict Row\r\n\r\nThis entry was matched in DbForge SQL Data Compare as a conflict due to matching the key in a non-unique manner. The approach the two tools take is a little different. In RG Data Compare\r\n\r\n\r\n\r\n Conflict Entry Only In Destination\r\n\r\nThe entry identified as potential conflict  by DbForge is identified in the Only In Destination.\r\n\r\n\r\n\r\nDiff Report\r\n\r\nBoth tools report differences. RG's tool has focused on the diff report being simple CSV output. This is fine in the majority of cases, though I'm hoping for additional XLSX and HTML diff reports similar to DbForge eventually. In the case of the CSV output, you could consume the information easily in Power-BI, Excel, or even... SQL Server :-) No screenshot on this as it's just a csv output.\r\n\r\n Devart SQL Data Compare\r\n\r\nGoing into the mapping, you can see support for Customers and Customers_Archive, which is the temporal history table for this.\r\nIn this case, I mapped the current table against the temporal table to compare the current against the change history.\r\n\r\n\r\n\r\nChoose the key column to compare against\r\n\r\nAs a simple example, I just provided the primary key. You could get creative with this though if you wanted to compare specific sets of changes.\r\n\r\n\r\n\r\n\r\n\r\n Handling Conflicts differently\r\n\r\nLooks like the conflict is handled differently in the GUI than Red Gate, as this provides back a separate tab indicating a conflict. Their documentation indicates:\r\nAppears only if there are conflict records (records, having non-unique values of the custom comparison key).\r\nDbForge Data Compare for SQL server Documentation - Data Comparison Document\r\n\r\n\r\n\r\nDiff Report\r\n\r\nThe diff reports provided by DbForge Data Compare are very well designed, and have some fantastic output options for allowing review/audit of the rows.\r\n\r\n\r\n\r\n Diff Report Details\r\n\r\nHere is a sample of a detail provided on the diff report. One feature I found incredibly helpful was the bold highlighting on the columns that had diffs detected. You can trim down the report output to only include the diff columns if you wish to further trim the information in the report.\r\n\r\n\r\n\r\nOverall, good experience with both, and they both support a lot of flexibility with more specialized comparisons.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-10-29-pizzicato-pizza",
        "content": "---\r\ndate: \"2016-10-29T00:00:00Z\"\r\ntags:\r\nmusic\r\nramblings\r\ntitle: Pizzicato Pizza\r\n---\r\n\r\nHad a blast making this song. Unfortunately, creating video of slow motion pizza cutting was going to be a bit awkward to do, as I didn't feel like asking a pizza place to let me stand behind the counter and video pizza assembly. :-)Best listened to with headphones, not a phone speaker :-)\r\niframe width=\"853\" height=\"480\" src=\"ui8XTj23j2E?rel=0\" frameborder=\"0\" allowfullscreen/iframe\r\n This was an experimentation with several new tools for me.\r\n\r\nsong creation\r\n\r\nI first used Presonus Studio One 3 to record some basic track parts like the guitar. I then added some drums using BFD3 (which I've previously reviewed). Using the tools to adjust and randomize the velocity and humanize the rhythm helped create something that sounds more realistic.\r\nThen I recorded a fun guitar riff, and used Melodyne Essentials, which is included in Studio One, to convert the guitar part into MIDI. This allowed me to then change the guitar part I had recorded into a MIDI song part and I choose to replace with Pizzicato strings, as I really liked the sound of Presence XT string section. I've never heard a post-rock style song with pizzicato strings, so figured I might bring a little variety to the post-rock mix.\r\n\r\n video editing\r\n\r\nThe video editing was extremely simplistic. I used Edius 8.2, which is a fantastic NLE for broadcast video editing. I plan on writing or creating a video presentation on that soon to help others interested in video editing to see a little more on that process. I graded with Magic Bullet Looks as I liked the grain and dark feel of the image. Using stabilization effect in Edius, and slowing the speed to 35% (originally 1080p 60fps Panasonic GH3) the resulting imagery was great. I recorded this imagery handheld from this camera on a rainy day in my backyard. I thought the close up of the rain puddling would be interesting for something, and finally found a use.\r\nHope you enjoy my experiment, I'll work later on doing some walkthroughs for anyone interested more in converting audio to MIDI, basic editing with Edius, and some workflow examples with Presonus Studio One.\r\nMusic:\r\n\r\n *   Presonus Studio One 3\r\n *   BFD3 Drums\r\n *   Presence XT Strings\r\n *   Guitar: Epiphone Wildkat\r\n *   Amp: Vox AC15C2\r\n *   Zoom H6 Interface\r\n *   5 string bass\r\n   Video\r\n *   Panasonic GH3\r\n *   Editing with Edius 8.2 \r\n *   Graded with Magic Bullet Looks\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-11-01-get-backup-history-for-all-databases-in-server",
        "content": "---\r\ndate: \"2016-11-01T00:00:00Z\"\r\nexcerpt_separator: !--more--\r\ntags:\r\nsql-server\r\ntitle: Get Backup History for All Databases in Server\r\n---\r\n\r\nHere's a quick snippet to get a listing of the database backups that last occurred on a server. Most solutions provided a single backup listing, but not the brief summary of the last backup details I was looking for.\r\n!--more--\r\n{{% gist bedd7f2d57384dacbe02e8692922236f %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-11-16-parallel-powershell-for-running-sql",
        "content": "---\r\ndate: \"2016-11-16T00:00:00Z\"\r\ntags:\r\npowershell\r\nsql-server\r\ntitle: Parallel Powershell for Running SQL\r\n---\r\n\r\nThis is just a quick look. I plan on diving into this in the future more, as I'm still working through some of the changes being made in the main parallel modules I utilize for SQL server. In the meantime, if you are looking for a quick way to leverage some parallel query running, take a look at PSParallel. I've avoided Powershell Jobs/Workflow due to limitations they have and the performance penalty I've seen is associated with them.For my choice, I've explored PSParallel & PoshRSJob.\r\nI've found them helpful for running some longer running queries, as I can have multiple threads running across server/database of my choice, with no query windows open in SSMS.\r\nAnother great option that is under more active development is PoshRsJob. Be clear that this will have a higher learning curve to deal with as it doesn't handle some of the implicit import of external variables that PSParallel does. You'll have to work through more issues initially to understand correctly passing parameters and how the differents scope of runspaces impact updating shared variables (ie, things get deeper with synchronized hashtables and more :-) )\r\nHope this helps get you started if you want to give parallel query execution a shot. Here's a function using PSParallel to get you started. Let me know if it helps\r\n\r\n{{% gist 5bb1a8adea09276c4fd274b5b2900b6a %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-11-18-attaching-database-using-smo-and-powershell",
        "content": "---\r\ndate: \"2016-11-18T00:00:00Z\"\r\ntags:\r\nautomation\r\npowershell\r\nsql-server\r\ntitle: Attaching Database Using SMO & Powershell\r\n---\r\n\r\nSteve Jones wrote a great article on using this automation here titled The Demo Setup-Attaching Databases with Powershell. I threw together a completed script and modified it for my functionality here. MSDN documentation on the functionality is located here Server.AttachDatabase Method (String, StringCollection, String, AttachOptions)I see some definitive room for improvement with some future work on this to display percentage complete and so on, but did not implement at this time.\r\n\r\nFor the nested error handling I found a great example of handling the error output from: Aggregated Intelligence: Powershell & SMO-Copy and attach database. If you don't utilize the logic to handle nested errors your powershell error messages will be generic. This handling of nested error property is a must to be able to debug any errors you run into.\r\nhttp://blog.aggregatedintelligence.com/2012/02/powershell-smocopy-and-attach-database.html\r\n\r\nIf you want to see some great example on powershell scripting restores with progress complete and more I recommend taking a look at this post which had a very detailed powershell script example. SharePoint Script - Restoring a Content Database\r\n\r\n{{% gist fe14ed313d1259f0aab7b73c7ce39f6f %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-11-22-scan-folder-of-dlls-to-identify-x86-or-x64-compiled-assemblies",
        "content": "---\r\ndate: \"2016-11-22T00:00:00Z\"\r\ntags:\r\npowershell\r\ntitle: Scan folder of dlls to identify x86 or x64 compiled assemblies\r\n---\r\n\r\nPoint this at a directory of dlls and you can get some of the loaded assembly details to quickly identify what type of processor architecture they were compiled for.I did this as I wanted to explore a large directory of dlls and see if I had mixed assemblies of x32 and x64 together from a visual studio build.\r\nSome dlls with invalid assembly header information were found, and this skips those as warnings.\r\n\r\n{{% gist ab1a65ce636231e72214dc1acad30f6d %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-12-03-the-traditional-birthday-song-is-terrible",
        "content": "---\r\ndate: \"2016-12-03T00:00:00Z\"\r\ntags:\r\nmusic\r\nrambling\r\nramblings\r\nmusic\r\ntitle: The Traditional Birthday Song Is Terrible\r\n---\r\n\r\nThe traditional birthday song is terrible.It's never really changed.\r\nIt's like singing a dirge.\r\nIt's really really hard for people to sing anywhere close to on key.\r\nWe all sing it because we have to, but there is this feeling of regret, like \"I'll do it for you, but just because I love you\".\r\nIt is followed by \"Many mourns\" by the closest available family clown.\r\nApparently, the roots were back in the 19th century, and wikipedia says:\r\n\r\n In 1988, Warner/Chappell Music purchased the company owning the copyright for US$25 million, with the value of \"Happy Birthday\" estimated at US$5 million....In February 2016 Warner/Chappell settled for US $14 million, paving the way for the song to become public domain.18] [WIKI\r\n Let's leave this tainted legacy behind. I propose a radical change.\r\n Ditch it for something fun. Make a new family tradition.\r\n\r\n\r\niframe data-preserve-html-node=\"true\" width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VZk5huJzElk?wmode=opaque&enablejsapi=1\" frameborder=\"0\" allowfullscreen=\"yes\"/iframe\r\n\r\n\r\nThis is much more like Rend Collective's reminder to be always practicing the Art of Celebration :-) It's super easy to sing the first time, promotes foot stomping and hand clapping, promotes dancing and jumping, and overall, I feel conveys the feeling a birthday should have.\r\nNew lyrics:\r\n\r\n    The Art of Celebration Birthday Song\r\n    created after much deliberation and refinement in 1 min at the house of Sheldon Hull\r\n    Hey hey hey, It's your birthday day\r\n    Hey hey hey, It's your birthday day\r\n    Hey hey hey, It's your birthday day\r\n    Sing Loud, Sing Proud, It's Your Birthday\r\n    Creative Commons Zero CC0, so do whatever you want with this bit of musical genius\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-12-05-dynamically-set-powershell-variables-from-json",
        "content": "---\r\ndate: \"2016-12-05T00:00:00Z\"\r\nexcerpt_separator: !--more--\r\ntags:\r\npowershell\r\ntech\r\nsql-server\r\ntitle: Dynamically Set Powershell Variables from json\r\n---\r\n\r\nI created this small snippet to allow a list of values from a json file be turned into variables to work with. For working with a fixed list of configuration values, this might be helpful to reduce some coding effort.\r\n!--more--\r\n{{% gist dbbc8356028264047fd742b56c5ee27e %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2016-12-09-manictime-timetracking-automation-done-right",
        "content": "---\r\ndate: \"2016-12-09T00:00:00Z\"\r\ntags:\r\ndevelopment\r\nramblings\r\ncool-tools\r\ntitle: ManicTime - Timetracking Automation Done Right\r\ntoc: true\r\n---\r\n\r\nTracking time is always a beast. With the amount of context switching many developers do, it can be tough to remember how much time went to each project. With companies looking to track effort on sprints, hours on a client project, or (as in my case) just a dev wanting to better evaluate the productive use of time, this app fills some gaps that others don't.For instance, I've tried tools such as Toggl, and similar. I found them useful, but requiring a lot of diligence to work with. There is very little \"automatic\" categorization of time. Many of those tools are focused on a timer based approach that requires you to start and stop the timer.\r\nManicTime approaches this differently. It has the typical stop watch, countdown, Pomodoro type functionality a time tracking tool might offer, but in addition to this it provides a captured timeline of activity with various forms of meta data to easily review and parse for categorization.\r\n\r\nThis categorization of information can take the form of details such as:\r\n\r\nDevelopment\r\nPersonal\r\nBrowsing\r\nor be specific based on user input such as\r\nDevelopment, Reticulating Splines\r\nPersonal, Contemplating Navel\r\nProject Manhattan, Task 666: Optimizing Nuclear Db Performance\r\n\r\nManually entered information is a big driver for better tagging accuracy, but it expands this to allowing dynamic tags based on matching of applications, titles of documents, titles of web pages, calendar entries, and more. This offers a vast range of meta data captured on your system to generate more accurate allocation of your time. The only real negative to this is that it is not as simple as something like Toggle. However, with a little work, you can work an entire day and quickly recap at the end, categorizing your effort with very little fuss, and a high amount of accuracy.\r\n\r\nIf you find yourself forgetting to hit the stop/start button on your time tracker and want to consider a better option for tracking your effort, then look no farther.\r\n\r\nI've used this application over the last year and found a lot of value in it, figured I'd share a little on it, as it's become on of the tools in my essentials pack. Disclaimer: I was provided with a license a while ago on this. This doesn't impact my review, as I just like finding great tools to help devs have a better workflow. I only end up reviewing the ones that I've really found useful\r\n\r\nOverview\r\n\r\n\r\n\r\n Time Categories\r\n\r\n\r\n\r\nOverview of Day's Activities\r\n\r\n\r\n\r\nThe list of all activities is organized into several timelines, whether it be the applications, extracted document titles from applications, calendar, or even calendar feeds. This allows a variety of ways to go back through and easily organize and track time. One recent improvement that I completely love is the integrated screenshots into the application timeline. This allows you to keep a running screenshot log of activity to easily go back through existing applications and remember exactly what was being done at the time. A very useful implementation!\r\n\r\n Tracking Exclusion\r\n\r\nNote that you can choose to go off record, as well as specifically only track certain times of day. This is a good option for those that have a work laptop that they might leave running and only want to report on certain periods of active work time.\r\n\r\nAutotagging\r\n\r\nAutotagging is where this tool gets powerful. Basically, the concept is to allow automatically tagging based on application, window title, url, or other parsed value. This means you can categorize your effort much more easily, with minimal effort.\r\n\r\n Regex Parsing\r\n\r\nI've yet to figure out the dynamic tags based on regex parsing  as it doesn't seem to give you a preview to test and refine results. Once I figure this out, or the app improves the ability to use this I think the additional timelines will be very handy as you could have one timeline focused on dynamic parsing and grouping of projects based on doc/chrome titles that doesn't interfer with the categorization that the other timeline might use.\r\nThis is a usability issue that I hope to see improved in the future. It has a lot of potential.\r\n\r\n\r\n\r\nMultiple Autotag Timelines\r\n\r\nThis is someone I've recently been exploring as it provides the capability to create an automatic tagging of apps, but for different purposes. For instance, you might setup one rule for parsing project numbers and send to a AutoProject timeline that aggregates the totals, but another timeline for categorization of the apps/websites. Another use might be a timeline focused on categorizing web usage, while another focuses on app usage.\r\n\r\n\r\n\r\n Tagging\r\n\r\nAway Time Tagging\r\n\r\nYou can have ManicTime prompt you when you return from your computer, or when a timer has detected minutes of idle on your system. This can help ensure that if you are gone to a meeting, or away from your PC you are still tracking the time you used.\r\n\r\n\r\n\r\n Narrow down untagged time quickly\r\n\r\nThere is a variety of ways to filter down the timeline to only untagged activities as the selected, or untagged as what's actually shown. This can help identify gaps in what you've reviewed.\r\n\r\n\r\n\r\nStatistics & Reports\r\n\r\n Generate Timesheet Report\r\n\r\n\r\n\r\nSome Nice Visual Statistics Available\r\n\r\n\r\n\r\n Other Statistics Available\r\n\r\nThese are listed based on the selected groups, tags and more.\r\n\r\n\r\n\r\nManic Time Server\r\n\r\nManic time offers server functionality to allow this tool to be used to help generate reports for staff members and usage.\r\nThis functionality is not for the non-technical user. I found it a little challenging to get things setup, so this current iteration wasn't designed as a simple \"central\" solution for all devices. With a better setup/configuration experience (no domain user logins etc) and perhaps more of a Google Drive/Dropbox type sync, I think the solution would be fantastic for tracking time on various devices.\r\nDue to the setup issues I had on server, I wasn't able to include tracking from the new ManicTime android client.\r\nI would say that homegrowing your own tracking solution with Tasker and a custom timeline here might not be a difficult project to consume through the app due to the documented format for consuming external timeline information. I haven't gone to that effort, but it's an intriguing concept.\r\n\r\n daily retrospective\r\n\r\nBeing able to retroactively tag and categorize effort at the end of the day, without having to constantly toggle the stopwatch. You can approach with a stop watch/pomodoro/countdown built in, but if you get pulled in multiple tangents, this tool makes it easy to go back and categorize throughout the day... IF your primary work is driven on using your computer. Since I'm approaching this from a developer tool point of view, it's a perfect fit!\r\n\r\nLast Thoughts\r\n\r\nFantastic app with a unique approach.\r\nCost is a little high, but it's an independent app so supporting the development can be a good thing as a really specialized tool. Not sure they'd be able to continue development if it was a lifetime purchase (those seem to have gone away over time). As a good office tool for better tracking and reporting on time (for instance if working with clients), then I believe it might just pay for itself.\r\nI'd like to see a smoother integration with the server components to being a better cloud tracking mechanism, allowing android, pc, mac, all to provide a solid reporting mechanism for families on the go. The app seems more focused on enterprise/business tracking though, so this might not be implemented.\r\nI'll continue using and finding great value in helping track my time with the least amount of work. For those looking for a solution, give it a shot. They have a lite version available as well with less features, so you can take a swing at it.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-01-23-bad-idea-jeans-query-optimization-through-minification",
        "content": "---\r\ndate: \"2017-01-23T00:00:00Z\"\r\ntags:\r\nbad-idea\r\nsql-server\r\ntech\r\ntitle: 'Bad Idea Jeans: Query Optimization Through Minification'\r\n---\r\n\r\nSQL is pretty verbose compared to some languages. It's a pretty big disappointment that I have to type out select customer from dbo.customers where id = 2 instead of a much simpler syntax like dbo.Customers ' Customer like Powershell might offer. As I considered the disappointing verbosity of sql server, I considered that perhaps one way to reduce network traffic, save electricity, and aid the garrulous language known as sql might be to require all code running to be minified.Think about the potential savings in bandwidth and having to scroll. Anyone who complains about this should just realize there is a thing called word-wrap which will solve all the readability problems. No more need for Red Gate Sql Prompt's beautiful yet wasteful formatting options. (sorry RG). In fact, no more debates on readability of formatting standards at all!\r\n\r\n\r\n\r\nIn a file size comparison on this small small query I found a full 1KB size savings.\r\n\r\n|         File         | Size |\r\n| -------------------- | ---- |\r\n| WITH-LINE-BREAKS.sql | 9 KB |\r\n| NO-LINE-BREAKS.sql   | 8 KB |\r\n\r\nIf you extrapolate this over a larger query I found a 20% reduction.\r\n\r\n|           File            |  Size  |\r\n| ------------------------- | ------ |\r\n| LONG-NO-LINE-BREAKS.sql   | 129 KB |\r\n| LONG-WITH-LINE-BREAKS.sql | 160 KB |\r\n\r\nWith a heavy traffic OLTP system, this might reduce traffic IO tremendously and reduce server energy costs. I did consider trying to calculate the wattage savings this might entail, but I plead laziness. I also considered running wireshark to analyze packets to compare the size to validate what magic the compression and other parser functions perform on the sent query, but decided I had better uses for my time. .... Maybe the next brilliant idea would be to name all your tables consecutive numbers like dbo.1, dbo.2, dbo.3 and so on. Since these names are stored in a nvarchar format in system tables, it might optimize the storage and performance of the system tables.... So many good ideas, so little time......\r\n\r\nthis post was inspired by the wonderful contributions to bad ideas that Brent Ozar and his team of eccentric consultants donated to the community to cause mayhem for those stupid enough to try them out. Thanks Brent :-)\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-18-implicit-transactions",
        "content": "---\r\ndate: \"2017-02-18T00:00:00Z\"\r\npublished: false\r\ntags:\r\nsql-server\r\ntitle: Implicit Transactions\r\n---\r\n\r\nNever messed around with this setting in the server configuration, so I was unfamilar with the impact it would have.If I ran a statement with something like the following:\r\n\r\n    insert into foo\r\n    select bar\r\n    insert into foo\r\n    select bar\r\n\r\nI know that if the first had an aborting error, such as text was too long, the second statement would not complete as the batch would have failed.\r\nIf you instead did\r\n\r\n    insert into foo\r\n    select bar\r\n    GO\r\n    insert into foo\r\n    select bar\r\n\r\nand had the same error, the second would be completed, since the first would throw an error, but the GO separates the second statement explicitly into another batch, and therefore another transaction.\r\nInterestingly, the Implicit Transactions option changes the behavior to making each statement act as if it was encapsulated by begin transaction --- commit transaction instead of requiring this to be defined.\r\nSo if you set implicit transactions on and ran the statement below with no go statement:\r\n\r\n    insert into foo\r\n    select bar\r\n    insert into foo\r\n    select bar\r\n\r\nIt is really operating as if:\r\n\r\n    begin transaction\r\n    insert into foo\r\n    select bar\r\n    commit transaction\r\n    GO\r\n    begin transaction\r\n    insert into foo\r\n    select bar\r\n    commit transaction\r\n\r\nMSDN - Implicit Conversions is a resource that further documents the behavior, indicating that a rollback for the particular transaction is handled automatically. This means that since that each statement is treated as a transaction that it will not abort the second statement and terminate execution if the first experinces the error, since by \"implicit conversions\" this would be handled separately.\r\nMSDN article with example code to walk through it\r\nhttps://technet.microsoft.com/en-us/library/ms190230(v=sql.105).aspx.aspx)\r\n\r\n    /***************\r\n    STEP 1: SETUP\r\n    ***************/\r\n    use tempdb;\r\n    set nocount on;\r\n    set xact_abort off;\r\n    set implicit_transactions off;\r\n    if object_id('dbo.TestImplicitTrans','U') is not null\r\n    begin\r\n    print 'Dropped dbo.TestImplicitTrans per existed';\r\n    drop table dbo.TestImplicitTrans;\r\n    end;\r\n    print 'create table dbo.TestImplicitTrans';\r\n    create table dbo.TestImplicitTrans\r\n    (\r\n    test_k int primary key\r\n    identity(1,1)\r\n    not null\r\n    ,random_text varchar(5) not null\r\n    );\r\n    go\r\n    /***************\r\n    TEST 1:\r\n    xact_abort off\r\n    set implicit_transactions off\r\n    Results in:\r\n    first transaction fails\r\n    second transaction succeeds (this is due to xact_abort off not being activated)\r\n    testk  randomtext\r\n    2           12345\r\n    ***************/\r\n    use tempdb;\r\n    go\r\n    set nocount on;\r\n    set xact_abort off;\r\n    set implicit_transactions off;\r\n    truncate table dbo.TestImplicitTrans;\r\n    print 'Statement 1 START';\r\n    insert into dbo.TestImplicitTrans (random_text) values ('00001x');\r\n    print 'Current trancount: ' + cast(@@trancount as varchar(100));\r\n    insert into dbo.TestImplicitTrans (random_text) values ('00002');\r\n    print 'Current trancount: ' + cast(@@trancount as varchar(100));\r\n    insert into dbo.TestImplicitTrans (random_text) values ('00003');\r\n    print 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\r\n    print 'Statement 1 END';\r\n    print char(13) + char(13) + 'Statement 2 START';\r\n    insert  into dbo.TestImplicitTrans\r\n    (random_text)\r\n    values\r\n    ('12345'  -- random_text - varchar(5)\r\n    );\r\n    print 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\r\n    print 'Statement 2 END';\r\n    select\r\n    from\r\n    dbo.TestImplicitTrans as TIT;\r\n    go\r\n    /***************\r\n    TEST 2:\r\n    xact_abort on\r\n    set implicit_transactions off\r\n    Results in:\r\n    first transaction fails\r\n    second transaction doesn't execute due to xact abort being set on\r\n    testk  randomtext\r\n    NONE\r\n    ***************/\r\n    use tempdb;\r\n    go\r\n    set nocount on;\r\n    set xact_abort on;\r\n    set implicit_transactions off;\r\n    truncate table dbo.TestImplicitTrans;\r\n    print 'Statement 1 START';\r\n    insert  into dbo.TestImplicitTrans\r\n    (random_text)\r\n    values\r\n    ('12345x'  -- random_text - varchar(5)  ONE CHARACTER TOO LARGE\r\n    );\r\n    print 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\r\n    print 'Statement 1 END';\r\n    print char(13) + char(13) + 'Statement 2 START';\r\n    insert  into dbo.TestImplicitTrans\r\n    (random_text)\r\n    values\r\n    ('12345'  -- random_text - varchar(5)\r\n    );\r\n    print 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\r\n    print 'Statement 2 END';\r\n    select\r\n    from\r\n    dbo.TestImplicitTrans as TIT;\r\n    go\r\n    /***************\r\n    TEST 2:\r\n    xact_abort off\r\n    set implicit_transactions off\r\n    Results in:\r\n    first transaction fails\r\n    second transaction doesn't execute due to xact abort being set on\r\n    testk  randomtext\r\n    NONE\r\n    ***************/\r\n    use tempdb;\r\n    go\r\n    set nocount on;\r\n    set xact_abort on;\r\n    set implicit_transactions on;\r\n    truncate table dbo.TestImplicitTrans;\r\n    print 'Statement 1 START';\r\n    insert  into dbo.TestImplicitTrans\r\n    (random_text)\r\n    values\r\n    ('12345x'  -- random_text - varchar(5)  ONE CHARACTER TOO LARGE\r\n    );\r\n    print 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\r\n    print 'Statement 1 END';\r\n    print char(13) + char(13) + 'Statement 2 START';\r\n    insert  into dbo.TestImplicitTrans\r\n    (random_text)\r\n    values\r\n    ('12345'  -- random_text - varchar(5)\r\n    );\r\n    print 'Successfully inserted: ' + cast(@@rowcount as varchar(10));\r\n    print 'Statement 2 END';\r\n    select\r\n    from\r\n    dbo.TestImplicitTrans as TIT;\r\n    go\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-18-quick-way-to-run-powershell-tasks-in-parallel",
        "content": "---\r\ndate: \"2017-02-18T00:00:00Z\"\r\ntags:\r\npowershell\r\nsql-server\r\ntitle: Quick Way to Run Powershell Tasks in Parallel\r\n---\r\n\r\nRunning tasks in parallel can be a bit difficult in powershell. However, there are a few projects out there that optimize the performance and provide a better experience of running tasks in parallel with less effort.#cool uses\r\nA few cool uses of this might be running parallel sql queries across multiple servers or databases while maintaining a throttled limit to avoid saturation of the target environment. Additionally, long running queries might benefit in running in parallel if running on multiple objects in the same database or in different databases.\r\n\r\nmodule magic\r\n\r\nI've utilized two main modules to advance this.\r\nPSParallel and PoshRSJobs. Both are fantastic options. The Invoke-Parallel is not steadily maintained, so I try to use PoshRSJob when possible. However, for ease of use the Invoke-Parallel option is pretty awesome as it automatically imports variables, functions, and modules into the block to allow for less work in defining parameters, having to use the $using:variablename clause, etc.\r\n\r\n lots of gotchas\r\n\r\nHowever, be prepared to deal with some complications in doing this with powershell. For instance, write-host, write-verbose, write-error, at this time can throw errors in PoshRSJob or not provide any output, as these streams are not incorporated the same as your local ISE session. In fact, at the time of this post, for output to stream from the PoshRSJob module, I had to change my output from:\r\n\r\n`powershell\r\nwrite-host 'I know a kitten dies every time writehost is used, but I just cannot stop myself'\r\n`\r\n\r\nto\r\n\r\n`powershell\r\n\"I know a kitten dies every time writehost is used, but I just cannot stop myself\"\r\n`\r\n\r\nYes... no write-host/write-error/write-verbose is used here, just quotes for it. The developer and github community is looking to improve this, but at this time, don't expect logging or error messages to come through the same way.\r\n\r\nBe prepared to deal with some complications on error handling when dealing with runspaces, as even though they are more performant, there is a lot of issues with scope to deal with in those isolated runspaces. Once you start increasing the size of the script blocks things can get hard to debug.\r\n\r\nI think the simpler the task to pass into the parallel tasks, the better.\r\n\r\nHowever, for some basic tasks that would benefit in parallel, you can definitely give it a shot.\r\nThis task focused on iterating through a directory recursively and cleaning up each of the files by stripping out comments and blank lines. The following results were a simple example and interesting to compare.\r\n\r\n`text\r\n    -------- Summary with PoshRSJobs--------\r\n    File Size:   9.59 MB\r\n    Total Count: 4,600.00\r\n    Filepath: C:\\temp\\MyCleanedUpZip.zip\r\n    Total Original Lines: 1221673\r\n    Total Lines: 1,201,746.00\r\n    Total Lines Saved:  21,959.00\r\n    TOTAL TIME TO RUN: 08:43\r\n\r\n    -------- Summary with Invoke-Parallel --------\r\n    File Size:   6.69 MB\r\n    Total Count: 4,447.00\r\n    Filepath: C:\\temp\\MyCleanedUpZip.zip\r\n    Total Original Lines: 1221436\r\n    Total Lines: 854,375.00\r\n    Total Lines Saved:  360,045.00\r\n    TOTAL TIME TO RUN: 05:22\r\n`\r\n\r\nPoshRSJobs seemed to approach creating the job list first, which took a long time, and then processed the output very quickly. Overall, this took longer for this type of task. Invoke-Parallel gave an almost instant response showing the progress bar with estimated time remaining, so for this type of job it actually ran faster.\r\n\r\n`text\r\n    -------- Summary - Native ForEach --------\r\n    File Size:   6.69 MB\r\n    Total Count: 4,621.00\r\n    Filepath: C:\\temp\\MyCleanedUpZip.zip\r\n    Total Original Lines: 1227408\r\n    Total Lines: 861,600.00\r\n    Total Lines Saved:  365,808.00\r\n    TOTAL TIME TO RUN: 04:52\r\n`\r\n\r\nSurprising to me, the native foreach which was single threaded was faster. I believe in this case, the overhead of setting up the jobs was not worth parallel task processing. Since the task was a lot of small tasks, this probably wasn't a good candidate for parallel tasks. Based on this small test case, I'd venture to look into parallel tasks when longer run times are involved, such as perhaps copying large files that aren't oversaturating your IO. In this case, slow long copies would probably benefit from parallel tasks, while small text file copies as I showed wouldn't.\r\nA simple example of the difference in syntax for using PSParallel would be just counting lines in files in a directory.\r\n\r\n\r\n`powershell\r\n$Folder = 'C:\\Temp'\r\n$startTime = get-date\r\n[int]$TotalManualCount = 0\r\nGet-ChildItem -Path $Folder -Recurse -Force ' where { ! $.PSIsContainer } ' % { $TotalManualCount += (Get-Content -Path ($.FullName) -Force ' Measure-Object -Line).Lines}\r\nwrite-host ('Total Lines: {0:N2}' -f $TotalManualCount)\r\nWrite-host ('FOREACH: Total time to process: {0}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('mm\\:ss'))\r\nUsing Invoke-Parallel\r\n$ManualCount = [hashtable]::Synchronized(@{})\r\n$ManualCount = @{\r\nTotalCount     = 0\r\n}\r\n$Folder = 'C:\\Temp'\r\n$startTime = get-date\r\nGet-ChildItem -Path $Folder -Recurse -Force ' where { ! $_.PSIsContainer } ' Start-RsJob -Throttle 4 -ArgumentList $ManualCount -ScriptBlock {\r\n[cmdletbinding()]\r\nparam($ManualCount)\r\n$ManualCount.TotalCount += (Get-Content -Path ($_.FullName) -Force ' Measure-Object -Line).Lines\r\n}\r\nwrite-host ('Total Lines: {0:N2}' -f $ManualCount.TotalCount)\r\nWrite-host ('INVOKE-PARALLEL: Total time to process: {0}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('mm\\:ss'))\r\n`\r\n\r\nNote that this simple code example might have had some issues with counts due to locking with the synchronized hash table usage. Based on a few searches, it looks like you need to implement a lock on the hash table which ensures that particular thread is able to safely update. I didn't find clear proof that the synchronized hash table was working or failing, but it's something to be aware of. There are some active efforts on improving in PoshRSJob github issues.\r\nHopefully you'll have a few new ideas on working with Parallel tasks in powershell now, and think about leveraging it for some tedious tasks that might benefit with SQL server or other administrative jobs.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-18-redgate-sql-data-compare-&-devart-db-forge-data-compare",
        "content": "---\r\ndate: \"2017-02-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Redgate SQL Data Compare & Devart DBForge Data Compare\r\n---\r\n\r\nI'm a big fan of Redgate, as I'm in the Friend of Redgate program. However, I do also utilize some other toolkits. One competitor that I find has some , but I do dabble with some other toolkits (I know heresy :-) . One of the competitors that I find has some brilliant features, but many time lacks the refinement and ease of use of Redgate is Devart tools. The tools they offer are often really nice, and continually updated based on feedback. As a general rule, I'd say the Devart tools feel less \"refined\" in some areas, but then offer some really nice usability features that RG hasn't yet implemented. Both have their place in my toolbelt depending on the need.Having just completed some very large data comparisons on views, generating over 15GB of network traffic in last few days, I've been really impressed with the usability and output from Devart DbForge Data Compare. The performance seems great.\r\n\r\nI've evaluated their schema compare before and found it fantastic for the price if I was strapped on a budget, but when able to pay for an overall more flexible and refined product I'd definitely choose SQL Compare. The differences are much smaller on the data compare tool though due to the much less complex nature of what it's performing. I ran across a few features in that I thought would be great to mention for the team working on Data Compare to provide some enhanced functionality.\r\n\r\nDiff Report: They provide a fantastic option of outputting a diff report not only in CSV but in XLS format. The formatted report is much more usable than the CSV I get from RG Data compare because they format, and apply bold to the _S and _T cells that actually have a difference, enabling a much easier review process to find the diffs. This is far more usable for an end audience that might want to view differences in data detected on a table. I've had the case to provide this report to analysts to look at differences. The typical use case of DBA's syncing data from one database to another probably would just use the tool and never need this. My particular use case has found a better report output would have been a major benefit.\r\nCached schema object definitions/mapping. They load up previous mappings so you can go and tweak without the need to refresh immediately. This would be nice when you are fine tuning the comparison results and keep needing to tweak to the figures.\r\nOther suggestions based on my recent work w/large table comparison.\r\nSince table size has a direct impact on the compare due to local caching of the data, consider providing a column that shows estimated & total space required for the comparison. This way if I compared a lot of small tables I'd see the rowcount/size (sp_spaceused) and then added a large table (3GB for example), I'd see the approx local storage and network transfer impact with total size of \"7GB total storage/transfer required\".\r\n\r\nIf I setup a comparison on a view with custom key (due to no index on the view), and I drop and recreate the view for a new definition, the comparison options are not persisted (for example the custom key). I'm not sure if this is due to the underlying changes on the object_id and lack of clustered index for explicit mapping, but persisting this would be really nice when the columns used for key comparison still exist.\r\nOverall, as a friend of Redgate I'm\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-18-ssms-2016-object-explorer-read-uncommitted",
        "content": "---\r\ndate: \"2017-02-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: SSMS 2016 - Object Explorer Read Uncommitted\r\n---\r\n\r\nI ran through some directions from others, including the very helpful post from SqlVariant, but I had issues locating the correct keys. For my Windows 10 machine, running SSMS 2016, I found the registry keys related to the object explorer located in a different path.\r\n\r\nI found matches for read committed/uncommitted string at: HKCU\\SOFTWARE\\Microsoft\\VisualStudio\\14.0\\SSDT\\SQLEditorUserSettings\r\n\r\nRunning the following powershell command:\r\nget-itemproperty -path 'Registry::HKCU\\SOFTWARE\\Microsoft\\VisualStudio\\14.0\\SSDT\\SQLEditorUserSettings' ' select SetTransactionIsolationLevel ' format-list\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-18-track-creation-of-databases",
        "content": "---\r\ndate: \"2017-02-18T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntitle: Track Creation of Databases\r\n---\r\n\r\nSys.Databases has some create information, but I was looking for a way to track aging, last access, and if databases got dropped. In a development environment, I was hoping this might help me gauge which development databases were actually being used or not.\r\n\r\nscript data-preserve-html-node=\"true\" id=\"codeblock\" src=\"830a4514f6c9a2fd938c6eeb67db6000.js\"/script\r\n\r\n`sql\r\n\r\n/***************\r\n    run check on each constraint to evaluate if errors\r\n***************/\r\nif object_id('tempdb..##CheckMe') is not null\r\n    drop table ##CheckMe;\r\n\r\nselect\r\n    temp_k =    identity(int, 1, 1)\r\n    ,X.*\r\ninto ##CheckMe\r\nfrom\r\n    (select\r\n\r\n            typeofcheck =                                            'FK'\r\n            ,'[' + s.name + '].[' + o.name + '].[' + i.name + ']'    as keyname\r\n            ,CheckMe =                                                'alter table ' + quotename(s.name) + '.' + quotename(o.name) + ' with check check constraint ' + quotename(i.name)\r\n            ,IsError =                                                convert(bit, null)\r\n            ,ErrorMessage =                                            convert(varchar(max), null)\r\n        from\r\n            sys.foreign_keys i\r\n            inner join sys.objects o\r\n                on i.parentobjectid = o.object_id\r\n            inner join sys.schemas s\r\n                on o.schemaid = s.schemaid\r\n        where\r\n            i.isnottrusted = 1\r\n            and i.isnotfor_replication = 0\r\n        union all\r\n        select\r\n            typeofcheck =                                            'CHECK'\r\n            ,'[' + s.name + '].[' + o.name + '].[' + i.name + ']'    as keyname\r\n            ,CheckMe =                                                'alter table ' + quotename(s.name) + '.' + quotename(o.name) + ' with check check constraint ' + quotename(i.name)\r\n            ,IsError =                                                convert(bit, null)\r\n            ,ErrorMessage =                                            convert(varchar(max), null)\r\n        from\r\n            sys.check_constraints i\r\n            inner join sys.objects o\r\n                on i.parentobjectid = o.object_id\r\n            inner join sys.schemas s\r\n                on o.schemaid = s.schemaid\r\n        where\r\n            i.isnottrusted = 1\r\n            and i.isnotfor_replication = 0\r\n            and i.is_disabled = 0) as X\r\n\r\n`",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-24-life-hack-when-you-need-a-mouse-pad",
        "content": "---\r\ndate: \"2017-02-24T00:00:00Z\"\r\ntags:\r\nlifehack\r\ntech\r\nramblings\r\ntitle: 'Life Hack: When you need a mouse pad'\r\n---\r\n\r\nMy Logitech Master mouse went on the fritz... Requiring me to use my trusty Microsoft mouse. This is an optical based mouse based mouse that started driving me insane with the lack of precision. This exhibited random skips and overall lack of agreeability. I ran across some posts mentioning optical mice have issues with certain surfaces making it hard to detect precise movements and suggested using a surface with a pattern or irregularity, even a piece of paper being scribbled on. I was using a white desk, very little pattern to work with. The MX Master worked perfectly, but it had spoiled me.\r\n\r\nAfter searching the office and failing to find a usable mousepad (seriously!)... I googled diy mousepad.\r\n\r\nLo and beheld, a fantastic website provided the World's original cheapest Mousepad. Well worth visiting despite its self disparaging remarks. Thank you for your contribution to the Internet's vast amalgamation of priceless documents.\r\n\r\nAfter feeling like an idiot, I printed, taped to my desk and the magical precision returned. Life hack!\r\n\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-02-27-red-gate-sql-clone-1-initial-setup",
        "content": "---\r\ndate: \"2017-02-27T00:00:00Z\"\r\ntags:\r\nredgate\r\nsql-server\r\ntitle: Red Gate SQL Clone (1) - Initial Setup\r\nslug: \"red-gate-sql-clone-(-1)-initial-setup\"\r\n---\r\n\r\nNote this was during earlier beta usage, so some of the UI and other features will have been updated more. I plan on writing more on this promising tool as  I get a chance to dive into it more, especially the powershell cmdlets for database cloning automation. In the meantime, I believe the permissions issue is still relevant, so I'll post this as a reminder in case someone is working through the initial setup.\r\n\r\nIt seems like a real promising toolkit for testing and reducing storage requirements for testing database automated deployment pipelines.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nError starting service\r\n\r\nThe Redgate SQL Clone service on Local Computer started and then stopped. Some services stop automatically if they are not in use by other services or programs.\r\n\r\nI wasn't using for a while due to error message I couldn't figure out. I then read through the help documentation again and found that the permissions required for the service account should be a local admin. Once I added the service account to local admins, it correctly allowed the service to start.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-03-07-tfs-custom-task-service-actions-(for-tfs-2015-update-2.1-or-before)",
        "content": "---\r\ndate: \"2017-03-07T00:00:00Z\"\r\ntags:\r\ntfs\r\nbuild-tasks\r\npowershell\r\ntech\r\ntitle: TFS Custom Task - Service Actions (for TFS 2015 Update 2.1 or before)\r\n---\r\n\r\nApparently, boolean values for custom VSTS tasks for versions prior to TFS 2015 Update 3) require some special handling as they don't pass the checkbox values as actual powershell $true or $false. Instead the task passes this information along as true or false. To properly handle this you'll need to pass in the value as a string then convert to boolean.\r\n\r\nI found a great start on working on this solution in a blog post by Rene which has more detail, so check it out. In addition, some reading on promiscuous types with powershell can be helpful to understand why special handling is needed with conversion.\r\nFor example, in the task.json file you'll have:\r\n\r\n``json\r\n    \"inputs\": [\r\n            {\r\n                \"defaultValue\": \"MyServiceName*\",\r\n                \"label\": \"ServiceName\",\r\n                \"name\": \"ServiceName\",\r\n                \"required\": true,\r\n                \"type\": \"string\"\r\n            },\r\n            {\r\n                \"defaultValue\": \"true\",\r\n                \"helpMarkDown\": \"issue restart command\",\r\n                \"label\": \"ChangeCredentials\",\r\n                \"name\": \"ChangeCredentials\",\r\n                \"required\": true,\r\n                \"type\": \"boolean\"\r\n            }\r\n`\r\n\r\nThis boolean value provides a checkbox on the custom task window.\r\n\r\n\r\n\r\nTo properly work with the boolean value, you have to bring it in as a script then convert it to a boolean value.\r\n\r\n`powershell\r\n\r\n    param(\r\n             [string]$ServiceName\r\n            ,[string]$ServiceAccount\r\n            ,[string]$RestartService\r\n            ,[string]$StartService\r\n            ,[string]$StopService\r\n            ,[string]$ChangeCredentials\r\n    )\r\n\r\n`\r\n\r\nonce you have the parameters, use .NET convert functionality to\r\n\r\n`powershell\r\n\r\n[bool]$_RestartService    = [System.Convert]::ToBoolean($RestartService)\r\n[bool]$_StartService      = [System.Convert]::ToBoolean($StartService)\r\n[bool]$_StopService       = [System.Convert]::ToBoolean($StopService)\r\n[bool]$_ChangeCredentials = [System.Convert]::ToBoolean($ChangeCredentials)\r\n\r\n`\r\n\r\nBelow I've included a custom TFS Task for basic start/stop/restart/change credentials with a custom tfs task. It's not super refined, but it's a good start to get you on your way.\r\n\r\n{{% gist 622ee7b3da8423b689c9a266816103aa %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-05-08-programming-fonts-for-the-newb",
        "content": "---\r\ndate: \"2017-05-08T00:00:00Z\"\r\ngallery:\r\nalt: fira-code-mono\r\n  image_path: /images/fira-code-mono.png\r\n  title: Image of fira-code-mono\r\n  url: /images/fira-code-mono.png\r\nalt: source-code-pro\r\n  image_path: /images/source-code-pro.png\r\n  title: Image of source-code-pro\r\n  url: /images/source-code-pro.png\r\nalt: bitstream-vera-sans-mono\r\n  image_path: /images/bitstream-vera-sans-mono.png\r\n  title: Image of bitstream-vera-sans-mono\r\n  url: /images/bitstream-vera-sans-mono.png\r\nlastmodifiedat: \"2018-03-30\"\r\ntags:\r\ntech\r\ntitle: Programming Fonts For The Newb\r\n---\r\n\r\nthe camps\r\n\r\nOnce you get into coding fonts, you'll find that there are two primary camps.\r\n\r\nDon't give a crap about it. \"I'll use defaults for everything and probably wouldn't care if I was coding in Arial\". If this is you, then this post is definitely not for you. Please continue to enjoy Comic Sans with my pity. :-)\r\nFont aficionados \"Your world will change forever once you use this specific font! It will increase your productivity 300%\"\r\n\r\nInside the font afficiando realm, you have various subcultures.\r\n\r\nFixed Font Only\r\nElastic Tabstops are the future, why can't anyone get with the program? (Elastic tabtop fonts allow proportional fonts with better alignment )\r\nLigature Fonts changed my world\r\n\r\n cool resource\r\n\r\nOne really cool resource for exploring these various types of fonts is Programming Fonts - Test Drive. This is a pretty cool resource to preview various fonts and find links and resources for them.\r\n\r\nmonospaced\r\n\r\nMonospaced fonts ensure that every character take up the same amount of space regardless. This means a period takes up the same space as any other letter of the alphabet.\r\n\r\nThe goal in recommending this for code editing has to do with the purpose of what's being written and read. In reading your eyes flow over words, and punctuation, while important, supports the words. It doesn't need to take up the same space. In code, every punctuation character is just as important as every single letter written. If you have a bunch of nested formulas for example, reading\r\n\r\n`powershell\r\n('....Total time to process: {0:g}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('hh\\:mm\\:ss'))\r\n`\r\n\r\nbecomes harder than ensuring all the punctuation and special characters are easily readable like this:\r\n\r\n`powershell\r\n('....Total time to process: {0:g}' -f [timespan]::fromseconds(((Get-Date)-$StartTime).Totalseconds).ToString('hh\\:mm\\:ss'))\r\n`\r\n\r\nVisual Studio, SSMS, and other editors by default choose a monospaced font in code editing. However, there are additional options besides the built in fonts.\r\n\r\n some i've explored\r\n\r\nBitstream Vera Sans Mono: My go to for a long time. It's aesthetically nice, and has a bit of the Ubuntu styling with some rounder edges.\r\nFira Code Retina: Very nice with ligature support. This has become my current favorite due to the very nice style with the added perk of the ligatures. That's a nice little typography enhancement that really makes special combinations of characters stand out for readability. This is just a rendering feature that doesn't impact the underlying text per documentation:\r\n This is just a font rendering feature: underlying code remains ASCII-compatible. This helps to read and understand code faster. FiraCode Github\r\n\r\n\r\n\r\nwhat to what to look for\r\n\r\nAs you dive into the world of exploring fonts, here's a couple things I'd look for.\r\n\r\nCharacters that can hide problems are easily identified such as a period, or dash, most monospaced fonts are great for this, but some have smaller symbols that might make them a little less readable.\r\nResizes well for your target zoom. I've tried some fonts that don't seem to look right once you change your zoom level or the size of the font. I looked up some details on this and apparently some fonts are bitmapped, and some vector images. If you are using bitmapped fonts, then the target size is ideal, while adjusting zoom level can cause blurriness or fuzzy quality as it's not going to rescale like a vector based font would. This isn't bad if you are ok with the normal font size levels.\r\n\r\n\r\n\r\n{% include gallery caption=\"A few examples of 3 main fonts I've used and how they look for a sql script.\" %}\r\n\r\n\r\nSo far my personal favorite is Fira Code, so check that one out if you are looking for something interesting to try.\r\n\r\n\r\n resource links\r\n\r\nFiraCode Github\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-05-10-automate-windows-updates-for-development",
        "content": "---\r\ndate: \"2017-05-10T00:00:00Z\"\r\ntags:\r\npowershell\r\ntech\r\nsql-server\r\ntitle: Automate Windows Updates for Development\r\n---\r\n\r\nI've run into the case where I wanted updates continually applied, while the machine still was part of the GPO that didn't automatically install updates. For this developer and test oriented machine I wanted every update applied.\r\n\r\nI utilized a great module for this and created a script to setup the task and logging to make this an easy task.\r\n\r\nIf you experience an issue with the WindowsUpdate Vs Microsoft update as the configured update provider, then you can just change the switch in the script for  -MicrosoftUpdate to  -WindowsUpdate\r\n\r\nThis isn't something I'd run in production, but I've found it helpful to updating a development server with the latest SQL Server updates, as well as a development machine, allowing me to keep up with any latest changes with minimal effort.\r\n\r\nChange the reboot parameter to your preferred option in the script. I left as autoreboot for the purpose of a low priority dev server being updated.\r\n\r\n{{% gist 3dc7333846aa93d3f01daaefbcce2898 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-05-17-setting-up-influx-db-chronograf-and-grafana-for-the-sql-server-dev",
        "content": "---\r\ndate: \"2017-05-17T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\nsql-server\r\ninfluxdb\r\nperformance-tuning\r\ntech\r\npowershell\r\ncool-tools\r\ntitle: Setting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev\r\ntoc: true\r\n---\r\n\r\nOther Posts in Series\r\n\r\nRunning InfluxDb As A Service in Windows\r\nSetting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev\r\nInfluxDB And Annotations\r\nCapturing Perfmon Counters With Telegraf\r\n\r\n\r\nThere are some beautiful ways to visualize time series data with the tools I'm going to go over. This post is purely focused on the initial setup and saving you some time there. In a future post, I'll show how some of these tools can help you visualize your server performance in a powerful way, including taking metrics from multiple types of servers that be working with SQL Server, and combining the metrics when appropriate to give a full picture of performance.\r\n\r\n A beautiful way to visualize performance across a variety of machines\r\n\r\nIt's pretty epic to combine information across a variety of sources and be able to relate the metrics to the \"big picture\" that individual machine monitoring might fail to shed light on.\r\n\r\n Downloading\r\n\r\nI started by running this quick powershell script to download the stable toolkit.\r\n{{% gist 5fa33704e2599e3ddb46a8299ad3bafe %}}\r\n\r\nOnce extracted, I moved the influx extracted subfolder into the InfluxDB folder to keep it clean. Now all the binaries rested in C:\\Influx\\InfluxDB folder with no nesting folders.\r\nI referenced the documentation for getting started with InfluxDB.\r\n\r\nSetup Local InfluxDb\r\n\r\nStarted up the local influxdb binary.\r\n\r\n{{% gist 6f4e11d60244af00edac438cb9ae6ea5 %}}\r\n\r\n\r\nInitializing the new database was simple as documented: create database statty\r\n\r\n warning \"Case Sensitivity\"\r\n InfluxDB is case sensitive. Make sure to check your case if something isn't working, such as use \"DatabaseName\" instead of use \"databasename\"\r\n\r\nAlso, if you get an error with access to the file, try running as admin.\r\n\r\n\r\n\r\n More Enviromental Variable Fun\r\n\r\nA simple fix to errors related to paths and the HOME variable these tools often need, per a Github issue, was to ensure the current path was available as a variable. I did this quickly with a simple batch file to launch the consoles as well as one option, as well as updated the Start-Process script to include a statement to set the env variable for the processes being started. This eliminated the issue. For more details see github issues\r\n\r\n`batch\r\nSET HOME=%~dp0\r\nstart influxd.exe\r\nstart influx.exe\r\n`\r\n\r\nAn additional snippet for launching the console version via a bat file:\r\n\r\n`batch\r\nset HOME=C:\\influx\r\ncmd /k influx.exe -host \"MyInfluxDbHost\" -database \"statty\" -precision \"s\" -format column\r\n`\r\n\r\nQuick Start for Telegraf\r\n\r\nOnce you have this running you can take the telegraf binaries and run them on any other server to start capturing some default preset metrics. I launched with the following script and placed this in C:\\Influx directory to make it easy to access for future runs.\r\n\r\n{{% gist 1a9641ce607569dde912f996137debae %}}\r\n\r\n\r\nEdit the conf file to add some tags, change default sampling interval and more. I'll post another article about setting up telegraf to run as a service in the future so search for more info\r\n\r\nYou can also apply the same bat file in the startup directory such as:\r\n\r\n`batch\r\n@REM alternative is using variable\r\n@REM set TELEGRAFCONFIGPATH=C:\\telegraf\\telegraf.conf\r\n\r\nstart %~dp0telegraf.exe -config %~dp0telegraf.conf\r\n`\r\n\r\n Run Chronograf\r\n\r\nOne these metrics began to run, I ran Chronograf. This is Influx's alternative to Grafana, another more mature product.\r\n\r\n{{% gist 958094675f6ab53897616755dd130144 %}}\r\n\r\n\r\n\r\n\r\nUpon loading and opening up the instance monitor, I found immediately that I was able to get some metrics from the defaults.\r\n\r\n\r\n\r\nGet Grafana\r\n\r\nMy preferred visualization tool, this was far more robust and well documented than Chronograf which has promise, but is a relatively new project.\r\n\r\nWhen starting Grafana, you can run the following script. It creates a copy of the default ini to copy for the user to edit if not already there.\r\n\r\n{{% gist 3cff34cf9029bd99cd1e888e755c307c %}}\r\n\r\n\r\nOnce you open the localhost page, if you don't see datasources in the left hand drop down, create an organization and ensure you are an admin, you'll then see the option to add datasources. I simple pointed the page to InfluxDB console running on the server I had setup previously.\r\n\r\n summary\r\n\r\nThis is just a quick guide on getting started as I found a lot of little bumps in the road since the projects are written in GO and not an easily run .NET project. Getting through this will hopefully give you a way to get started. I'll blog a bit more soon on visualization of the metrics captured, some custom annotations to help make metrics come alive with real-time event notifications (like \"load test started\" and \"build ended\" etc). It's a really promising solution for those who want some really nice flexibility in using perfmon and related metrics to visualize Windows and SQL Server performance.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-05-24-running-influx-db-as-a-service-in-windows",
        "content": "---\r\ndate: \"2017-05-24T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\ninfluxdb\r\npowershell\r\nconfiguration\r\ntech\r\nsql-server\r\ntitle: Running InfluxDB as a service in Windows\r\ntoc: true\r\n---\r\n\r\nOther Posts in Series\r\n\r\n\r\nRunning InfluxDb As A Service in Windows\r\nSetting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev\r\nInfluxDB And Annotations\r\nCapturing Perfmon Counters With Telegraf\r\n\r\n\r\n Run as a Service\r\n\r\nAs part of the process to setup some metrics collections for sql-server based on perfmon counters I've been utilizing InfluxDB. Part of getting started on this is ensuring InfluxDB runs as a service instead of requiring me to launch the exe manually. For more information on InfluxDb, see my other post: Setting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev\r\n\r\nThis of course, did not go without it's share of investigation since I'm working with a compiled executable that was originally built in GO. I had issues registering InfluxDB as a service. This is typically due to enviromental/path variables. In my powershell launch of InfluxD.exe I typically used a script like the following:\r\n\r\n{{% gist 6f4e11d60244af00edac438cb9ae6ea5 %}}\r\n\r\nI investigated running as a service and found a great reminder on using NSSM for this: Running Go executables ... as windows services ' Ricard Clau I went and downloaded NSSM again and first setup and register of the service went without a hitch, unlike my attempt at running New-service -name 'InfluxDB' -BinaryPathName 'C:\\Influx\\influxdb\\InfluxD.exe' -DisplayName 'InfluxDB' -StartupType Automatic -Credential (get-credential). I'm pretty sure the core issue was the PATH variables and other related enviromental paths were not setup with \"working directory\" being the InfluxDB which would be expected by it.\r\n\r\nNSSM - Non-Sucking Service Manager\r\n\r\nUsing nssm install provided the GUI which I used in this case. Using the following command I was able to see the steps taken to install, which would allow reproducing the install from a .bat file very easily.\r\n\r\n    set-location C:\\tools\r\n    .\\nssm.exe dump InfluxDB\r\n\r\nThis resulted in the following output:\r\n\r\n    C:\\tools\\nssm.exe install InfluxDB C:\\Influx\\influxdb\\influxd.exe\r\n    C:\\tools\\nssm.exe set InfluxDB AppDirectory C:\\Influx\\influxdb\r\n    C:\\tools\\nssm.exe set InfluxDB AppExit Default Restart\r\n    C:\\tools\\nssm.exe set InfluxDB AppEvents Start/Pre C:\\Influx\\influxdb\\influx.exe\r\n    C:\\tools\\nssm.exe set InfluxDB AppEvents Start/Post C:\\Influx\\influxdb\\influx.exe\r\n    C:\\tools\\nssm.exe set InfluxDB AppNoConsole 1\r\n    C:\\tools\\nssm.exe set InfluxDB AppRestartDelay 60000\r\n    C:\\tools\\nssm.exe set InfluxDB DisplayName InfluxDB\r\n    C:\\tools\\nssm.exe set InfluxDB ObjectName SERVICENAME \"PASSWORD\"\r\n    C:\\tools\\nssm.exe set InfluxDB Start SERVICEAUTOSTART\r\n    C:\\tools\\nssm.exe set InfluxDB Type SERVICEWIN32OWN_PROCESS\r\n\r\nPretty awesome! It's a nice change to have something perfectly the first time with no issues.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-05-31-powershell-module-improvements-for-sql-server-in-2017",
        "content": "---\r\ndate: \"2017-05-31T00:00:00Z\"\r\ntags:\r\npowershell\r\nsql-server\r\ntech\r\ntitle: Powershell Module Improvements for SQL Server in 2017\r\n---\r\n\r\n info \"Updated: 2018-03-19\"\r\n I don't use these much, if any now. Check out dbatools which is a much better module with a full range of features to save you a ton of time.\r\n\r\nsimple setup\r\n\r\nA major improvement that seems to have quietly slipped into the sql developers world is an improved SQLServer powershell module. The improved module is finally available in the powershell gallery, allowing a super quick setup on a server. No more installing SSMS to get them!\r\n\r\nThis is very promising, and great if you want to leverage some of the functionality on various build servers, or other machines that might not have SSMS installed.\r\n\r\nPowershell Gallery - SqlServer\r\n\r\n new cmdlets\r\n\r\nIn reviewing, I ran across a few new cmdlet's as well. For instance, you could easily right click on a table and output the results into a powershell object, json, csv, gridview, or anything else you want. This is great flexibility.\r\n\r\n\r\n\r\n\r\nIn versions of SQL Server (as of 2012 or earlier) I believe the version SQL Server was utilizing was out of date with the installed version. For instance, on Windows Server 2012 with Powershell ISE reporting PsVersion of 4.0, Sql Server reported version 2.0 being utilized.\r\n\r\nIn 2014 instances I had, the powershell invoked from SSMS shows the matching up to date version, which gives much better capability and functionality.\r\n\r\nsimple benefits for the inquiring mind\r\n\r\nIf you are not familar with the potentional benefits from being able to quickly invoke a powershell prompt and use SQL server cmdlets (prebuilt functionality that is easily called), I can give you a few use cases.\r\n\r\nIf you were asked to run a query, then export the results to a spreadsheet, it would be relatively simple as a cut and paste. However, if you needed to loop through every table in the database, and put each one to it's own excel workbook, powershell would allow you to quickly loop, convert the datatable returned into an excel worksheet, and either append into new worksheets, or create completely seperate new files. For automation possibilities, you've got a tremendous amount of potentional time savings if you can get comfortable with powershell.\r\n\r\nIn my case, I've found Powershell to be a great tool to help me understand more of the .NET framework as I use various cmdlets or .NET accelerators.",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-06-07-add-user-to-admin-group-on-machine",
        "content": "---\r\ndate: \"2017-06-07T00:00:00Z\"\r\ntags:\r\npowershell\r\ntech\r\ntitle: Add User To Admin Group on Machine\r\n---\r\n\r\nIn setting up some build machines for development, it's tedious to go and add several users to the admin group. Here's a snippet to expedite that task and help you setup more quickly.\r\n\r\n{{% gist aeaeceb3716227c246ede5f94e6b0113 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-06-07-best-practices-defining-explicit-length-for-varchar-nvarchar",
        "content": "---\r\ndate: \"2017-06-07T00:00:00Z\"\r\ntags:\r\nsql-server\r\ntech\r\ntitle: 'Best Practices: Defining Explicit Length for Varchar/Nvarchar'\r\n---\r\n\r\n SA0080 : Do not use VARCHAR or NVARCHAR data types without specifying length. Level: Warning\r\n\r\nWhen using varchar/nvarchar it should be explicitly defined. This can be a very nasty bug to track down as often nothing will be thrown if not checked in an application. Instead, ensure your script explicitly defines the smallest length that fits your requirements. The reason I rate this as a very dangerous practice, is that no error is thrown. Instead, the results being returned will be shorter than expected and if validation checks aren't implemented this behavior can lead to partial results returned and used. Make sure to always explictly define length!\r\n\r\nHere's an short example script that demonstrates the behavior.\r\n\r\n{{% gist ab043077045ed9d5f26b4bbf6b326f45 %}}",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-06-26-edit-in-vscode",
        "content": "---\r\ndate: \"2017-06-26T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-09\"\r\ntags:\r\nsql-server\r\ntext-manipulation\r\nvscode\r\ntech\r\ncool-tools\r\ntitle: External Tool VSCODE called from SQL Management Studio\r\n---\r\n\r\nPrevious Related Post:\r\nSplit personality text editing in SSMS with Sublime Text 3\r\n\r\nIn this prior post I wrote about how to call Sublime Text 3 from SSMS to allow improved text manipulation to be quickly called from an active query window in SQL Management Studio. Vscode is a newer editor from Microsoft, and the argument calls took a little work to get working. Here is what I found for having your SQL file open in vscode via call from SSMS (I imagine also works in Visual Studio 2017 this way as well).\r\n\r\nExternal Tools Setup for Vscode\r\n\r\n`text\r\nTitle:  \"Edit In VSCODE\"\r\nCommand C:\\Program Files (x86)\\Microsoft VS Code\\Code.exe\r\nArguments: --reuse-window --goto $(ItemPath):$(CurLine):$(CurCol)\r\n`\r\n\r\nPlease note unsaved files such as \"SQLQuery11.sql\" that haven't been explictly saved are not accessible to this, so it will just open an empty file. I have not found any workaround for that, as I believe the tmp files are cached in one of the .DAT files. I've not had luck finding the Autorecover or temp files with the actual contents until saved.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-07-03-update-ssms-with-ps1",
        "content": "---\r\ndate: \"2017-07-03T00:00:00Z\"\r\nexcerpt: Installing and Updating SSMS with a simple PowerShell script can be a nice\r\n  little timesaver...\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\nsql-server\r\npowershell\r\ncool-tools\r\ntech\r\ntitle: Update SSMS With PS1\r\n---\r\n\r\n Update \"Updated: 2018-03-29\"\r\n Use Chocolatey. This page keeps changing it's structure, so the regex to parse for Ketarin and this PS1 script keep breaking. Updated to latest version as of 2018-03-29, but recommend checking out the Chocolately Package created for SSMS for this by flcdrg as chocolately is a much nicer way to keep up to date and more likely long-term to succeed than my gist :-) To use chocolatey (after setup), you only have to use choco upgrade sql-server-management-studio which is much easier to remember than using this gist. Still a nice light-weight tool.\r\n Also, for less overhead, investigate SQL Operations Studio instead of SSMS for those situations you need to run some queries on a machine. Less overhead, size, and complexity for some nice basic SQL Server management functionality (even if it is missing my precious SQL Prompt)\r\n\r\n\r\nWith how many updates are coming out I threw together a script to parse the latest version from the webpage, and then provide a silent update and install if the installed version is out of date with the available version. To adapt for future changes, the script is easy to update. Right now it's coded to check for version 17 (SSMS 2017). I personally use Ketarin, which I wrote about before if you want a more robust solution here: Automating SSMS 2016 Updates & Install\r\n\r\nThe bat file is a simple way for someone to execute as admin.\r\n\r\nHope this saves you some time. I found it helpful to keep a bunch of developers up to date with minimal effort on their part, since SSMS doesn't have auto updating capability, and thus seems to never get touched by many devs. :-) Better yet adapt to drop the SSMS Installer into a shared drive and have it check that version, so you just download from a central location.\r\n\r\n{{% gist 8f2bbd2455fe2f2ba8d41af33525464e %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-07-21-ants-performance-profiler-for-the-sql-server-dev",
        "content": "---\r\ndate: \"2017-07-21T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\nsql-server\r\ncool-tools\r\nperformance-tuning\r\nredgate\r\npowershell\r\ntech\r\ntitle: ANTS Performance Profiler for the SQL Server Dev\r\n---\r\n\r\nThere are a few .NET tools that until recently I haven't had the chance to work with as much, specifically ANTS Memory Profiler and ANTS Performance Profiler. The memory profiler is more useful for someone focused on memory leaks which a SQL Dev isn't as focused on for performance tuning. However, there are major benefits for diving into SQL Performance tuning with ANTS Performance profiler. I think I'd say this tool makes the epic category of my #cooltools kit and will be added to my COOL TOOLS page here for SQL performance tuning.\r\n\r\nOne of the most challenging processes for profiling activity is really identifying the single largest pain point. Trying to line up timings with the SQL plans and the application side by side is a big timesaver, and Red Gate improved ANTS Performance profiler to include the executed SQL with execution plans, making it a single stop to profile and get some useful information.\r\n\r\nThere are other ways to get useful information, such as running Brent Ozar's First Responder kit queries, Glenn Berry's diagnostic queries, Query Store, and more. These tend to focus on server performance. As someone working in software development, there is something to be said for the simplicity of running the application and profiling the .NET and SQL performance in one captured & filtered result set. It's a pretty quick way to immediately reduce noise and view a complete performance picture of the application.\r\n\r\nFor performance profiling, Visual Studio has an option called Performance Profiler. I found my initial look at it to be positive, just really noisy.\r\n\r\nDisclaimer: As a member of the Friends of Red Gate program, I get to try out all the cool Red Gate tools. Lucky me! This doesn't bias my reviews as I just like great tools that help me work with SQL server. This is one of them!\r\n\r\nProfiling .NET App\r\n\r\n Setting up your profiling session\r\n\r\nAt the time of this articles publishing, there is no 2017 Visual studio extension which makes this process a few clicks less. For now, it still is simple. All you do is go to the bin/debug folder and select the executable you want to profile. Attaching to the .NET excecutable is required for my purpose, as attaching to an existing process doesn't give you the ability to get all the SQL calls which we definitely want.\r\n\r\n Timeline & Bookmarks\r\n\r\n\r\n\r\nDuring the profiling you can perform actions with the application and create bookmarks of points in time as you are performing these actions to make it easier to compare and review results later.\r\n\r\nReviewing Results\r\n\r\n Call Tree View\r\n\r\nThis is based on the call tree. It shows code calls, and is a great way to be the database guy that says... \"hey SQL server isn't slow, it's your code\" :-)\r\n\r\n Database Calls\r\n\r\n Database Calls\r\n\r\nThe database calls are my favorite part of this tool. This integration is very powerful and lets you immediately trim down to the calls made with timings and associated executed sql text. RG even went and helped us out by providing an execution plan viewer! When I first saw this I fell in love. Having had no previous experience with Entity framework of other ORMs, I found the insight into the performance and behavior of the application to be tremendously helpful the first time I launched this.\r\n\r\nExporting HTML Report\r\n\r\n HTML Exported Report\r\n\r\nA benefit for summarizing some action for others to consume is the ability to select the entire timeline, or narrow to a slide of time, and export the results as a HTML report.\r\n\r\nThis was pretty helpful as it could easily provide a way to identify immediate pain points in a daily performance testing process and focus effort on the highest cost application actions, as well as database calls.\r\n\r\n Automation in Profiling\r\n\r\nRG Documentation shows great flexibility for the profiler being call from command line. I see a lot of potential benefit here if you want to launch a few actions systematically from your application and establish a normal performance baseline and review this report for new performance issues that seem to be arising.\r\n\r\nI generated some reports automatically by launching my executable via command line, profiling, and once this was completed, I was provided with a nice formatted HTML report for the calls. At the time of this article, I couldn't find any call for generating the SQL calls as their own report.\r\n\r\n{{% gist 8055f44fd25af7d010ba22c6e54692e4 %}}\r\n\r\n\r\nTL;DR\r\n\r\nPros\r\n\r\nIncredibly powerful way to truly get a picture into an application's activity and the true pain points in performance it is experiencing. It truly helps answer the question very quickly of what is the area that needs the most attention.\r\nVery streamlined way to get a summary of the SQL activity an application is generating and the associated statements and execution plans for further analysis.\r\n\r\nCons\r\n\r\nAt times, with larger amounts of profiled data the application could feel unresponsive. Maybe separating some of the panes activity into asynchronous loads with progress indicators would make this feel better.\r\n\r\n* Neutral/Wishlist *\r\n\r\nMore an observation than complaint, but I sure would like to see some active work being released on this with more functionality and SQL performance tuning focus. Seems to be stable and in maintenance mode rather than major enhancements being released. For those involved in software development, this tool is a powerful utility and I'd love to see more improvements being released on it. RedGate... hint hint? :-)\r\nI'd like to see even more automation focus, with the option of preset Powershell cmdlets, and team foundation server task integration to help identify changes in performance patterns when scaled up. Leveraging this to help baseline application performance overall and report and develop trends against this might help catch issues that crop up more quickly.\r\n\r\n additional info on more profiling focused apps\r\n\r\nSince the material is related, I thought I'd mention a few tools I've used to help profile activity, that is not focused on a wholistic performance analysis, and more about activity.\r\n\r\nFor more \"profiling\" and less performance analysis my favorite SQL profiling tool Devart's DbForge Sql Profiler uses extended events and while amazing, isn't as focused a tool for app and SQL performance analysis. If you haven't checked that tool (free!) out I highly recommend it vs running profiler. It uses extended events and provides a nice experience in profiling and reviewing results. Super easy to use and very flexible for filtering/sorting/exporting. The only issues I have with it are the filtering setup is annoying, but tolerable to work with, and no execution plans that I've been able to find built in, unlike running extended events in SSMS directly. Hopefully, Devart will recognize what an awesome tool they've made and continue to push it forward.\r\nFor just getting Entity framework and other ADO.net calls you can use intellitrace with the option for ADO.NET tracing enabled. I found this nice, but a little clunky to use compared to Linq Insight or other options mentioned. It's included with visual studio so if only using periodically then this would be ok to work with.\r\nFor a cleaner implementation of Entity Framework Profiling than the Intellitrace route use Devarts dbForge Linq Insight (I love this tool for cleaner profiling of ADO.NET activity when you aren't focused on overall performance of the application) and are working in Visual studio.\r\n\r\nIf all else fails... you can always succumb to dark side and just use SQL Profiler or worse yet...SSMS activity monitor :-)\r\n\r\n Image courtesy of Gratisography.com CC0\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-05-spotify-vs-google-play",
        "content": "---\r\ndate: \"2017-08-05T00:00:00Z\"\r\ntags:\r\nmusic\r\nramblings\r\ntitle: Spotify vs Google Play\r\n---\r\n\r\nI've recently switched over to Spotify for the nth time to explore the Spotiverse world and compare as a long time Google Play Music user. This is a big deal for me, as I was a first adopter of Google Play and have used it before All Access, and subscribed immediately to All Access once it was released. As a ravenous music consumer, I've preferred the subscription model for instant, on demand access.\r\nThis is my personal comparison of some of the strengths and weaknesses of each, compiled over actual usage. Hopefully, it will help anyone who is interested in the differences and trying to decide.\r\n\r\nSpotify\r\n\r\nPlaylists: A wealth of socially driven playlists exist with Spotify. You can even create a adhoc playlist on the fly via their api from your own music app. They also have collaborative lists so it's very easy to build a playlist with like minded folks.\r\nCrossfade: It's a pretty smooth experience to have tracks merge seamlessly. Setting 12 secs of crossfade makes it feel like a DJ is mixing up a radio experience for you.\r\nRadio is getting better than my first experience of it, at playing related music.\r\nExplicit filtering. Spotify doesn't offer as a global setting. They suprisingly seem to have ignored user requests for this. Makes it a little less family friendly if you aren't careful.\r\nAbility to follow artists helps with good \"new releases\" I'd be interested in.\r\n\r\n Google Play\r\n\r\nRadio is ok.\r\nOffline: Pinning a radio station gives you a fresh mix of songs, but not too many. Pinning a playlist in spotify for offline = every song downloaded.\r\nQuality designated for mobile vs wifi. Spotify just has Download vs Stream, with no designation between streaming on a wifi network and mobile network.\r\nExplicit filter is implemented.\r\nTerrible new release feed. For a while it never even updated (some bug in sub-genre at the time I believe). No way to mark uninterested, so recommendations are pretty off.\r\n\r\nThings I Can't Believe Aren't Fixed\r\n\r\nGlobal Thumbs Down. Why is this not there! Spotify should add a global thumbs down, instead of just in radio stations. I should be able to dislike a song to remove it from any playlist or the recommended discovery songs. Google Play has a little better experience with this, but still far to limited.\r\n\r\n algorithms taking away my choice?\r\n\r\nThis seems speak to the mentality today that algorithms and user behavior drive all the results, not requiring user feedback. The negative to this is for people like myself who want to improve the results I'm offered... well we are left out in the cold. Pandora offers the best radio design out there, imo. They rely on feedback to help ensure the results are provided as the user wants. I'd like to see this design choice in more of the music services instead of them trying to assume they've identified my preferred choices.\r\n\r\nhonorable mention Pandora\r\n\r\nOverall, the winner for me is Spotify. However, if Pandora's new service wasn't so darn limited in selection, inconsistent with on demand availability, and implemented on desktop as well as mobile I'd recommend them. As it stands, I can't recommend it at this time.\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-07-dataedo-first-look",
        "content": "---\r\ndate: \"2017-08-07T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-22\"\r\ntags:\r\nsql-server\r\ncool-tools\r\ntech\r\ntitle: Dataedo - First Look\r\n---\r\n\r\n info \"update 2019-02-22\"\r\n Image links broken. Since their product is continually improving, I'm going to just link to their product here instead so you can review their latest demo content there. Dataedo.\r\n Overall, I've enjoyed the product and think it has been improved over time. There are SQL scripts for bulk updating certain values on their website that can help improve building a project for an existing database as well.\r\n\r\nDiagramming and Documentation\r\n\r\nPreviously, I've written up on database diagrammingfor visualization of database structures. Check that out for more detailed review on what I've used.\r\n\r\nI've recently taken a swing at this newer tool and found it very promising as a database documentation tool that bridged the gap of usability and flexibility.\r\n\r\n TL;DR\r\n\r\nPromising future tool for diagramming, and worth evaluating if looking to build out some documentation for the first time. For my purposes, it was missing some diagramming options that prevent me from leveraging as a replacement for my existing toolkit.\r\n\r\nSetting Up Initial Diagrams\r\n\r\nThis process was very manual, and did not allow filtering a selection of tables and dragging or bulk added to a module/category. I'm sure this will be an improvement quick to be implemented. At the current time, it was very tedious when dealing with a large database structure.\r\nOnce assigning a module, and then clicking to the next table, a modal pop-up would ask if I wanted to save instead of letting me continue to assign modules. To bypass this I had to hit Ctrl+S to save prior to navigating to the next table or dismiss the dialogue by clicking.\r\nDiscovered that moving to the Module  ERD tab allowed assignment of multiple tables or views to the ERD diagram. This provided the solution of easily assigning multiple objects to the ERD diagram, _but did not add the tables to the Module itself_, requiring the full manual step mentioned before. The filter tab was useful, though I was hoping for a basic search filter with a negation clause to help trim down the results selected. Example: CoreTables -ETL to allow easily filtering large amounts of objects. Maybe that would be a future enhancement the development team could add.\r\n\r\nThe only difference I could see for adding tables to the ERD when adding previously to the Module was that they were highlighted in bold before the other tables\r\n\r\n Exporting Customization Is Easy\r\n\r\nExporting documentation provided immediate feedback on generating a custom template, along with all the required files. This was a definite plus over some other tools I've worked with, as it promoted the customization that would be required by some, with all the necessary files generated. My props to the developers of this, as this showed a nice touch for their technical audience, not forcing the user into a small set of options, or making it complicated to customize.\r\n\r\nNo delete button for the CustomTemplate was a bit confusing, but an easy fix for them in the future. At this time, you'd just delete the folder in Dataedo/Templates/HTML and they won't show up in the template dialogue.\r\n\r\nDuring the export process you also have the option of saving the export command options you already setup to a dataedo command file to make it easily automated. That's a nice touch!\r\n\r\nERD Diagrams\r\n\r\nPROS\r\n\r\nFirst, the snapping and arrangement of the connectors was excellent. This allows easy rearrangement with clean relationship lines shown.\r\nThe generated documentation files looked fantastic, and with some improvements and customization, I could see this generating a full documentation set that would make any dba proud :-)\r\n\r\nCONS\r\n\r\nI could not find any \"auto-arrange\" or \"preferred layout\" options to arrange in a set pattern if I didn't like the way I had changed it or it had laid it out initially\r\nNo option to show the columns that have FK relationships. The relationship connector could be shown, with a label, but nothing to match Column5 that had a FK but was not part of the primary key to the matching column on another table. The diagram displayed only the PK columns. For my requirements, this was a critical omission as I need to display PK, and FK.\r\n\r\n Core Features I'd Like To See\r\n\r\nSearch box so that I could replace CHM files with the local HTML document. This would require a search mechanism to allow easily finding what the user needed. Currently, no provider I've tested has implemented a local html package that included a static file search that worked well.\r\nImproved ERD with FK/PK\r\nImproved ERD with auto-layout options. Based on my initial research I'd say this is a tough one to implement, but giving a basic layout option to the user and then allowing customization from there would be a great help.\r\nGrouping objects in the ERD to group related elements in part of a larger module\r\nProducivity enhancements to allow quickly creating multiple modules, and dragging objects into the modules. Eliminate manual 1 by 1 actions to work with those.\r\n\r\nend comments\r\n\r\nWell done Dataedo team :-) Looking forward to the continued improvements. I've found visualization of database structures is very helpful to design, and a new toolkit out like yours promises to provide even more great tools to use to do this.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-07-influx-db-and-annotations",
        "content": "---\r\ndate: \"2017-08-07T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\ntime-series\r\npowershell\r\ninfluxdb\r\nmonitoring\r\nsql-server\r\ncool-tools\r\ntitle: InfluxDB and Annotations\r\ntoc: true\r\n---\r\n\r\nOther Posts in Series\r\n\r\nRunning InfluxDb As A Service in Windows\r\nSetting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev\r\nInfluxDB And Annotations\r\nCapturing Perfmon Counters With Telegraf\r\n\r\n\r\nThis post assumes you've already setup InfluxDB and have Grafana running.\r\n\r\n Inserting annotations\r\n\r\nAnnotations are not a special type of resource, instead it's just another metric that you query with a feature in Grafana to display on other metrics. This means the same insert Line Protocol applies to the Annotation.\r\n\r\nThis post on maxchadwick.xyz greatly helped me get started: Creating Grafana Annotations with InfluxDb Max Chadwick\r\n\r\nPer Max's original post it supports html as well, so you could link for example to a build, test result, or anything else you want to link to from your performance statistics.\r\n\r\n{{% gist e95ca6d909f741ebe80fa28c6da4de5b %}}\r\n\r\n\r\nThis provides an annotation on your timeline in a nice format for browsing through the timeline. I can see usage cases for identifying specific activity or progress in tests, helping coorelate the performance metrics with known activity steps from a build, script, or other related tasks. You could have an type of activity trigger this powershell insert, providing a lot of flexibility to help relate useful metrics to your monitoring.\r\n\r\nMy personal use case has been to ensure load testing start/end times and other significant points of time in a test are easily visible in the same timeline I'm reviewing metrics on.\r\n\r\nWarning: I did experience performance degradation with Grafana and many annotations on a timeline. I found just disabling the annotations kept this from occurring, so you only pull them when youd them.\r\n\r\n\r\n\r\nAdding Annotations to Grafana\r\n\r\nNow that you have the results being inserted into InfluxDB, you can query these in Grafana as annonations to overlay your graphs.\r\n\r\n\r\n\r\n Potential Uses\r\n\r\nI could see a whole lot of uses for this!\r\n\r\ninsert at build related activity\r\nWindows update\r\nSpecific Database Related Maintenance like Ola Hallengren's index optimize or database integrity check\r\n\r\nMonitoring always loses it's value when you have a limited picture of what is happening. Triggering relevant details for stuff that might help analyze activity might be the key to immediately gaining an understanding on what is causing a spike of activity, or of better evaluating the timeline of a load test.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-08-capturing-perfmon-counters-with-telegraf",
        "content": "---\r\ndate: \"2017-08-08T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\nsql-server\r\nmonitoring\r\ngrafana\r\ninfluxdb\r\ncool-tools\r\npowershell\r\ntitle: Capturing Perfmon Counters With Telegraf\r\n---\r\n\r\nOther Posts in Series\r\n\r\nRunning InfluxDb As A Service in Windows\r\nSetting Up InfluxDb, Chronograf, and Grafana for the SqlServer Dev\r\nInfluxDB And Annotations\r\nCapturing Perfmon Counters With Telegraf\r\n\r\n\r\n Setting up Telegraf to Capture Metrics\r\n\r\nI had a lot of issues with getting the GO enviroment setup in windows, this time and previous times. For using telegraf, I'd honestly recommend just leveraging the compiled binary provided.\r\n\r\nOnce downloaded, generate a new config file by running the first command and then the next to install as service. (I tried doing through NSSM originally and it failed to work with telegraf fyi)\r\n\r\n{{% gist 583210cfb588d1958b5c2ba67515ec29 %}}\r\n\r\n\r\nOnce this service was setup and credentials entered, it's ready to run as a service in the background, sending whatever you've configured to the destination of choice.\r\n\r\nIn my test in Amazon Web Services, using EC2 with Windows Server 2016, I had no issues once EC2 issues were resolved to allow the services to start sending their metrics and show me the load being experienced across all in Grafana.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-09-exploring-sql-server-with-powershell-and-smo-basics",
        "content": "---\r\ncategories:\r\nsql-server\r\npowershell\r\ndate: \"2017-08-09T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\nsql-server\r\npowershell\r\nsmo\r\ntech\r\ntitle: Exploring SQL Server With Powershell And SMO Basics\r\n---\r\n\r\nSqlServer Powershell Cmdlets 2017 - Initialize Look\r\n\r\nDiving into the Sql Server Management Objects library can be a pretty interesting process. You get to work with database objects as in a new way, and begin manipulating and execute code in a much different approach than purely using T-SQL. Powershell offers a unique way to interact with prebuilt cmdlets, and you can explore leveraging .NET in powershell as well to have a powerful toolkit of options.\r\n\r\nThis post is a not focused on a full walk-through, but instead to communicate some of the exploration I've done, to help if you are beginning to explore more database automation and management.\r\n\r\nI plan on doing some basic walk-throughs for the powershell newbie in the future, so if you are confused about anything powershell related feel free to post a comment and I'll add it to my list of stuff to walk through.\r\n\r\n cmdlets vs .NET approach\r\n\r\nWhat I've found interesting is there are really 2 main approaches to interacting with SQL Server. You can directly invoke the SMO dlls and access the methods, properties, and extensibility this offers. This requires more .NET knowledge as you would be directly working with the SMO namespace, in a way that is almost the same as what you code in C#. The other approach is to leverage cmdlets. The cmdlets try to abstract away a lot of the complexities that working directly with the SMO namespace for ease of use and automation, and to simplify the process for those not as comfortable with coding in C# or directly leverage the SMO namespace in C#\r\n\r\nIf purely focused on automation and little experience working with .NET then cmdlet's will be by far the way to go. There is a serious learning curve in working with .NET directly vs prebuilt cmdlets. If desiring to expand your .NET knowledge, as well find that the prebuilt cmdlets don't offer the behavior you are trying to achieve, then exploring the SMO namespace for directly invoking the methods and accessing properties can be valuable. The learning curve is more intense, so just be prepared for that if you are new to working with .NET directly in Powershell.\r\n\r\ndbatools.io & other sources\r\n\r\nWhen possible, I personally am going to recommend to leverage a package like dbatools instead of rolling your own. Dbatools.io is a powerful project that I've recently begun to explore more. This well rounded package gives you a powerful powershell set of commands that can help you set server properties, obtain default paths, backup, restore, migrate entire sets of databases to a new location and more. To code all of this from scratch would be a massive project. I'd recommend considering dbatools.io and just getting involved in that project if you have something to contribute. I found it really helpful to quickly setup some default server options without having to configure manually myself.\r\n\r\n Exploring SQL Path Provider\r\n\r\n\r\n\r\nTrying to find the path initially can be challenging. However, by opening SSMS up, right clicking, and launching the powershell window you'll be able to easily find the correct path to get the server level object.\r\n\r\nThis allows you to leverage default methods in powershell like Get-ChildItem for iterating through objects. It treats the navigated SQL server path basically as a \"file structure\" allowing some interesting actions to be performed. One of these is a different approach to killing connections to a particular database.\r\n\r\nI found this great pointer by reading Killing SPIDS in Powershell from MidnightDBA\r\n\r\nReview that article for scripts focused on the termination of running spids.\r\n\r\nFor an adhoc purpose the scripts MidnightDba provided are excellent and would allow quickly executing a kill script on connections from ssms  powershell prompt.\r\n\r\n    import-module -name sqlserver -disablenamechecking -verbose:$false -debug:$false\r\n     CD SQLSERVER:\\SQL\\$ServerName -Verbose:$false -Debug:$false\r\n     dir ' ?{$.Name -eq \"$DatabaseName\"} ' %{$.KillAllProcesses($DatabaseName)}\r\n\r\nI approach this with a different method in one final script using just the SMO server method KillAllProcesses. For some tasks I've found it really helpful to have a simple 1 line kill statement thanks to MidnightDba's pointer with the statements similar to the one above.\r\n\r\nUsing Microsoft's documented method shows another example of how to use to restart the service. This was one modified approach I took. I prefer not to use this type of approach as working with get-childitem with server objects to me as a little unintuitive.\r\n\r\n    <#\r\n            .LINK https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/start-stop-pause-resume-restart-sql-server-services#PowerShellProcedure\r\n\r\n    #\r\n    $ErrorActionPreference = 'continue'\r\n    [bool]$Matched = (get-module -name SqlServer -listavailable ' measure-object).count\r\n    if($Matched -eq $false) { install-package SqlServer -scope CurrentUser -verbose:$false -Force}\r\n\r\n    [datetime]$StepTimer = [datetime]::Now\r\n\r\n    $private:ServerName = $env:ServerName\r\n    import-module -name sqlserver -disablenamechecking -verbose:$false -debug:$false\r\n\r\n    # Get a reference to the ManagedComputer class.\r\n    CD SQLSERVER:\\SQL\\$private:ServerName -Verbose:$false -Debug:$false\r\n    $Wmi = (get-item -debug:$false -verbose:$false .).ManagedComputer\r\n    $DfltInstance = $Wmi.Services['MSSQLSERVER']\r\n\r\n    #Display the state of the service.\r\n    write-host \"Stopping Instance: $($DfltInstance.ServiceState.value__)\"\r\n    $DfltInstance.Stop()\r\n\r\n    while($DfltInstance.ServiceState.value__ -ne 1) #1 stopped\r\n    {\r\n        Start-Sleep -seconds 5\r\n        $DfltInstance.Refresh()\r\n        write-host \"... state: $($DfltInstance.ServiceState)\"\r\n    }\r\n    ## Start the service.\r\n    $DfltInstance.Refresh()\r\n    write-host \"Current Service State: $($DfltInstance.ServiceState)\"\r\n    write-host \"Initiating Service Start\"\r\n    $DfltInstance.Start()\r\n\r\n    while($DfltInstance.ServiceState.value__ -ne 4) #4 running\r\n    {\r\n        Start-Sleep -seconds 5\r\n        $DfltInstance.Start()\r\n        $DfltInstance.Refresh()\r\n        write-host \"... state: $($DfltInstance.ServiceState)\"\r\n    }\r\n    write-host( \"{0:hh\\:mm\\:ss\\.fff} {1}: finished\" -f [timespan]::FromMilliseconds(((Get-Date)-$StepTimer).TotalMilliseconds),'SQL Service Restart')\r\n\r\nDatabase as an Object\r\n\r\nGetting the database as an object proved to be easy though, if a little confusing to navigate initially.\r\n\r\n    $s = SqlServer\\Get-SqlDatabase -ServerInstance $ServerInstance -Verbose\r\n\r\nOnce the object is obtained, you can begin scripting objects, change database properties and more very easily.\r\n\r\nI found this method an interesting alternative to invoking using .NET accelerators as it was a quick way to easily get a database level object to work with. However, some of the limitations of not having the server level object immediately available made me end up preferring the .NET accelerator version which could look like this.\r\n\r\n    param(\r\n             $ServerName = 'localhost'\r\n             ,$DatabaseName = 'tempdb'\r\n     )\r\n     $s = [Microsoft.SqlServer.Management.Smo.Server]::New($ServerName)\r\n     $d = [Microsoft.SqlServer.Management.Smo.Database]::New($s, $DatabaseName)\r\n     $s.EnumProcesses() ' format-table -AutoSize\r\n     $d.EnumObjects() ' Out-GridView\r\n\r\nInterestingly, to actually access the many of the database properties you actually would call it via reference to the server object with SMO calls instead of the cmdlet. Trying $d.PrimaryFilePath doesn't work as I believe it's initiating the instance of a new database object for creation instead of referencing the initialization of a new object to an existing database. I found documentation a bit challenging to immediately sift through to get an answer, so YMMV. Someone coming from a .NET focused background might find the process a little more clear, but for me it did take some work to correctly identify the behavior.\r\n\r\n    doesn't work. Probably trying to initialize new object for creating a db\r\n     $d = [Microsoft.SqlServer.Management.Smo.Database]::New($s, $db)\r\n     $d.PrimaryFilePath\r\n\r\n     #works to access current existing object\r\n     $s.Databases[$db].PrimaryFilePath\r\n\r\nExploring Properties\r\n\r\nIf you want to explore properties of an object, try using the ever faithful get-member\r\n\r\nDepending on the type of object, you can additionally explore them with GetEnumerator, GetProperties, etc. You'll find intellisense helpful as you explore more.\r\n\r\nFor instance, here's a walkthrough on the various ways you might explore the object and find you need to dig into it to get the full detail of what you have access to.\r\n\r\n{{% gist e3ed8534b1565c67d6d59163b0921d59 %}}\r\n\r\n\r\n Comparing Restoring a Database with Cmdlet vs SMO\r\n\r\nusing dbatools cmdlet\r\n\r\nAn example of how simple using dbatools cmdlet can make restoring a database copy\r\n\r\n{{% gist 7314ffa3fc830f36a2eda8ee7e27f7c4 %}}\r\n\r\n\r\n rolling your own wheel\r\n\r\nNow compare this to the complexity of running your own invocation of the SMO namespace and requires a lot more coding. Since dbatools wraps up a lot of the functionality, I've actually migrated to leveraging this toolkit for these dba related tasks instead of trying to reinvent the wheel.\r\n\r\n{{% gist 08fe28dd236a239f25821378268ef8e5 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-11-fixing-virtual-box-only-showing-32bit-windows-os-options",
        "content": "---\r\ndate: \"2017-08-11T00:00:00Z\"\r\ntags:\r\ntroubleshooting\r\nvirtualization\r\nramblings\r\ntech\r\ntitle: Fixing VirtualBox Only Showing 32bit Windows OS Options\r\n---\r\n\r\nOriginal help was identified from this article Why is VirtualBox only showing 32 bit guest versions on my 64 bit host OS?\r\n\r\nIn browsing through the comments, I saw mention that the root issue is that Hypervisor running interferes with Virtualboxes virtual management, so I disabled Hypervisor service, repaired the install, and rebooted. I also disabled automatic start for Hypervisor.\r\nThis resolved the issue without requiring the uninstallation of the Hypervisor feature in Windows.\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-16-ultrawide-monitor-samsung-cf971-first-impressions",
        "content": "---\r\ndate: \"2017-08-16T00:00:00Z\"\r\ntags:\r\ngear\r\nramblings\r\ntech\r\ntitle: Ultrawide Monitor - Samsung CF971 - First impressions\r\nexcerpt: \"\"\r\n---\r\n\r\nAs a developer, I've been a big fan of multiple monitors for a long time. I moved to a triple screen setup years ago, and up until recently, had no desire to try anything different.\r\n\r\nWell... I've finally found the replacement. The holy grail of productivity, immersion, as well as usability. Ultrawides :-)\r\n\r\nDon't you miss the extra screen?\r\n\r\nNo. The width of the screen is plenty for 2 full Visual Studio environments, or 3 text editors side by side. I've always had issues with effectively managing multiple IDE environmen with floating windows as those can tend to get pretty confusing on which parent they belong to. I've found I prefer to keep floating windows in the parent environment, and an ultrawide allows this type of workflow perfectly.\r\n\r\n Do you need the curve?\r\n\r\nI prefer it.the Samsung CF971 is curved at 1500R, which is one of the most curve displays as of the current time. Unlike TV, where this feature is pretty much useless, with an ultrawide monitor only a few feet from your face, the curve is welcome, and still subtle. I do videography, photography, and some gaming, and for my purposes I've found it to be perfect.\r\n\r\nDo I need to get a 3440x1440 or can I save money and get 2560x1080?\r\n\r\nGet the 3440x1440 if you are doing anything besides gaming. Then for gaming just toggle to the lower resolution. I way overthought this too, and thought the downscaling would be terrible. It's not. Don't listen to the naysayers. If you aren't a competitive ESports gamer, then it will serve you just fine at the lower resolution, and then you can toggle back up to the high resolution for anytype of productivity work. I recommend Display Fusion as you can setup some quick monitor profiles to toggle without delay.\r\n\r\n Gaming on it?\r\n\r\nAwesome! I still am running an old AMD 7950, and when I down the settings I'm still enjoying it. It's about time with Vega to upgrade, but until then, I'm still getting some life out of the current card, and still not leveraging the 100hz refresh rate with Freesync.\r\n\r\nAny negatives?\r\n\r\nI had 1 stuck pixel that I couldn't fix, so with black background this small dot can be seen (red). However, I'm still happy with the choice.\r\n\r\n For Development\r\n\r\nI plan on moving to this screen in the future for development as well. I included some images of the workspace you gain, so I have no complaints about my time at home working on it.\r\n\r\nBut I'm scared of moving away from IPS!\r\n\r\nMe too. No regrets. The SVA panel beautiful. No complaints whatsoever, and no regrets about leaving behind my IPS display for this.\r\n\r\n{{% fancybox-gallery\r\n\"gallery1\"\r\n\"20170708+1304+2352134.jpg\"\r\n\"20170708+1429+2352140.jpg\"\r\n\"20170708+1445+2352144.jpg\"\r\n\"20170708+1446+2352147.jpg\"\r\n\"2017-07-08_14-52-53.png\"\r\n\"2017-07-08_16-34-48.png\"\r\n\"2017-07-08_14-51-59.png\"\r\n\"2017-07-08_16-04-20.png\"\r\n\"2017-07-08_15-16-31.png\"\r\n\"2017-07-08_16-03-25.png\"\r\n\"2017-07-20_12-45-12.png\"\r\n%}}",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-29-a-not-so-waterlogged-texan-tuesday",
        "content": "---\r\ndate: \"2017-08-29T00:00:00Z\"\r\ntags:\r\nramblings\r\nflood\r\ntitle: A Not-So Waterlogged Texan - Tuesday\r\n---\r\n\r\n info \"Updated: 2017-08 1:30pm Wednesday\"\r\n Better than expected, we are looking to be back in our house Thursday. Apparently some SUV's are just starting to gain access. Don't want to risk it so giving it a little extra time. That's much better than I expected at beginning of this whole situation!\r\n Relief efforts mentioned at bottom are underway at nwvineyard.com so if you are looking to help the Tomball/Northwest Houston area that's one place to help or find out how to help.\r\n\r\nOriginal Post\r\n\r\nFrom my favorite \"No Hype\" weatherman:\r\n\r\n We are probably about 24 hours away from a general cessation of rainfall, including for most of the eastern half of the Houston area. It's been an absolutely miserable four or five days, but it's almost over -  Space City Weater\r\n\r\nAlmost through it!\r\n\r\nMessage from neighbor reports that everything seems high and dry still after the rains last night, so pretty sure my house, along with a few of the houses high enough on our street are not going to have water enter. If the water keeps going down, I'm hoping to have the family back in the house Friday or Saturday.\r\n\r\n\r\n\r\nThere will definitely be people in our neighborhood that will be dealing with repairs and issues as the video I posted previously shows.\r\n\r\nWalked over to the church today from our friends house. Figured I'd get some work done, and was doubtful of my ability to concentrate with my kiddos getting stir crazy. The walk was supposed to be 37 mins... an hour later I arrived. Gave me some extra time to pray at least :-)\r\n\r\n\r\n\r\nProjections for the area are never certain, but seems to point to being clear of anything that would cause water to get into the house.\r\n\r\n\r\n\r\nFor those interested in helping out in the area, my church is participating in relief efforts and starting to gather resources to help those in the community. Randy (the lead pastor at nwvineyard.com) will make sure that any donations made are used to impact the local area as this is a core passion of our small church plant.\r\n\r\n... * video no longer available on facebook - sorry *\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-08-29-a-waterlogged-texan-reporting-in-from-northpoint-tomball",
        "content": "---\r\ndate: \"2017-08-29T00:00:00Z\"\r\nexcerpt: What a crazy weekend. Choose to evacuate the family as water was slowly creeping\r\n  up closer to the house.\r\ntags:\r\nramblings\r\nfamily\r\nfollower-of-Jesus\r\ntitle: A Waterlogged Texan Reporting In from Northpoint Tomball\r\n---\r\n\r\nWaterlogged\r\n\r\nWhat a crazy weekend. Choose to evacuate the family as water was slowly creeping up closer to the house. I was ready today when I made it back to the house to see the water in the house, but to my astonishment it was only half-way up the driveway. Turning down the road 1 house away, many houses did not fare so well, and water looks to have made it in many homes.\r\n\r\n\r\n\r\nIt has been a stressful time, and still not finished, but what a remarkable time to see people coming together. From a neighbor walking my family in a small boat to the front to ease the way for the kids and pregant wife in 4 ft of dirty water to the evacuation boats running all day to help get people out. I saw first responders from a surfer walking a raft in his wetsuit to Kemah volunteer firefighters walking the roads to check on people and confirm everyone wanting evacuation was taken care of.\r\n\r\n\r\niframe src=\"https://www.youtube.com/embed/ze5enLNWnqU?wmode=opaque&enablejsapi=1\" height=\"480\" width=\"640\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"yes\"\r\n/iframe\r\n\r\n\r\n\r\nOur area was not as devastated as downtown and other areas that have been on the news, but for us it was still a pretty nerveracking experience.\r\n\r\nThe hospitality and friendship shown by my church was encouraging, and without these friends we'd have been in a tough spot. From driving through some deeper water, to providing warm meals and hospitality (and giving up some rooms/space) for us. As it stands, thanks to my friends I've got my kids tucked in warm and safe and am able to sit back and think about the days' events without being in the rain. Being removed for our home for a week helps put the important things back in perspective for sure!\r\n\r\n\r\n\r\nIf we make it through tonight we should be fine with no water damage, but this storm has definitely given us surprises.\r\n\r\nPrayers out there for those in worse situations from this storm. It's been a bit numbing to see just the little pieces I've seen in the last 2 days. The financial impact (no flood insurance for many in this storm, including us, as we are outside any normal flood areas) is going to be tough, along with the stress and rebuilding that will be required.\r\n\r\n\r\niframe src=\"https://www.youtube.com/embed/6jVehJxi-0o?wmode=opaque&enablejsapi=1\" height=\"480\" width=\"854\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"yes\"\r\n/iframe\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-09-03-recovery-is-a-marathon,-not-a-sprint-hurricane-harvey",
        "content": "---\r\ndate: \"2017-09-03T00:00:00Z\"\r\nexcerpt: It's important to remember that the road to recovery is not an instant process.\r\ntags:\r\nramblings\r\nflood\r\nfollower-of-Jesus\r\ntitle: Recovery is a Marathon, Not a Sprint - Hurricane Harvey\r\n---\r\n\r\nIt's important to remember that the road to recovery is not an instant process. Right now we have a ton of volunteers and people working to help those in need. I'm reminding myself, as much as any readers here, the process to recovery for people most impacted is not a single week of effort event. In some cases, entire homes were lost and without flood insurance, the financial devastation can be profound.\r\n\r\n\r\n\r\nI'm proud to be part of the Vineyard Church association which is sending an equipped response team to the area to send volunteers and various professionals to aid in recovery. This will definitely help increase the impact we have as people have to go back to work and available time is reduced.\r\n\r\n\r\n\r\n\r\niframe src=\"https://www.youtube.com/embed/FqizN37Tws4?wmode=opaque&enablejsapi=1\" height=\"480\" width=\"854\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"yes\"\r\n/iframe\r\n\r\n\r\n\r\ncontinued help\r\n\r\nIn the meantime, if you are helping with some giving @ nwvineyard.com/give please consider doing a recoccuring donation for a period of months to continue the help. I'm hoping that the impact the church community has in Houston is going to be a testimony to commitment as believers we have in serving those in suffering and in need.\r\n\r\n not be forgetful\r\n\r\nAs a reminder to all my fellow followers-of-Christ, our actions are under scrutinity (just see the barrage of accusations against Joel Olsteen right now). As I was driving to a site on the first day back in my house, I saw a small church with a guy riding a lawnmower and trimming a lawn that already looked good.\r\n\r\nIt made me angry and sad at the same time.\r\n\r\nYes, I should not judge. That church may have had a large response team out serving, and I'm probably just seeing part of the picture.\r\n\r\nHowever, that same part of the picture is what others would be seeing. Less than 1 mile up the road were houses devasted by the flood, ripping out drywall, furniture, and dealing with the loss of a lot of belongings. I pray that we all remember the hardship that others are going through and while we can enjoy and relish the moments of life we are given, let us strive to not forget those others those can't move on yet. Those who have lost everything, are in shelters, dealing with the emotional fallout of a disaster, and in the process of rebuilding a place they can call home for their family.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-09-08-welcome-to-the-world-little-ella",
        "content": "---\r\ndate: \"2017-09-08T00:00:00Z\"\r\ntags:\r\nfamily\r\nphotography\r\nfollower-of-Jesus\r\nramblings\r\ntitle: Welcome to the World Little Ella\r\n---\r\n\r\nProud to announce Ella the newest addition to my family joined us today @ 6:19pm, weighing in at 6lbs and 11 ounces. Yes, that's 6lbs of cuteness. :-) I'm incredibly blessed to have this third addition to my family, and look forward to treasuring every moment with her. God sure knows how to give an amazing gift!\r\n\r\nIt was a rough 24 hours resulting in our first cesarean experience. This was a bit intense, but Sarah pulled through as a trooper, and we are now finished with post-operation recovery, and enjoying time with our new bundle of awesome.\r\n\r\n!-- {% include gallery caption=\"Welcome to the World Little Ella\" %} --\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThanks for all the prayers!\r\nNow for some sleep... or not ;-)\r\n\r\nShout out to Willowbrook Women's Center for their consideration and stellar care during the delivery. I left my camera in the other room, not knowing operating room rules, and wanted to respect that, and not complicate the moment. A nurse asked me where my camera was so I could capture a few of the precious first moments, and walked to the other side of the hospital section to go fetch it for me to make it happen. That's how considerate they were!\r\n\r\nThanks to their care I have a healthy baby girl, healthy wife, and some beautiful moments despite the complication of the delivery.\r\n\r\n\r\niframe src=\"https://www.youtube.com/embed/vB1EqN-WfkI?wmode=opaque&enablejsapi=1\" height=\"480\" width=\"854\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"yes\"\r\n/iframe\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-09-29-origami-101",
        "content": "---\r\ncategories:\r\ncreative\r\ndate: \"2017-09-29T00:00:00Z\"\r\npublished: false\r\ntitle: Origami 101\r\n---\r\n\r\nMy son and i have begun exploring origami.\r\n\r\nFew things I've learned....\r\n\r\nPatience is key\r\n\r\nThe right paper matters for trying intermediate models with lots of folds or 3d models needing some shape control. I wasted nearly 2 hours on one complex design with 2 hours of work left... And had to stop as the paper was almost splitting from the work I had put into it.\r\n\r\nAdvanced origami is insanely complex, and only in recent years morphed into a realistic portrayal of objects due to advances in mathematics in origami pioneers. (See Robert Lang, Eric Joisel for examples)\r\n\r\nAdvanced origami practitioners typically are wet folding paper to allow shaping flexibility\r\n\r\nThis is a fun start but definitely have a long way to go! I look forward to trying more soon, as it's been a great way to spend some relaxing time and more so to have a great connection with my son.\r\n\r\n A good example of a model that would benefit from a different paper as I'm unable to curve the tail\r\n\r\n Classic crane. Why didn't I learn this earlier! Very relaxing but I don't see myself doing the 1000 crane task. Not enough patience\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-11-10-delight-in-the-little-moments",
        "content": "---\r\ncategories:\r\nfamily\r\ndate: \"2017-11-10T00:00:00Z\"\r\ntags:\r\nramblings\r\nvideography\r\nfamily\r\nfollower-of-Jesus\r\ntitle: Delight in the Little Moments\r\n---\r\n\r\nThis last Easter I had some fun with the kids at their grandparents. Remember the days of spinning until you were nauseous? Was fun to watch the kids enjoying this so I took a few clips, nothing fancy, just a snapshot of enjoying the little moments in life.\r\n\r\nThinking I'll try this approach for a while. Small 2 minute videos that aren't refined a lot are easy for me to produce. I typically only do a couple a year because I tend to work on them extensively, but maybe just like blogging, consistency and simplicity can be the key to keeping the momentum going.\r\n\r\niframe src=\"https://www.youtube.com/embed/H6hf6rna6gk?wmode=opaque&enablejsapi=1\" height=\"480\" width=\"854\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"yes\"/iframe\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2017-12-31-time-with-family-thanksgiving",
        "content": "---\r\ncategories:\r\nfamily\r\ndate: \"2017-12-31T00:00:00Z\"\r\ntags:\r\nfamily\r\nramblings\r\ntitle: Time With Family - Thanksgiving\r\n---\r\n\r\nI've made it my personal quest as a creative who has a passion for videography to try and capture simple moments during my life with family. There are plenty of epic indie films, and I can't commit this type of time with a day job, family, and other interests. I think this is a good start. Thanks to Robert & Steph for their hospitality and making this a special thanksgiving. Was a wonderful time with family\r\n\r\niframe src=\"https://www.youtube.com/embed/BAj2xx1_sTg?wmode=opaque&enablejsapi=1\" height=\"480\" width=\"854\" scrolling=\"no\" frameborder=\"0\" allowfullscreen=\"yes\"\r\n/iframe\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2018-03-18-migration-to-jekyll",
        "content": "---\r\ndate: \"2018-03-18T00:00:00Z\"\r\nlastmodifiedat: \"2019-02-21\"\r\ntags:\r\ntech\r\njekyll\r\ntitle: Migration To Jekyll\r\ntoc: true\r\n---\r\n\r\nI've been in the process of migrating my site to it's final home (as far as my inner geek can be satisfied staying with one platform)... Jekyll.\r\n\r\nJekyll\r\n\r\nJekyll is a static website generator that takes plain markdown files and runs through through files that are basically templates for the end html content, allowing flexibility in content generation. The result ends up being a static website with beautifully generated typography, search, pagination, and other great features for a blogging engine. You also keep the benefit of writing in our beloved markdown, allowing easy source controlling of your blog.\r\n\r\nThis site at this time basically is a github repo. Upon commit, Netlify provides an amazing free resource for developers to automatically launch a remote build, minify, ensure content is with their CDN and publishes the changes to your site upon successful build. Pretty amazing! They also have more flexibility than Github-Pages in that you can use other Ruby based plugins for Jekyll, while Github limits the plugins available, resulting in less features in Jekyll that are available.\r\n\r\n\r\n Worth It?\r\n\r\nThis was a pretty exhaustive migration process, primarily because I worked on ensuring all links were correctly remapped, features like tag pages were in place, and all assets were migrated from Cloudinary and other locations. Overall it was a very time consuming affair but considering free hosting that will scale for any load required vs $144 at squarespace, I think it's a win. In addition, no MySQL databases to manage, Apache webservers to maintain, PHP editions to troubleshoot.... well that sold me.\r\n\r\nUsing Fuzzy String Matching To Fix Urls\r\n\r\n resources\r\n\r\nPowerShell: Check List of Urls and Retrieve Status Codes\r\nPowerShell: Generate CSV from Sitemap.xml\r\n\r\nmatching urls\r\n\r\nI noticed a lot of broken url's when migrating my site, so I got a list of url's and wanted to compare the old broken urls against a list of current url's and do a match to find the best resulting match. For instance, with the Jekyll title generating the link, I had issues with a url change like this:\r\n\r\nI generated a sitemap csv by using ConvertFrom-Sitemap, originally written by Michael Hompus at TechCenter.\r\n\r\n| Original                                       | New                                                          |\r\n| ---------------------------------------------- | ------------------------------------------------------------ |\r\n| https://www.sheldonhull.com/blog/syncovery-arq | https://www.sheldonhull.com/blog/syncovery-&-arq-syncing-&-backup |\r\n\r\nWhat I wanted was a way to do a fuzzy match on the url to give me the best guess match, even if a few characters were different... and I did not want to write this from scratch in the time I have.\r\n\r\nI found a reference to a great library called Communary.PASM and in PowerShell ran the install command: Install-Package Communary.PASM -scope currentuser\r\n\r\nThe resulting adhoc script I created:\r\n\r\n{{% gist c57c51882e7102e6b9b383443c115409 %}}\r\n\r\nThe resulting matches were helpful in saving me a lot of time, finding partial matches when a few characters were off.\r\n\r\n| Original                                      | FuzzyMatched                                  |\r\n| --------------------------------------------- | --------------------------------------------- |\r\n| /blog/transaction-logging-recovery-101        | /blog/transaction-logging-&-recovery-(101)    |\r\n| /blog/transaction-logging-recovery-part-2     | /blog/transaction-logging-&-recovery-(part-2) |\r\n| /blog/transaction-logging-&-recovery-(part-3) | /blog/transaction-logging-recovery-part-3     |\r\n\r\nI then tested out another several algorithms. I had come across references to the Levenshtein algorithm when reading about string matching on Stack Overflow. I added that logic into my script, and watched paint dry while it ran. It wasn't a good fit for my basic string matching. Learning more about string matching sounds interesting though as it seems to be a common occurrence in development, and I'm all for anything that lets me write less regex :-)\r\n\r\nFor my rough purposes the Fuzzy match was the best fit, as most of the title was the same, just typically missing the end, or slight variance in the delimiter.\r\n\r\nI had other manual cleanup to do, but still, it was an interesting experiment. Gave me an appreciation for a consistent url naming schema, as migration to a new engine can be made very painful by changes in the naming of the posts. After some more digging, I decided to not worry about older post urls and mostly just focused on migrating any comments. I think the most interesting part of this was learning a little about the various string matching algorithm's out there... It's got my inner data geek interested in learning more on this.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2018-03-29-git-cracking",
        "content": "---\r\ndate: \"2018-03-29T00:00:00Z\"\r\nexcerpt: Git can be an interesting learning curve if you are coming from TFS (Team\r\n  Foundation Server). However, there are some great things about using Git if you\r\n  can wrap you head around the terminology\r\nlastmodifiedat: \"2018-03-29\"\r\npublished: true\r\ntags:\r\ncool-tools\r\ntech\r\ngit\r\ntitle: Git Cracking\r\ntoc: true\r\n---\r\n\r\n info \"Resources\"\r\n - GitKraken\r\n - Source Tree\r\n - Posh-Git\r\n - Cmder\r\n\r\nGit Some Pain\r\n\r\nHaving come from a Team Foundation Server background, I found Git to be a bit confusing. The problem is primarily the big difference in a distributed version control system vs non-distributed. In addition to that complexity the terminology is not exactly intuitive. A lot of phrases like PULL have different results depending on what step you are in.\r\n\r\n Here's Your Sign\r\nHere's my version of \"Here's Your Sign\" For Newbie Git Users That Are Coming from TFS Background\r\n\r\nYou Must a TFS'er using Git when...\r\nYou commit changes, and refresh the TFS Source Control Server website trying to see your changes... but nothing ... ever... changes.\r\nYou pull changes to get things locally, but then get confused about why you are submitting a pull request to give someone else changes?\r\nYou want to use a GUI\r\nYou use force options often because: 1) You are used to forcing Get Latest to fix esoteric issues 2) Force makes things work better in TFS (no comment)\r\nYou are googling ways to forcibly reset your respository to one version because you don't know what the heck is out of sync and are tired of merging your own mistakes.\r\nYou think branching is a big deal\r\nYou think it's magical that you can download a Git repo onto a phone, edit, commit, and all without a Visual Studio Installation taking up half your lifespan.\r\n\r\nI claim I'm innocent of any of those transgressions.\r\nAnd yes, I use command line through Cmder to get pretend some geek cred, then I go back to my GUI. :-) I have more to learn before I become a Git command line pro. I need pictures.\r\n\r\nThe Key Difference From TFS\r\n\r\nThe biggest difference to wrap my head around, was that I was working with a DVCS (Distributed Version Control System). This is a whole different approach than TFS, though they have many overlaps. I won't go into the pros/cons list in detail but here's the basics I've pulled (pun intended) from this.\r\n\r\n Pros\r\n\r\nI can save my work constantly in a local commit before I need to send remotely (almost like if I did shelves for each piece of work, and finally when pushing to server I'd be sending all my work with history/combined history)\r\nFile Based Workspace. Local Workspaces in TFS have benefit of recognizing additions and other changes, but it's tedious to do. Git makes this much cleaner.\r\nBranching! Wow. This is the best. I honestly don't mess around with branching in TFS. It has more overhead from what I've seen, and is not some lightweight process that's constantly used for experimentation. (Comment if you feel differently, I'm not a pro at TFS branching). With Git, I finally realized that instead of sitting on work that was in progress and might break something I could branch, experiment and either merge or discard all very easily. This is probably my favorite thing. I'll be using this a lot more.\r\n\r\nCons\r\n\r\nThe wording.\r\nMore complicated merging and branching seem a little more complex with DVCS than non distributed like TFS, but that's just my high level impression. YMMV\r\n\r\n GitKraken\r\n\r\nGitKraken, a Git GUI to solve your learning woes.\r\n\r\nGit GUI Goodness\r\n\r\nI'm a Powershell prompt addict. I prefer command line when possible. However, I think GitKraken helped make this process a bit easier for me. I was using posh-git and Cmder initially, then Vscode with GitLens. However, other than basic commit/pull, I've found myself relying on GitKraken a lot more, as it's just fast, intuitive and easier to understand with my addled brain. I'd rather leave energy for figuring out how to get Query Optimization Through Minification\r\n\r\n  Timeline\r\nTo be honest, their timeline view and the navigation and staging of the changes seemed pretty intuitive to me compared to what I'd seen in other tools. Overall, I found it easier to wrap my head around the concepts of Git with it, and less fear of merging changes from remote as I was able to easily review and accept changes through it's built in merging tool.\r\n\r\n\r\n\r\nOverall Impression\r\n\r\nOverall impression is positive. I'd say it's a nice solution to help with understanding and getting up and running faster than some other solutions, or using Git via command line along. While that's a worthy goal, being able to easily review changes, amend commits, pull and merge remote changes from multiple sources, and other things, I'm not sure a newbie could do all at any time near what a little effort in GitKraken would provide. So overall, it's a win. I've used it for this blog and am pretty darn happy with it. The cost for professional if using in a work environment with the need for better profile handling, integration with VSTS and other services is a reasonable cost. For those just working with some Github open source repos and Jekyll blogs, they have a free community version, so it's a win!\r\n\r\n A Free Alternative\r\n\r\nSource Tree from Atlassian is a pretty solid product as well that I've used. Unfortunatelym I've had stability issues with it lately, and it lacks the most important feature required for all good code tools... a dark theme :-)... on Windows at least as of now. No success getting dark theme traction except on Mac. -1 demerits for this omission! Overall it has promise, but it tends towards so many options it can be daunting. I'd lean towards the implementation by GitKraken being much cleaner, designed for simplicity and flexibility.\r\n\r\nDisclaimer: I like to review developer software from time to time, and occcasionally recieve a copy to continue using. This does not impact my reviews whatsoever, as I only use the stuff I find helpful that might be worth sharing. Good software makes the world go round!\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2018-03-30-offline-net35-install",
        "content": "---\r\ndate: \"2018-03-30T00:00:00Z\"\r\nexcerpt: SQL Server requirements vary based on the version.\r\nlastmodifiedat: \"2019-02-21\"\r\npublished: true\r\ntags:\r\nsql-server\r\ntech\r\ntitle: SQL .NET Requirements\r\ntoc: true\r\n---\r\n\r\nSQL Server Install Requirements\r\n\r\nSQL Server Installation requirements indicate .NET 3.5, 4.0, or 4.6 depending on the version. This is not including SSMS. At this point you shouldn't use SSMS from any SQL ISO. Just install SQL Management Studio directly. See  for more details on this\r\nImprovements with SSMS 2016\r\nUpdate SSMS With PS1\r\n\r\nFrom a quick review here's what you have regarding .NET requirements for the database engine.\r\n\r\n| SQL Version                                                  | .NET Required                                                |\r\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\r\n| = SQL 2016 RC1 (SQL 2017 included) | .NET 4.6                                                    |\r\n| SQL 2014                            | .NET 3.5 (manual install required)br /.NET 4.0 (automatic) |\r\n| SQL 2012                            | .NET 3.5 (manual install required)br /.NET 4.0 (automatic) |\r\n\r\nSpecifically noted in SQL 2012-2014 documentation is:\r\n\r\n .NET 3.5 SP1 is a requirement for SQL Server 2014 when you select Database Engine, Reporting Services, Master Data Services, Data Quality Services, Replication, or SQL Server Management Studio, and it is no longer installed by SQL Server Setup.\r\n\r\n When .NET 3.5 Install Just Won't Cooperate\r\n\r\nIf you need to install SQL Server that requires .NET 3.5 things can get a little tricky. This is a core feature with windows, so typically it's just a matter of going to Features and enabling, both in Windows 10 and Windows Server.\r\n\r\nHowever, if you have a tighter GPO impacting your windows update settings, then you probably need to get this whitelisted. If you are on a time-crunch or unable to get the blocking of .NET 3.5 fixed, then you can also resolve the situation by using a manual offline install of .NET 3.5. Even the setup package Microsoft offers has online functionality and thereby typically fails in those situations.\r\n\r\nOffline Approach\r\n\r\nSurprisingly, I had to dig quite a bit to find a solution, as the .NET 3.5 installers I downloaded still attempted online connections, resulting in installation failure.\r\n\r\nTurns out that to get an offline install correctly working you need a folder from the Windows install image (ISO) located at sources\\sxs.\r\n\r\nSince I wouldn't want to provide this directly here's the basic steps you take.\r\n\r\n Get NetFx3 Cab\r\n\r\nDownload ISO of Windows 10 (I'm guessing the version won't really matter as you just want the contents in one folder)\r\nMount ISO\r\nNavigate to: MountedISO  sources and copy the sxs directory to your location. It should contain microsoft-windows-netfx3-ondemand-package.cab. This is the big difference, as the other methods provide an MSI, not the cab file.\r\n\r\nCreate Package\r\n\r\nNext to create a reusable package\r\n\r\nCreate a directory: Install35Offline\r\n\r\nCopy SXS directory to this\r\n\r\nCreate 2 files. Gist below to save you some time.\r\n    Install35Offline.ps1\r\n    Install35Offline.bat\r\n\r\n{{% gist 954303c02bf1a5e05b45628dada83f9a %}}\r\n\r\nHopefully this will save you some effort, as it took me a little to figure out how to wrap it all up to make it easy to run.\r\n\r\nPacking this up in an internal chocolately package would be a helpful way to fix for any developers needing the help of their local dba wizard, and might even earn you some dev karma.",
        "tags": []
    },
    {
        "uri": "/content/blog/2018-07-23-deleting-a-directory-that-has-a-trailing-space-shouldn-t-be-this-hard",
        "content": "---\r\ndate: \"2018-07-23T00:00:00Z\"\r\nexcerpt: Can't delete a directory due to some edge case in naming errors... like a\r\n  trailing space. Here's the fix!\r\nlastmodifiedat: \"2019-03-18\"\r\npublished: true\r\ntags:\r\ntech\r\npowershell\r\ntitle: Deleting a Directory That Has a Trailing Space Shouldn't Be This Hard\r\n---\r\n\r\nIt shouldn't be this hard. This is a consumate #windowsmoment\r\n\r\n\r\n\r\nIf you occasionally use something like Robocopy, or other command line tool, it can be possible to create a directory with a trailing slash. For instance\r\n\r\n`cmd\r\nrobocopy \"C:\\test\\Taco\" \"C:\\Burritos\\are\\delicious \"\r\n`\r\n\r\nThis trailing space would be actually used by Robocopy to initialize a directory that has a trailing space in the name. This can appear as a duplicator in explorer, until you try to rename and notice a trailing slash. Attempts to rename, delete, or perform any activity to manipulate this directory fail as Windows indicates that it can't find the directory as it no longer exists or might have been moved.\r\n\r\nTo resolve this I found details on SO about how to delete in response to the question \"Can't Delete a Folder on Windows 7 With a Trailing Space\".\r\n\r\nApparently it's an issue with NFTS handling. To resolve you have to use cmd.exe to rd (remove directory), and change your path to a UNC path referring to your local path.\r\n\r\nTo resolve the error then you'd do:\r\n\r\n`cmd\r\nrm \\\\?\\C:\\Burritos\\are\\delicious\r\n`\r\n\r\nTo confirm that PowerShell can't resolve this I did a quick test by running:\r\n\r\n`cmd\r\ncd C:\\temp\r\nmd \".\\ Taco \"\r\n`\r\n\r\n`powershell\r\nFails - No error\r\nremove-item \"\\\\?\\C:\\temp\\taco \"\r\n\r\n Fails with error: Remove-Item : Cannot find path '\\\\localhost\\c$\\temp\\taco ' because it does not exist.\r\n$verbosepreference = 'continue'; Remove-Item \"\\\\localhost\\c$\\temp\\taco \"\r\n\r\nSUCCESS: Succeeds to remove it\r\nGCI C:\\Temp | Where-Object { $_.FullName -match 'taco'} | Remove-Item\r\n`\r\n\r\nSo for me, I wanted to confirm that PowerShell was truly unable to resolve the issue without resorting to cmd.exe for this. Turns out it can, but you need to pass the matched object in, not expect it to match the filepath directly.\r\n\r\nNow to go eat some tacos....\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2018-08-08-ntfs-compression-and-sql-server-dont-play-well-together",
        "content": "---\r\ndate: \"2018-08-08T00:00:00Z\"\r\nexcerpt: accidentally copying a database to a ntfs compressed volume can result in\r\n  some unpleasant aftertaste. If you get the error The requested operation could not\r\n  be completed due to a file system limitation you are probably at the right place.\r\nlastmodifiedat: \"2019-02-21\"\r\npublished: true\r\ntags:\r\ntech\r\nsql-server\r\npowershell\r\ntitle: NTFS Compression and SQL Server Do Not Play Well Together\r\n---\r\n\r\nWanted to be proactive and move a database that was in the default path on C:\\ to a secondary drive as it was growing pretty heavily.\r\n\r\nWhat I didn't realize was the adventure that would ensure.\r\n\r\nLesson 1\r\nDon't move a SQL Server database to a volume that someone has set NTFS Compression on at the drive level.\r\n\r\n Lesson 2\r\nCopy the database next time, instead of moving. Would have eased my anxious dba mind since I didn't have a backup. before you judge me.. it was a dev oriented enviroment, not production... disclaimer finished\r\n\r\nThe Nasty Errors and Warnings Ensue\r\nFirst, you'll get an error message if you try to mount the database and it has been compressed. Since I'd never done this before I didn't realize the mess I was getting into. It will tell you that you can't mount the database without marking as read-only as it's a compressed file.\r\n\r\nOk... so just go to file explorer  properties  advanced  uncheck compress ... right?\r\n\r\nNope...\r\n\r\n`cmd\r\nChanging File Attributes 'E:\\DATA\\FancyTacos.mdf' The requested operation could not be completed due to a file system limitation`\r\n`\r\n\r\nI found that message about as helpful as the favorite .NET error message object reference not found that is of course so easy to immediately fix.\r\n\r\n The Fix\r\nPull up volume properties. Uncheck compress drive\r\nOR\r\nIf you really want this compression, then make sure to uncompress the folders containing SQL Server files and apply.\r\n\r\nSince I wasn't able to fix this large of a file by toggling the file (it was 100gb+), I figured to keep it simple and try copying the database back to the original drive, unmark the archive attribute, then copy back to the drive I had removed compression on and see if this worked. While it sounded like a typical \"IT Crowd\" fix (have you tried turning it on and off again) I figured I'd give it a shot.\r\n\r\n... It worked. Amazingly enough it just worked.\r\n\r\nHere's a helpful script to get you on your way in case it takes a while. Use at your own risk, and please... always have backups! #DontBlameMeIfYouDidntBackThingsUp #CowsayChangedMyLife\r\n\r\n{{% gist c13eec8bbd570f762fd3834b19464465 %}}\r\n\r\nand finally to remount the database after copying it back to your drive ...\r\n\r\n{{% gist 274861a17a7db002bddd55861b781719 %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2018-08-19-dedupe-google-drive-with-rclone",
        "content": "---\r\ndate: \"2018-08-19T00:00:00Z\"\r\nexcerpt: Trouble with duplicate files in Google Drive? Want to fix a mess? Found the\r\n  tool to do it!\r\nlastmodifiedat: \"2019-02-21\"\r\npublished: true\r\ntags:\r\ntech\r\nramblings\r\ntitle: Dedupe Google Drive with RClone\r\n---\r\n\r\nAn issue with duplicates\r\n\r\nI migrated from Amazon Cloud Drive to a paid Google Drive account. To facilate this move, I used a paid service called MultCloud. For me, Comcast prevents unlimited data, so it would have been challenging to manage 1.5TB of video and photography files movement by downloading then reuploading to Google Drive.\r\n\r\nI ran into issues due to hitting rate limiting with Multcloud. As a result, I had to work through their awkard user interface to relaunch those jobs, which still had failures. I basically was left at the point of not really trusting all my files had successfully transferred.\r\n\r\nWhat's worse is that I found that the programmatic access by MultCloud seemed to be creating duplicates in the drive. Apparently Google Drive will allow you have to files side by side with the same name, as it doesn't operate like Windows in this manner, instead each file is considered unique. Same with folders.\r\n\r\n\r\n\r\n\r\n RClone\r\n\r\nI ran across RClone a while ago, and had passed over it only to arrive back at the documentation regarding Google Drive realizing they have specific functionality for this: dedupe. After working through some initial issues, my situation seems to have improved, and once again Google Drive is usable. In fact, it's time for some house cleaning.\r\n\r\nSuccessfully Running\r\n\r\nI suggest you make sure to find the developer api section and create an api access key. If you don't do this and just use Oauth2, you are going to get the dreaded message: Error 403: Rate Limit Exceeded and likely end up spending 30+ mins trying to track down what to do about this.\r\n\r\n\r\nYou'll see activity start to show up in the developer console and see how you are doing against your rate limits.\r\n\r\n\r\n\r\n Start Simple and Work Up From There\r\n\r\nTo avoid big mistakes, and confirm the behavior is doing what you expect, start small. In my script at the bottom, I walked through what I did.\r\n\r\n\r\n\r\n\r\nOther Cool Uses for RClone\r\n\r\n Merging\r\nWhile I think the dedupe command in RClone is specific to Google Drive, you can leverage it's logic for merging folders in other systems, as well as issue remote commands that are server side and don't require download locally before proceeding.\r\n\r\nServer Side Operations\r\nThis means, basically I could have saved the money over MultCloud, and instead used Rclone to achieve a copy from Amazon Cloud Drive to Google Drive, all remotely with server side execution, and no local downloads to achieve this. This has some great applications for data migration.\r\n\r\nFor an update list of what support they have for server side operations, take a look at this page: Server Side Operations\r\n\r\n AWS\r\nThis includes quite a few nifty S3 operations. Even though I'm more experienced with the AWSPowershell functionality, this might offer some great alternatives to syncing to an s3 bucket\r\n\r\nMounting Remote Storage As Local\r\n\r\nBuried in there was also mention of the ability to mount any of the storage systems as local drives in Windows. See RClount Mount documentation.. This means you could mount an S3 bucket as a local drive with RClone. I'll try and post an update on that after I try it out. It's pretty promising.\r\n\r\n{{% gist e286bd05ff154b47c8a1f8ecf2bdc22b %}}\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2019-02-07-migrating-from-hipchat-to-slack",
        "content": "---\r\ndate: \"2019-02-07T23:55:00Z\"\r\nexcerpt: Migrating from hipchat to slack can be a little painful if you have some\r\n  issues similar to mine to cleanup. Maybe this will help save you some time.\r\nlastmodifiedat: \"2019-02-07 23:55:00\"\r\npublished: true\r\ntags:\r\ntech\r\npowershell\r\ndevelopment\r\ntitle: Migrating From Hipchat To Slack\r\ntoc: true\r\ntypora-copy-images-to: ..\\assets\\img\r\ntypora-root-url: ..\\assets\\img\r\n---\r\n\r\nLast Minute Migration?\r\nIf you are about to perform a last minute migration here's a couple tips as you jump ship from Hipchat and move to Slack. Hipchat is sunsetting I believe on Feb 15th, so I figured I'd share what I do have in case it's helpful, as it won't stay tremendously relevant for long.\r\n\r\n Problem: Hipchat access will be removed and you need more time\r\nExport the hipchat content to a file and upload to your own s3 bucket. That will ensure you have some time to work through the migration and reverse it and try again if you aren't happy with the results.\r\n\r\nProblem: You want to do an initial import of Hipchat content and then update with deltas.\r\nDon't even consider it. The slack import can't add delta content for private messages and private rooms. This means you'd get a lot of duplicate rooms being created. It's better to do the migration import in one batch rather than try to incrementally pull in content. Don't go down this route, as I didn't discover this till later in the process resulting in a time-crunch.\r\n\r\n Problem: You have hipchat users that have email address that have been migrated to a new domain since they were created.\r\nYou've migrated to a new domain, but your Hipchat accounts all have the previous email which you've setup as email aliases. You can't easily change in Hipchat due to the fact it's set a profile level, \"synced\" to the Atlassian account. I had no luck in working on changing this so I instead leveraged the Slack API to bulk update during migration (after all the accounts were created). I mapped the active directory user to the current user by parsing out the email aliases and reversing this. I also created an alternative approach for those that had no matching email alias, and iffy full name matching to use fuzzy matching based soley on last name in the email address.\r\n\r\nImproving Your Migration Experience\r\n\r\n Rename Your Hipchat Rooms Prior to Migration (optional)\r\nThe Slack Migration tool is pretty good, but the auto renaming had some rename behavior that didn't align in a clean manner with what my naming convention was going to be. This means to simplify your migration, it's better to rename your Hipchat rooms prior to migration so all your rooms now create slack channels that don't have to be renamed again. Also, if you pull in a delta of content for public rooms, it can automatically match and incrementally add content (this doesn't work for private content).\r\n\r\nGetting Started with Hipchat CLI\r\nIt's painful. Hipchat's going into the great beyond so don't expect support for it.\r\n\r\n warning \"Important\"\r\n API Key for personal won't access full list of rooms in the action getRoomList in the CLI. Instead, you'll need to obtain the room list using Add-On token which I found too complex for my one time migration. Instead, you can copy the raw html of the table list, and use a regex script to parse out the room name and number list and use this. You can still perform room rename, just not sendmessage action on the rooms using the API token.\r\n\r\nInstall integration from marketplace to the entire account\r\nDownload the CLI for running locally\r\nCreate API Key. Important. This is a 40 character personal key, not the key you create as an admin in the administrators section. You need to go to your personal profile, and then create a key while selecting all permissions in the list to ensure full admin privileges.\r\nTo get the raw HTML easily, simply try this Chrome extension for selecting the table and copying the raw html of the table. CopyTables\r\nOpen the room listing in Hipchat. Using the extension select Rows as your selection criteria and then select Next Table. Copy the Raw html to an empty doc. Go to the next page (I had 3 pages to go through) and copy each full table contents to append to the raw html in your doc.\r\nOnce you have obtained all the html rows, then run the following script to parse out the html content into a [pscustomobject[]] collection to work with in your script.\r\n\r\n`powershell\r\n[reflection.assembly]::loadwithpartialname('System.Web')\r\n$HtmlRaw = Get-Content -Path '.\\TableRowRawHtml.html'\r\n$Matched = Select-String -InputObject $HtmlRaw -Pattern '((?=rooms/show/)\\d(?=\"))(.?\\n?.?)(?<=[])(.*?(?=<))' -AllMatches | Select-Object -ExpandProperty Matches\r\n\r\nWrite-PSFMessage -Level Important -Message \"Total Match Count: $(@($Matched).Count)\"\r\n\r\n[pscustomobject[]]$RoomListing = $Matched | ForEach-Object -Process {\r\n    $m = $_.Groups\r\n    [pscustomobject]@{\r\n            RoomId           = $m[1].Value\r\n            OriginalRoomName = [system.web.httputility]::HtmlDecode($m[3].Value)\r\n        }\r\n}\r\n\r\nWrite-PSFMessage -Level Important -Message \"Total Rooms Listed: $(@($RoomListing).Count)\"\r\n`\r\n\r\nNow you'll at least have a listing of room id's and names to work with, even if it took a while to get to it. There are other ways to get the data, such as expanding the column-format=999 but this timed out on me and this ended actually being the quickest way to proceed.\r\n\r\n Using CLI\r\nTo get started, cache your credentials using the fantastic BetterCredentials module. To install you'll need to run Install-Module BetterCredentials -Scope CurrentUser -AllowClobber -Force\r\n\r\nThen set your cached credentials so we don't need to hard code them into scripts. This will cache it in your Windows Credential manager.\r\n\r\n`powershell\r\n$cred = @{\r\n    credential   = ([pscredential]::new('myHipchatEmail' , (\"APITokenHere\" | ConvertTo-SecureString -AsPlainText -Force) ) )\r\n    type         = 'generic'\r\n    Persistence  = 'localcomputer'\r\n    Target       = 'hipchatapi'\r\n    description  = 'BetterCredentials cached credential for hipchat api'\r\n}\r\nBetterCredentials\\Set-Credential @cred\r\n`\r\n\r\nInitialize the working directory and default parameters for the CLI so you can easily run other commands without having to redo this over and over.\r\n\r\n`powershell\r\n----------------------------------------------------------------------------\r\nset location for the java cli environment                  \r\n----------------------------------------------------------------------------\r\n$Dir = Join-Path 'C:\\PathToCli' 'atlassian-cli-8.1.0-distribution\\atlassian-cli-8.1.0'\r\nSet-Location $Dir\r\n$Url = 'https://TACOS.hipchat.com'\r\n\r\n----------------------------------------------------------------------------\r\nconfigure default arguments for calling java cli              \r\n----------------------------------------------------------------------------\r\n$JavaCommand = \"java -jar $(Join-Path $dir 'lib/hipchat-cli-8.1.0.jar') --server $url --token $Password --autoWait --quiet\"\r\n`\r\n\r\nNow you can issue some simple commands to start manipulating the CLI.\r\n\r\n`powershell\r\n----------------------------------------------------------------------------\r\nGet Entire Room Listing -- Including Archived & Private           \r\n----------------------------------------------------------------------------\r\n$Action = '--action getRoomList --includePrivate --includeArchived --outputFormat 1'\r\n$result = Invoke-Expression -command \"$JavaCommand $Action\"\r\n$RoomList = $result | ConvertFrom-CSV\r\n$RoomList | Export-CliXml -Path (Join-Path $ScriptsDir 'CurrentRoomList.xml') -Encoding UTF8 -Force #just so we have a copy saved to review\r\n`\r\n\r\nI just tweaked this snippet for other types of commands, but this should get you pretty much what you need to run interactive commands via CLI. I've also written up some Slack functions and will likely share those soon as well as I've found them helpful in automatically fixing email addresses, activating & deactivating users, identifying active billed users, and other basic administrative focused actions.\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2019-02-07-sql-server-database-experimentation-assistance-how-to-run-a-capture",
        "content": "---\r\ndate: \"2019-02-07T00:07:21Z\"\r\nexcerpt: Just a quick look at the very beginning of setting up SQL Server Database\r\n  Experimentation Assistant\r\nlastmodifiedat: \"2019-02-08 00:07:21\"\r\ntags:\r\nsql-server\r\ntech\r\nperformance-tuning\r\ntitle: SQL Server Database Experimentation Assistant - How to Run a Capture\r\ntoc: false\r\ntypora-copy-images-to: ..\\assets\\img\r\ntypora-root-url: ..\\assets\\img\r\n---\r\n\r\nDEA\r\nVery basic look at the setup as I couldn't find much documentation on this when I last tried this out in 2018. Maybe it will help you get started a little more quickly. I've not had a chance to leverage the actual comparisons across a large workload. When I originally wrote up the basics on this last year I found my needs required more customized load testing approaches.\r\n\r\n Adding The Feature\r\n\r\nAdded the DRCReplay.exe and the controller services by pulling up the feature setup and adding existing features to existing SQL instance installed.\r\n\r\n\r\n\r\nPointed the controller directory to a new directory I created\r\n\r\n`powershell\r\n[io.directory]::CreateDirectory('X:\\Microsoft SQL Server\\DReplayClient\\WorkingDir')\r\n[io.directory]::CreateDirectory('X:\\Microsoft SQL Server\\DReplayClient\\ResultDir')\r\n`\r\n\r\nInitializing Test\r\n\r\nStarted with backup of the database before executing the activity I wanted to trace.\r\n\r\n`powershell\r\ndbatools\\backup-dbadatabase -sqlinstance localhost -database $Dbname -CopyOnly -CompressBackup\r\n`\r\n\r\nInitialized application application activity, and then recorded in DEA. The result was now in the capture section.\r\n\r\n\r\n\r\nRestoring after trace was recorded in DEA was simple with the following command from Dbatools\r\n\r\n`powershell\r\nrestore-dbadatabase -SqlInstance localhost -Path \"BackupFilePath\" -DatabaseName SMALL -WithReplace\r\n`\r\n\r\nAfter this restore, initiating the replay was achieved by going to the replay tab.\r\n\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/content/blog/2019-03-11-debugging-type-binding-in-powershell",
        "content": "---\r\ndate: \"2019-03-11T00:00:00Z\"\r\nexcerpt: What is the best way to debug more complicated parameter binding and type\r\n  casting scenarios in PowerShell.\r\nlastmodifiedat: \"2019-03-12\"\r\ntags:\r\npowershell\r\ndeep-dive\r\ntitle: Debugging Type Binding in PowerShell\r\ntoc: false\r\ntypora-copy-images-to: ..\\assets\\img\r\ntypora-root-url: ..\\assets\\img\r\n---\r\n\r\nSome effort I spent in researching Type Binding in Stack Overflow to help answer a question by Chris Oldwood helped me solidify my understanding of the best way to debug more  complicated scenarios such as this in PowerShell.\r\n\r\n\r\n Why does this PowerShell function's default argument change value based on the use of . or & to invoke a command within it? \r\n\r\nSpent some digging into this and this is what I've observed.\r\n\r\nFirst for clarity I do not believe that you should consider the NullString value the same as null in a basic comparison. Not sure why you need this either, as this is normally something I'd expect from c# development. You should be able to just use $null for most work in PowerShell.\r\n\r\n`powershell\r\nif($null -eq [System.Management.Automation.Language.NullString]::Value)\r\n{\r\n    write-host \"`$null -eq [System.Management.Automation.Language.NullString]::Value\"\r\n}\r\nelse\r\n{\r\n    write-host \"`$null -ne [System.Management.Automation.Language.NullString]::Value\"\r\n}\r\n`\r\n\r\nSecondly, the issue is not necessarily because of the call operator, ie &. I believe instead you are dealing with underlying parameter binding coercion. Strong data typing is definitely a weak area for PowerShell, as even explicitly declared [int]$val could end up being set to a string type by PowerShell automatically in the next line when writing Write-Host $Val.\r\n\r\nTo identify the underlying behavior, I used the Trace-Command function (Trace Command) .\r\n\r\nI changed the Use-Dot to just call the function as no write-host was needed to output the string.\r\n\r\n`powershell\r\nfunction Use-Ampersand\r\n{\r\n    param(\r\n        [string]$NullString = [System.Management.Automation.Language.NullString]::Value\r\n    )\r\n    Format-Type $NullString\r\n    &cmd.exe /c exit 0\r\n}\r\n`\r\n\r\nThe Format-Type I modified to also use what is considered a better practice of $null on the left, again due to type inference.\r\n\r\n`\r\nfunction Format-Type($v= [System.Management.Automation.Language.NullString]::Value)\r\n{\r\n\r\n    if ($null  -eq $v)\r\n    {\r\n     '(null)'\r\n    }\r\n    else {\r\n        $v.GetType().FullName\r\n     }\r\n}\r\n`\r\n\r\nTo narrow down the issue with the data types, I used the following commands, though this is not where I found insight into the issue. Theyh  when called directly worked the same.\r\n\r\n`\r\nTrace-Command -Name TypeConversion -Expression { Format-Type $NullString} -PSHost\r\nTrace-Command -Name TypeConversion -Expression { Format-Type ([System.Management.Automation.Language.NullString]$NullString) } -PSHost\r\n`\r\n\r\nHowever, when I ran the functions using TypeConversion tracing, it showed a difference in the conversions that likely explains some of your observed behavior.\r\n\r\n`\r\nTrace-Command -Name TypeConversion  -Expression { Use-Dot} -PSHost\r\nTrace-Command -Name TypeConversion  -Expression { Use-Ampersand} -PSHost\r\n`\r\n\r\n`\r\nUSE DOT\r\nDEBUG: TypeConversion Information: 0 :  Converting \"\" to \"System.String\".\r\nDEBUG: TypeConversion Information: 0 :      Converting object to string.\r\nDEBUG: TypeConversion Information: 0 :  Converting \"\" to \"System.Object\". <<<<<<<<<<<\r\nDEBUG: TypeConversion Information: 0 :  Converting \".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW;.CPL\" to \"System.String\".\r\nDEBUG: TypeConversion Information: 0 :      Result type is assignable from value to convert's type\r\n`\r\n\r\nOUTPUT: (null)\r\n\r\n`\r\n Use-Ampersand\r\nDEBUG: TypeConversion Information: 0 : Converting \"\" to \"System.String\".\r\nDEBUG: TypeConversion Information: 0 :     Converting object to string.\r\nDEBUG: TypeConversion Information: 0 : Converting \"\" to \"System.String\". <<<<<<<<<<<\r\nDEBUG: TypeConversion Information: 0 :     Converting null to \"\".        <<<<<<<<<<<\r\nDEBUG: TypeConversion Information: 0 : Converting \".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW;.CPL\" to \"System.String\".\r\nDEBUG: TypeConversion Information: 0 :     Result type is assignable from value to convert's type\r\n`\r\n\r\nOUTPUT: System.String\r\n\r\nThe noticeable difference is in Use-Ampersand it shows a statement of Converting null to \"\" vs Converting \"\" to \"System.Object\".\r\n\r\nIn PowerShell, $null  [string]''. An empty string comparison will pass the null check, resulting in the success of outputting GetType().\r\n\r\nA Few Thoughts On Approach With PowerShell\r\n\r\nWhy it's doing this, I'm not certain, but before you invest more time researching, let me provide one piece of advice based on learning the hard way.\r\n\r\nIf start dealing with issues due to trying to coerce data types in PowerShell, first consider if PowerShell is the right tool for the job\r\n\r\nYes, you can use type extensions. Yes, you can use .NET data types like $List = [System.Collections.Generic.List[string]]::new() and some .NET typed rules can be enforced. However, PowerShell is not designed to be a strongly typed language like C. Trying to approach it like this will result in a many difficulties. While I'm a huge fan of PowerShell, I've learned to recognize that it's flexibility should be appreciated, and it's limits respected.\r\n\r\nIf I really had issues that required mapping [System.Management.Automation.Language.NullString]::Value so strongly, I'd consider my approach.\r\n\r\nThat said, this was a challenging investigation that I had to take a swing at, while providing my 10 cents afterwards.\r\n\r\nOther Resources\r\n\r\nAfter posting my answer, I found another answer that seemed relevant, and also backs up the mentioning of not using [NullString] normally, as its usage in PowerShell is not really what it was designed for.\r\n\r\nStackoverflow specific content republished under CC-BY-SA\r\n",
        "tags": []
    }
]